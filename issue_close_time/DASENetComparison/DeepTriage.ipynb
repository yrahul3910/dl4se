{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepTriage code from https://bugtriage.mybluemix.net/#code\n",
    "\n",
    "Some changes were made:\n",
    "* **Data:** - Adjusted to conform to the DASENet data\n",
    "* **Architecture:** - Simplified slightly; output changed to DASENet settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "import json, re, nltk, string\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.models import Word2Vec\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Input, merge\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.utils import np_utils\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ryedida/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "bugs_json = './chromium.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters required for the entire code can be initialized upfront as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Word2vec parameters\n",
    "min_word_frequency_word2vec = 5\n",
    "embed_size_word2vec = 200\n",
    "context_window_word2vec = 5\n",
    "\n",
    "#2. Classifier hyperparameters\n",
    "numCV = 10\n",
    "max_sentence_len = 50\n",
    "min_sentence_length = 15\n",
    "rankK = 10\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bugs are loaded from the JSON file and the preprocessing is performed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bugs_json) as data_file:\n",
    "    data = json.load(data_file, strict=False)\n",
    "\n",
    "all_data = []\n",
    "all_owner = []\n",
    "all_y = []\n",
    "for item in data:\n",
    "    #1. Remove \\r \n",
    "    current_title = item['issue_title'].replace('\\r', ' ')\n",
    "    current_desc = item['description'].replace('\\r', ' ')    \n",
    "    #2. Remove URLs\n",
    "    current_desc = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', current_desc)    \n",
    "    #3. Remove Stack Trace\n",
    "    start_loc = current_desc.find(\"Stack trace:\")\n",
    "    current_desc = current_desc[:start_loc]    \n",
    "    #4. Remove hex code\n",
    "    current_desc = re.sub(r'(\\w+)0x\\w+', '', current_desc)\n",
    "    current_title= re.sub(r'(\\w+)0x\\w+', '', current_title)    \n",
    "    #5. Change to lower case\n",
    "    current_desc = current_desc.lower()\n",
    "    current_title = current_title.lower()    \n",
    "    #6. Tokenize\n",
    "    current_desc_tokens = nltk.word_tokenize(current_desc)\n",
    "    current_title_tokens = nltk.word_tokenize(current_title)\n",
    "    #7. Strip trailing punctuation marks    \n",
    "    current_desc_filter = [word.strip(string.punctuation) for word in current_desc_tokens]\n",
    "    current_title_filter = [word.strip(string.punctuation) for word in current_title_tokens]      \n",
    "    #8. Join the lists\n",
    "    current_data = current_title_filter + current_desc_filter\n",
    "    current_data = filter(None, current_data)\n",
    "    all_data.append(current_data)\n",
    "    all_owner.append(item['owner'])\n",
    "    all_y.append(item['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = [list(x) for x in all_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A vocabulary is constructed and the word2vec model is learnt using the preprocessed data. The word2vec model provides a semantic word representation for every word in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_model = Word2Vec(all_data, min_count=min_word_frequency_word2vec, size=embed_size_word2vec, window=context_window_word2vec)\n",
    "vocabulary = wordvec_model.wv.vocab\n",
    "vocab_size = len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ten times chronological cross validation split is performed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalLength = len(all_data)\n",
    "splitLength = int(totalLength / (numCV + 1))\n",
    "\n",
    "for i in range(1, numCV+1):\n",
    "    train_data = all_data[:i*splitLength-1]\n",
    "    test_data = all_data[i*splitLength:(i+1)*splitLength-1]\n",
    "    train_owner = all_owner[:i*splitLength-1]\n",
    "    test_owner = all_owner[i*splitLength:(i+1)*splitLength-1]\n",
    "    train_y = all_y[:i*splitLength-1]\n",
    "    test_y = all_y[i*splitLength:(i+1)*splitLength-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ith cross validation set, remove all the words that is not present in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1 # Denotes the cross validation set number\n",
    "updated_train_data = []    \n",
    "updated_train_data_length = []    \n",
    "updated_train_owner = []\n",
    "updated_train_y = []\n",
    "final_test_data = []\n",
    "final_test_owner = []\n",
    "final_test_y = []\n",
    "for j, item in enumerate(train_data):\n",
    "    current_train_filter = [word for word in item if word in vocabulary]\n",
    "    if len(current_train_filter)>=min_sentence_length:  \n",
    "        updated_train_data.append(current_train_filter)\n",
    "        updated_train_owner.append(train_owner[j])\n",
    "        updated_train_y.append(train_y[j])\n",
    "\n",
    "for j, item in enumerate(test_data):\n",
    "    current_test_filter = [word for word in item if word in vocabulary]  \n",
    "    if len(current_test_filter)>=min_sentence_length:\n",
    "        final_test_data.append(current_test_filter)  \n",
    "        final_test_owner.append(test_owner[j])\n",
    "        final_test_y.append(test_y[j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the ith cross validation set, remove those classes from the test set, for whom the train data is not available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1 # Denotes the cross validation set number\n",
    "# Remove data from test set that is not there in train set\n",
    "train_owner_unique = set(updated_train_owner)\n",
    "test_owner_unique = set(final_test_owner)\n",
    "train_y_unique = set(updated_train_y)\n",
    "test_y_unique = set(final_test_y)\n",
    "unwanted_owner = list(test_owner_unique - train_owner_unique)\n",
    "unwanted_y = list(test_y_unique - train_y_unique)\n",
    "updated_test_data = []\n",
    "updated_test_owner = []\n",
    "updated_test_y = []\n",
    "updated_test_data_length = []\n",
    "for j in range(len(final_test_owner)):\n",
    "    if final_test_owner[j] not in unwanted_owner:\n",
    "        updated_test_data.append(final_test_data[j])\n",
    "        updated_test_owner.append(final_test_owner[j])\n",
    "        updated_test_y.append(final_test_y[j])\n",
    "\n",
    "unique_train_label = list(set(updated_train_owner))\n",
    "unique_train_ys = list(set(updated_train_y))\n",
    "classes = np.array(unique_train_ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-58726f164dbf>:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  X_train[j, sequence_cnt, :] = wordvec_model[item]\n",
      "<ipython-input-31-58726f164dbf>:23: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  X_test[j, sequence_cnt, :] = wordvec_model[item]\n"
     ]
    }
   ],
   "source": [
    "X_train = np.empty(shape=[len(updated_train_data), max_sentence_len, embed_size_word2vec], dtype='float32')\n",
    "Y_train = np.empty(shape=[len(updated_train_y),1], dtype='int32')\n",
    "# 1 - start of sentence, # 2 - end of sentence, # 0 - zero padding. Hence, word indices start with 3 \n",
    "for j, curr_row in enumerate(updated_train_data):\n",
    "    sequence_cnt = 0         \n",
    "    for item in curr_row:\n",
    "        if item in vocabulary:\n",
    "            X_train[j, sequence_cnt, :] = wordvec_model[item] \n",
    "            sequence_cnt = sequence_cnt + 1                \n",
    "            if sequence_cnt == max_sentence_len-1:\n",
    "                break                \n",
    "    for k in range(sequence_cnt, max_sentence_len):\n",
    "        X_train[j, k, :] = np.zeros((1,embed_size_word2vec))        \n",
    "    Y_train[j,0] = unique_train_ys.index(updated_train_y[j])\n",
    "\n",
    "X_test = np.empty(shape=[len(updated_test_data), max_sentence_len, embed_size_word2vec], dtype='float32')\n",
    "Y_test = np.empty(shape=[len(updated_test_y),1], dtype='int32')\n",
    "# 1 - start of sentence, # 2 - end of sentence, # 0 - zero padding. Hence, word indices start with 3 \n",
    "for j, curr_row in enumerate(updated_test_data):\n",
    "    sequence_cnt = 0          \n",
    "    for item in curr_row:\n",
    "        if item in vocabulary:\n",
    "            X_test[j, sequence_cnt, :] = wordvec_model[item] \n",
    "            sequence_cnt = sequence_cnt + 1                \n",
    "            if sequence_cnt == max_sentence_len-1:\n",
    "                break                \n",
    "    for k in range(sequence_cnt, max_sentence_len):\n",
    "        X_test[j, k, :] = np.zeros((1,embed_size_word2vec))        \n",
    "    Y_test[j,0] = unique_train_ys.index(updated_test_y[j])\n",
    "\n",
    "y_train = np_utils.to_categorical(Y_train, len(unique_train_ys))\n",
    "y_test = np_utils.to_categorical(Y_test, len(unique_train_ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Wrapper, InputSpec, TimeDistributed, BatchNormalization, Bidirectional\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_safe(x):\n",
    "    return K.clip(x, K.common._EPSILON, 1.0 - K.common._EPSILON)\n",
    "\n",
    "class ProbabilityTensor(Wrapper):\n",
    "    \"\"\" function for turning 3d tensor to 2d probability matrix, which is the set of a_i's \"\"\"\n",
    "    def __init__(self, dense_function=None, *args, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        #layer = TimeDistributed(dense_function) or TimeDistributed(Dense(1, name='ptensor_func'))\n",
    "        layer = TimeDistributed(Dense(1, name='ptensor_func'))\n",
    "        super(ProbabilityTensor, self).__init__(layer, *args, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        if K.backend() == 'tensorflow':\n",
    "            if not input_shape[1]:\n",
    "                raise Exception('When using TensorFlow, you should define '\n",
    "                                'explicitly the number of timesteps of '\n",
    "                                'your sequences.\\n'\n",
    "                                'If your first layer is an Embedding, '\n",
    "                                'make sure to pass it an \"input_length\" '\n",
    "                                'argument. Otherwise, make sure '\n",
    "                                'the first layer has '\n",
    "                                'an \"input_shape\" or \"batch_input_shape\" '\n",
    "                                'argument, including the time axis.')\n",
    "\n",
    "        if not self.layer.built:\n",
    "            self.layer.build(input_shape)\n",
    "            self.layer.built = True\n",
    "        super(ProbabilityTensor, self).build()\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        # b,n,f -> b,n \n",
    "        #       s.t. \\sum_n n = 1\n",
    "        if isinstance(input_shape, (list,tuple)) and not isinstance(input_shape[0], int):\n",
    "            input_shape = input_shape[0]\n",
    "\n",
    "        return (input_shape[0], input_shape[1])\n",
    "\n",
    "    def squash_mask(self, mask):\n",
    "        if K.ndim(mask) == 2:\n",
    "            return mask\n",
    "        elif K.ndim(mask) == 3:\n",
    "            return K.any(mask, axis=-1)\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if mask is None:\n",
    "            return None\n",
    "        return self.squash_mask(mask)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        energy = K.squeeze(self.layer(x), 2)\n",
    "        p_matrix = K.softmax(energy)\n",
    "        if mask is not None:\n",
    "            mask = self.squash_mask(mask)\n",
    "            p_matrix = make_safe(p_matrix * mask)\n",
    "            p_matrix = (p_matrix / K.sum(p_matrix, axis=-1, keepdims=True))*mask\n",
    "        return p_matrix\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {}\n",
    "        base_config = super(ProbabilityTensor, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class SoftAttentionConcat(ProbabilityTensor):\n",
    "    '''This will create the context vector and then concatenate it with the last output of the LSTM'''\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        # b,n,f -> b,f where f is weighted features summed across n\n",
    "        return (input_shape[0], 2*input_shape[2])\n",
    "\n",
    "    def compute_mask(self, x, mask=None):\n",
    "        if mask is None or mask.ndim==2:\n",
    "            return None\n",
    "        else:\n",
    "            raise Exception(\"Unexpected situation\")\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # b,n,f -> b,f via b,n broadcasted\n",
    "        p_vectors = K.expand_dims(super(SoftAttentionConcat, self).call(x, mask), 2)\n",
    "        expanded_p = K.repeat_elements(p_vectors, K.int_shape(x)[2], axis=2)\n",
    "        context = K.sum(expanded_p * x, axis=1)\n",
    "        last_out = x[:, -1, :]\n",
    "        return K.concatenate([context, last_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(max_sentence_len, embed_size_word2vec))\n",
    "#sequence_embed = Embedding(vocab_size, embed_size_word2vec, input_length=max_sentence_len)(inp)\n",
    "\n",
    "forwards_1 = Bidirectional(LSTM(1024, return_sequences=True, recurrent_dropout=0.2))(inp)\n",
    "attention_1 = SoftAttentionConcat()(forwards_1)\n",
    "after_dp_forward_5 = BatchNormalization()(attention_1)\n",
    "\n",
    "after_merge = Dense(1000, activation='relu')(after_dp_forward_5)\n",
    "after_dp = Dropout(0.4)(after_merge)\n",
    "output = Dense(2, activation='softmax')(after_dp)                \n",
    "model = Model(inputs=inp, outputs=output)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(lr=1e-4), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        [(None, 50, 200)]         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 50, 2048)          10035200  \n",
      "_________________________________________________________________\n",
      "soft_attention_concat_24 (So (None, 4096)              2049      \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 2)                 2002      \n",
      "=================================================================\n",
      "Total params: 14,152,635\n",
      "Trainable params: 14,144,443\n",
      "Non-trainable params: 8,192\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the deep learning model and test using the classifier as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.argmax(y_train, axis=-1)\n",
    "y_test = np.argmax(y_test, axis=-1)\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train < 5, num_classes=2)\n",
    "y_test = tf.keras.utils.to_categorical(y_test < 5, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "  4/390 [..............................] - ETA: 18:18 - loss: 0.4092 - accuracy: 0.8438"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-a17659f19c86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mearly_stopping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
    "hist = model.fit(X_train, y_train, batch_size=batch_size, epochs=200, callbacks=[early_stopping])              \n",
    "    \n",
    "predict = model.predict(X_test)        \n",
    "accuracy = []\n",
    "sortedIndices = []\n",
    "pred_classes = []\n",
    "for ll in predict:\n",
    "    sortedIndices.append(sorted(range(len(ll)), key=lambda ii: ll[ii], reverse=True))\n",
    "for k in range(1, rankK+1):\n",
    "    id = 0\n",
    "    trueNum = 0\n",
    "    for sortedInd in sortedIndices:            \n",
    "        pred_classes.append(classes[sortedInd[:k]])\n",
    "        if y_test[id] in classes[sortedInd[:k]]:\n",
    "            trueNum += 1\n",
    "        id += 1\n",
    "    accuracy.append((float(trueNum) / len(predict)) * 100)\n",
    "print('Test accuracy:', accuracy)       \n",
    "\n",
    "train_result = hist.history        \n",
    "print(train_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
