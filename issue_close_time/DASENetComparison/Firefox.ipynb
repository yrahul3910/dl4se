{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Bug-Related-Activity-Logs'...\n",
      "remote: Enumerating objects: 5, done.\u001b[K\n",
      "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
      "remote: Compressing objects: 100% (5/5), done.\u001b[K\n",
      "remote: Total 5 (delta 0), reused 5 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (5/5), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/yrahul3910/Bug-Related-Activity-Logs.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Bug-Related-Activity-Logs/firefox.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>bugID</th>\n",
       "      <th>user_comments</th>\n",
       "      <th>system_records</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "      <th>s9</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1000002</td>\n",
       "      <td>['eyedropper', 'style', 'editor', 'use', 'shar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1000002</td>\n",
       "      <td>['258347', 'assignee', 'comment1', 'createdatt...</td>\n",
       "      <td>['assignee', 'nobody', 'archaeopteryx', 'statu...</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>260</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1000002</td>\n",
       "      <td>['75935', 'comment7']</td>\n",
       "      <td>['keywords', 'checkin-needed', 'whiteboard', '...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1000025</td>\n",
       "      <td>['keyboard', 'long', 'word', 'suggestions', 'c...</td>\n",
       "      <td>['see', 'also', 'bug', 'blocking-b2g', '2.0', ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1000025</td>\n",
       "      <td>['465859', 'comment2', 'ni', 'william', 'regre...</td>\n",
       "      <td>['cc', 'bhuang', 'whsu', 'flags', 'needinfo', ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    bugID                                      user_comments  \\\n",
       "0           0  1000002  ['eyedropper', 'style', 'editor', 'use', 'shar...   \n",
       "1           1  1000002  ['258347', 'assignee', 'comment1', 'createdatt...   \n",
       "2           2  1000002                              ['75935', 'comment7']   \n",
       "3           3  1000025  ['keyboard', 'long', 'word', 'suggestions', 'c...   \n",
       "4           4  1000025  ['465859', 'comment2', 'ni', 'william', 'regre...   \n",
       "\n",
       "                                      system_records  s1  s2  s3   s4  s5  s6  \\\n",
       "0                                                 []   1   1   0   32   1   0   \n",
       "1  ['assignee', 'nobody', 'archaeopteryx', 'statu...  13   6   7  260   2   2   \n",
       "2  ['keywords', 'checkin-needed', 'whiteboard', '...   2   1   1   10   3   3   \n",
       "3  ['see', 'also', 'bug', 'blocking-b2g', '2.0', ...   5   2   3   81   1   0   \n",
       "4  ['cc', 'bhuang', 'whsu', 'flags', 'needinfo', ...   2   1   1   27   2   3   \n",
       "\n",
       "          s7  s8         s9  y  \n",
       "0  [0, 0, 0]   1  [0, 1, 0]  3  \n",
       "1  [0, 1, 0]   3  [0, 1, 1]  1  \n",
       "2  [1, 0, 1]   1  [0, 0, 1]  0  \n",
       "3  [0, 0, 0]   2  [1, 0, 1]  9  \n",
       "4  [1, 0, 0]   1  [0, 0, 1]  6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Unnamed: 0', 'bugID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_comments</th>\n",
       "      <th>system_records</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>s7</th>\n",
       "      <th>s8</th>\n",
       "      <th>s9</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['eyedropper', 'style', 'editor', 'use', 'shar...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['258347', 'assignee', 'comment1', 'createdatt...</td>\n",
       "      <td>['assignee', 'nobody', 'archaeopteryx', 'statu...</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>260</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "      <td>3</td>\n",
       "      <td>[0, 1, 1]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['75935', 'comment7']</td>\n",
       "      <td>['keywords', 'checkin-needed', 'whiteboard', '...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['keyboard', 'long', 'word', 'suggestions', 'c...</td>\n",
       "      <td>['see', 'also', 'bug', 'blocking-b2g', '2.0', ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[0, 0, 0]</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 0, 1]</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['465859', 'comment2', 'ni', 'william', 'regre...</td>\n",
       "      <td>['cc', 'bhuang', 'whsu', 'flags', 'needinfo', ...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[1, 0, 0]</td>\n",
       "      <td>1</td>\n",
       "      <td>[0, 0, 1]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       user_comments  \\\n",
       "0  ['eyedropper', 'style', 'editor', 'use', 'shar...   \n",
       "1  ['258347', 'assignee', 'comment1', 'createdatt...   \n",
       "2                              ['75935', 'comment7']   \n",
       "3  ['keyboard', 'long', 'word', 'suggestions', 'c...   \n",
       "4  ['465859', 'comment2', 'ni', 'william', 'regre...   \n",
       "\n",
       "                                      system_records  s1  s2  s3   s4  s5  s6  \\\n",
       "0                                                 []   1   1   0   32   1   0   \n",
       "1  ['assignee', 'nobody', 'archaeopteryx', 'statu...  13   6   7  260   2   2   \n",
       "2  ['keywords', 'checkin-needed', 'whiteboard', '...   2   1   1   10   3   3   \n",
       "3  ['see', 'also', 'bug', 'blocking-b2g', '2.0', ...   5   2   3   81   1   0   \n",
       "4  ['cc', 'bhuang', 'whsu', 'flags', 'needinfo', ...   2   1   1   27   2   3   \n",
       "\n",
       "          s7  s8         s9  y  \n",
       "0  [0, 0, 0]   1  [0, 1, 0]  3  \n",
       "1  [0, 1, 0]   3  [0, 1, 1]  1  \n",
       "2  [1, 0, 1]   1  [0, 0, 1]  0  \n",
       "3  [0, 0, 0]   2  [1, 0, 1]  9  \n",
       "4  [1, 0, 0]   1  [0, 0, 1]  6  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-7cfeebf4c52b>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _df['s70'] = df['s7'].apply(lambda x: eval(x)[0])\n"
     ]
    }
   ],
   "source": [
    "_df = df[['s1', 's2', 's3', 's4', 's5', 's6', 's8', 'y']]\n",
    "_df['s70'] = df['s7'].apply(lambda x: eval(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-c20f512c3275>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _df['s71'] = df['s7'].apply(lambda x: eval(x)[1])\n"
     ]
    }
   ],
   "source": [
    "_df['s71'] = df['s7'].apply(lambda x: eval(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-1e3222879f2f>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  _df['s72'] = df['s7'].apply(lambda x: eval(x)[2])\n"
     ]
    }
   ],
   "source": [
    "_df['s72'] = df['s7'].apply(lambda x: eval(x)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>s8</th>\n",
       "      <th>y</th>\n",
       "      <th>s70</th>\n",
       "      <th>s71</th>\n",
       "      <th>s72</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>260</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   s1  s2  s3   s4  s5  s6  s8  y  s70  s71  s72\n",
       "0   1   1   0   32   1   0   1  3    0    0    0\n",
       "1  13   6   7  260   2   2   3  1    0    1    0\n",
       "2   2   1   1   10   3   3   1  0    1    0    1\n",
       "3   5   2   3   81   1   0   2  9    0    0    0\n",
       "4   2   1   1   27   2   3   1  6    1    0    0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df['s90'] = df['s9'].apply(lambda x: eval(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df['s91'] = df['s9'].apply(lambda x: eval(x)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df['s92'] = df['s9'].apply(lambda x: eval(x)[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "      <th>s8</th>\n",
       "      <th>y</th>\n",
       "      <th>s70</th>\n",
       "      <th>s71</th>\n",
       "      <th>s72</th>\n",
       "      <th>s90</th>\n",
       "      <th>s91</th>\n",
       "      <th>s92</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>260</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   s1  s2  s3   s4  s5  s6  s8  y  s70  s71  s72  s90  s91  s92\n",
       "0   1   1   0   32   1   0   1  3    0    0    0    0    1    0\n",
       "1  13   6   7  260   2   2   3  1    0    1    0    0    1    1\n",
       "2   2   1   1   10   3   3   1  0    1    0    1    0    0    1\n",
       "3   5   2   3   81   1   0   2  9    0    0    0    1    0    1\n",
       "4   2   1   1   27   2   3   1  6    1    0    0    0    0    1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = _df.drop('y', axis=1)\n",
    "y = _df['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from raise_utils.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(*train_test_split(x, y))\n",
    "data.y_train = data.y_train < 4\n",
    "data.y_test = data.y_test < 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raise_utils.learners import FeedforwardDL\n",
    "from raise_utils.hyperparams import DODGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-570def329128>, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-570def329128>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    dodge = DODGE(config)dodge.optimize()\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"n_runs\": 10,\n",
    "    \"transforms\": [\"normalize\", \"standardize\", \"robust\", \"maxabs\", \"minmax\"] * 30,\n",
    "    \"metrics\": [\"d2h\", \"pd\", \"pf\", \"prec\"],\n",
    "    \"random\": True,\n",
    "    \"learners\": [FeedforwardDL(wfo=True, weighted=True, random={'n_layers': (2, 6), 'n_units': (3, 20)}, n_epochs=20)],\n",
    "    \"log_path\": \"./orig-ghost-log/\",\n",
    "    \"data\": [data],\n",
    "    \"name\": \"firefox\"\n",
    "}\n",
    "for _ in range(50):\n",
    "    config[\"learners\"].append(\n",
    "        FeedforwardDL(weighted=True, wfo=True, random={'n_layers': (2, 6), 'n_units': (3, 20)}, n_epochs=20))\n",
    "\n",
    "dodge = DODGE(config)dodge.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raise_utils.interpret import DODGEInterpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = DODGEInterpreter(files=['./orig-ghost-log/firefox.txt'], max_by=lambda p: p[1]-p[2], \n",
    "                          metrics=['d2h', 'pd', 'pf', 'prec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = interp.interpret()['firefox.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = results['pd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = results['pf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "prec = results['prec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.80222099, 0.79822321, 0.75613548, 0.78656302, 0.79133815,\n",
       "        0.79622432, 0.73903387, 0.79322599, 0.80888395, 0.79344808]),\n",
       " array([0.41572505, 0.42831921, 0.37911959, 0.41089925, 0.40760358,\n",
       "        0.41525424, 0.34357345, 0.40995763, 0.43961864, 0.41866761]),\n",
       " array([0.67162514, 0.66389582, 0.67886341, 0.66985058, 0.67296251,\n",
       "        0.67021873, 0.69511176, 0.67221909, 0.66104002, 0.66763222]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd, pf, prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7933370349805664, 1.260498320268757)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(pd), 1./np.median(pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.41307674199623357, 2.420857672033053)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(pf), 1. / np.median(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6709219359652034, 1.4904863686732457)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(prec), 1. / np.median(prec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `max_by = 0`:\n",
    "\n",
    "\\begin{align}\n",
    "tp &= 44 \\\\\n",
    "fn &= 12 \\\\\n",
    "fp &= 21 \\\\\n",
    "tn &= 31\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acc = 75 / 108 = 0.694"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `max_by = f1`:\n",
    "\n",
    "\\begin{align}\n",
    "tp &= 47 \\\\\n",
    "fn &= 8 \\\\\n",
    "fp &= 26 \\\\\n",
    "tn &= 26\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acc = 73 / 108 = 0.676"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `max_by = pd - pf`:\n",
    "\n",
    "\\begin{align}\n",
    "tp &= 44 \\\\\n",
    "fn &= 11 \\\\\n",
    "fp &= 22 \\\\\n",
    "tn &= 31\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acc = 0.694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5124"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data.y_train) / len(data.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141a96a90>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 3, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141a96a30>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 3, 'n_units': 18, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141aaed90>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 2, 'n_units': 8, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ac7100>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 2, 'n_units': 4, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ac73a0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 5, 'n_units': 16, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ac7640>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 3, 'n_units': 6, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141aaee20>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 3, 'n_units': 15, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ac7b50>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 3, 'n_units': 17, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ac7df0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 5, 'n_units': 20, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141acd0d0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 5, 'n_units': 5, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141acd370>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 3, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141acd610>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 5, 'n_units': 15, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141acd8b0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 2, 'n_units': 12, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141acdb50>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 6, 'n_units': 19, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141acddf0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 3, 'n_units': 17, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ad40d0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 3, 'n_units': 11, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ad4370>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 4, 'n_units': 12, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ad4610>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 2, 'n_units': 18, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ad48b0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 6, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ad4b50>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 2, 'n_units': 5, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ad4df0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 4, 'n_units': 19, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ad90d0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 5, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ad9370>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 6, 'n_units': 20, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ad9610>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 5, 'n_units': 18, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ad98b0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 6, 'n_units': 18, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ad9b50>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 5, 'n_units': 16, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ad9df0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 4, 'n_units': 4, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ae10d0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 3, 'n_units': 8, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ae1370>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 6, 'n_units': 11, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ae1610>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 6, 'n_units': 12, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ae18b0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 2, 'n_units': 14, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ae1b50>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 6, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ae1df0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 3, 'n_units': 9, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141aea0d0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 2, 'n_units': 17, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141aea370>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 2, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141aea610>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 4, 'n_units': 14, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141aea8b0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 6, 'n_units': 9, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141aeab50>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 3, 'n_units': 14, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141aeadf0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 6, 'n_units': 19, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ae30d0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 2, 'n_units': 4, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ae3370>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 3, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ae3610>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 4, 'n_units': 8, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ae38b0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 4, 'n_units': 19, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ae3b50>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 5, 'n_units': 20, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141ae3df0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 4, 'n_units': 5, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141af60d0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 5, 'n_units': 11, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141af6370>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 5, 'n_units': 4, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141af6610>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 5, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141af68b0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 4, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141af6b50>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 2, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x141af6df0>, 'loss': 'binary_crossentropy', 'n_epochs': 20, 'n_layers': 6, 'n_units': 18, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'weighted': False, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxabsx|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6488 - val_loss: 0.6219\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6166 - val_loss: 0.6024\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6047 - val_loss: 0.5941\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5985 - val_loss: 0.5887\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5944 - val_loss: 0.5857\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5921 - val_loss: 0.5839\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5909 - val_loss: 0.5831\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5900 - val_loss: 0.5820\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5893 - val_loss: 0.5810\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5885 - val_loss: 0.5807\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5880 - val_loss: 0.5801\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5875 - val_loss: 0.5798\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5872 - val_loss: 0.5794\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5869 - val_loss: 0.5793\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5865 - val_loss: 0.5790\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5863 - val_loss: 0.5792\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5863 - val_loss: 0.5784\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5860 - val_loss: 0.5785\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5864 - val_loss: 0.5783\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5858 - val_loss: 0.5790\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/raise_utils/learners/feedforward.py:100: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "1\n",
      "minmaxn|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6874 - val_loss: 0.6690\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6413 - val_loss: 0.6131\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6102 - val_loss: 0.5967\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5987 - val_loss: 0.5876\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5940 - val_loss: 0.5867\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5922 - val_loss: 0.5838\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5910 - val_loss: 0.5817\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5888 - val_loss: 0.5813\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5878 - val_loss: 0.5827\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5869 - val_loss: 0.5811\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5870 - val_loss: 0.5797\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5863 - val_loss: 0.5794\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5855 - val_loss: 0.5799\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5857 - val_loss: 0.5786\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5853 - val_loss: 0.5782\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5845 - val_loss: 0.5779\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5843 - val_loss: 0.5777\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5843 - val_loss: 0.5778\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5844 - val_loss: 0.5780\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5838 - val_loss: 0.5772\n",
      "2\n",
      "minmaxQ|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6939 - val_loss: 0.6872\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6817 - val_loss: 0.6737\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6659 - val_loss: 0.6547\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6493 - val_loss: 0.6392\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6375 - val_loss: 0.6286\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6292 - val_loss: 0.6211\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6229 - val_loss: 0.6155\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6180 - val_loss: 0.6105\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6137 - val_loss: 0.6061\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6100 - val_loss: 0.6024\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6069 - val_loss: 0.5995\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6045 - val_loss: 0.5969\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6025 - val_loss: 0.5949\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6008 - val_loss: 0.5932\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5993 - val_loss: 0.5916\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5982 - val_loss: 0.5904\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5972 - val_loss: 0.5893\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5963 - val_loss: 0.5886\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5954 - val_loss: 0.5877\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5947 - val_loss: 0.5869\n",
      "3\n",
      "normalizeh|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6805 - val_loss: 0.6577\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6384 - val_loss: 0.6160\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6161 - val_loss: 0.6026\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6058 - val_loss: 0.5942\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5992 - val_loss: 0.5906\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5951 - val_loss: 0.5888\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5931 - val_loss: 0.5838\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5909 - val_loss: 0.5840\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5902 - val_loss: 0.5813\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5893 - val_loss: 0.5807\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5882 - val_loss: 0.5803\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5877 - val_loss: 0.5799\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5874 - val_loss: 0.5798\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5870 - val_loss: 0.5811\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5870 - val_loss: 0.5797\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5865 - val_loss: 0.5791\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5863 - val_loss: 0.5789\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5867 - val_loss: 0.5786\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5862 - val_loss: 0.5790\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5859 - val_loss: 0.5785\n",
      "4\n",
      "normalizeM|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6949 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "5\n",
      "normalizer|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_20 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6820 - val_loss: 0.6655\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6447 - val_loss: 0.6249\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6188 - val_loss: 0.6083\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6083 - val_loss: 0.6001\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6025 - val_loss: 0.5953\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5990 - val_loss: 0.5920\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5968 - val_loss: 0.5896\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5954 - val_loss: 0.5881\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5935 - val_loss: 0.5863\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5926 - val_loss: 0.5852\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5917 - val_loss: 0.5844\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5912 - val_loss: 0.5841\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5905 - val_loss: 0.5829\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5905 - val_loss: 0.5826\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5897 - val_loss: 0.5824\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5896 - val_loss: 0.5821\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5892 - val_loss: 0.5818\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5888 - val_loss: 0.5814\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5884 - val_loss: 0.5815\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5886 - val_loss: 0.5813\n",
      "6\n",
      "robustv|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_25 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6569 - val_loss: 0.6134\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6052 - val_loss: 0.5929\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5920 - val_loss: 0.5848\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5857 - val_loss: 0.5776\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5818 - val_loss: 0.5814\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5819 - val_loss: 0.5745\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5786 - val_loss: 0.5724\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5786 - val_loss: 0.5759\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5772 - val_loss: 0.5715\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5761 - val_loss: 0.5719\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5752 - val_loss: 0.5710\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5745 - val_loss: 0.5698\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5738 - val_loss: 0.5708\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5740 - val_loss: 0.5714\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5728 - val_loss: 0.5689\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5721 - val_loss: 0.5705\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5733 - val_loss: 0.5681\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5722 - val_loss: 0.5677\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5715 - val_loss: 0.5683\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5715 - val_loss: 0.5681\n",
      "7\n",
      "standardizeL|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_32 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6764 - val_loss: 0.6349\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6187 - val_loss: 0.6047\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6050 - val_loss: 0.5961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5974 - val_loss: 0.5899\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5918 - val_loss: 0.5847\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5885 - val_loss: 0.5807\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5857 - val_loss: 0.5797\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5841 - val_loss: 0.5779\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5831 - val_loss: 0.5771\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5817 - val_loss: 0.5753\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5810 - val_loss: 0.5750\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5798 - val_loss: 0.5746\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5795 - val_loss: 0.5738\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5788 - val_loss: 0.5734\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5782 - val_loss: 0.5730\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5781 - val_loss: 0.5726\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5778 - val_loss: 0.5720\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5768 - val_loss: 0.5724\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5763 - val_loss: 0.5718\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5761 - val_loss: 0.5713\n",
      "8\n",
      "normalizeg|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_37 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6833 - val_loss: 0.6605\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6354 - val_loss: 0.6110\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6115 - val_loss: 0.6021\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6051 - val_loss: 0.5965\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6019 - val_loss: 0.5956\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5993 - val_loss: 0.5918\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5960 - val_loss: 0.5895\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5938 - val_loss: 0.5874\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5917 - val_loss: 0.5876\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5915 - val_loss: 0.5895\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5899 - val_loss: 0.5818\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5866 - val_loss: 0.5800\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5855 - val_loss: 0.5789\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5839 - val_loss: 0.5793\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5837 - val_loss: 0.5771\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5830 - val_loss: 0.5770\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5822 - val_loss: 0.5764\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5833 - val_loss: 0.5756\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5815 - val_loss: 0.5753\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5808 - val_loss: 0.5757\n",
      "9\n",
      "normalizeI|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6817 - val_loss: 0.6554\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6263 - val_loss: 0.6022\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6028 - val_loss: 0.5936\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5977 - val_loss: 0.5905\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5948 - val_loss: 0.5886\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5926 - val_loss: 0.5861\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5913 - val_loss: 0.5853\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5905 - val_loss: 0.5837\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5899 - val_loss: 0.5845\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5889 - val_loss: 0.5830\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5884 - val_loss: 0.5826\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5880 - val_loss: 0.5819\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5873 - val_loss: 0.5815\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5874 - val_loss: 0.5813\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5874 - val_loss: 0.5817\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5868 - val_loss: 0.5814\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.5809\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.5809\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.5810\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.5809\n",
      "10\n",
      "minmaxl|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_48 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6740 - val_loss: 0.6346\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6191 - val_loss: 0.6035\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6040 - val_loss: 0.5945\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5980 - val_loss: 0.5919\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5950 - val_loss: 0.5876\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5928 - val_loss: 0.5856\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5915 - val_loss: 0.5864\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5899 - val_loss: 0.5827\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5883 - val_loss: 0.5835\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5872 - val_loss: 0.5805\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5866 - val_loss: 0.5798\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5860 - val_loss: 0.5789\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5849 - val_loss: 0.5805\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5856 - val_loss: 0.5821\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5847 - val_loss: 0.5791\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5842 - val_loss: 0.5790\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5833 - val_loss: 0.5772\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5841 - val_loss: 0.5812\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5839 - val_loss: 0.5770\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5835 - val_loss: 0.5759\n",
      "11\n",
      "maxabsb|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_54 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6516 - val_loss: 0.6125\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6092 - val_loss: 0.5980\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6017 - val_loss: 0.5940\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5988 - val_loss: 0.5905\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5953 - val_loss: 0.5882\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5931 - val_loss: 0.5851\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5904 - val_loss: 0.5833\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5893 - val_loss: 0.5842\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5877 - val_loss: 0.5822\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5880 - val_loss: 0.5833\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.5816\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5875 - val_loss: 0.5833\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.5794\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5857 - val_loss: 0.5819\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5851 - val_loss: 0.5788\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5851 - val_loss: 0.5804\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5846 - val_loss: 0.5789\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5839 - val_loss: 0.5800\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5846 - val_loss: 0.5779\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5838 - val_loss: 0.5787\n",
      "12\n",
      "minmaxd|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_59 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6738 - val_loss: 0.6428\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6192 - val_loss: 0.6028\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6055 - val_loss: 0.5972\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6013 - val_loss: 0.5983\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5996 - val_loss: 0.5943\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5984 - val_loss: 0.5928\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5968 - val_loss: 0.5911\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5959 - val_loss: 0.5906\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5942 - val_loss: 0.5885\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5928 - val_loss: 0.5865\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5926 - val_loss: 0.5866\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5908 - val_loss: 0.5838\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5887 - val_loss: 0.5831\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5885 - val_loss: 0.5818\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5870 - val_loss: 0.5807\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5867 - val_loss: 0.5812\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5858 - val_loss: 0.5797\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5853 - val_loss: 0.5793\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5854 - val_loss: 0.5817\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5852 - val_loss: 0.5829\n",
      "13\n",
      "maxabsg|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_64 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6787 - val_loss: 0.6567\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6386 - val_loss: 0.6174\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6120 - val_loss: 0.6008\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6029 - val_loss: 0.5955\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5994 - val_loss: 0.5931\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5977 - val_loss: 0.5911\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5957 - val_loss: 0.5899\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5954 - val_loss: 0.5890\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5936 - val_loss: 0.5875\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5930 - val_loss: 0.5885\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5922 - val_loss: 0.5861\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5911 - val_loss: 0.5848\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5903 - val_loss: 0.5843\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5900 - val_loss: 0.5836\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5895 - val_loss: 0.5832\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5887 - val_loss: 0.5823\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5882 - val_loss: 0.5829\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5876 - val_loss: 0.5822\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5872 - val_loss: 0.5812\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5866 - val_loss: 0.5809\n",
      "14\n",
      "maxabsP|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6930 - val_loss: 0.6926\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6914 - val_loss: 0.6845\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6458 - val_loss: 0.6076\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6096 - val_loss: 0.5975\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6015 - val_loss: 0.5906\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5948 - val_loss: 0.5862\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5911 - val_loss: 0.5824\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5883 - val_loss: 0.5807\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5887 - val_loss: 0.5815\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.5792\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.5815\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5849 - val_loss: 0.5817\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5856 - val_loss: 0.5778\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5842 - val_loss: 0.5802\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5840 - val_loss: 0.5780\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5833 - val_loss: 0.5825\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5840 - val_loss: 0.5776\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5828 - val_loss: 0.5773\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5819 - val_loss: 0.5805\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5821 - val_loss: 0.5772\n",
      "15\n",
      "maxabsQ|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_72 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6567 - val_loss: 0.6079\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6051 - val_loss: 0.5933\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5962 - val_loss: 0.5876\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5921 - val_loss: 0.5867\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5890 - val_loss: 0.5835\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5875 - val_loss: 0.5819\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5854 - val_loss: 0.5792\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5860 - val_loss: 0.5797\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5863 - val_loss: 0.5790\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5832 - val_loss: 0.5780\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5827 - val_loss: 0.5839\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5835 - val_loss: 0.5770\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5821 - val_loss: 0.5791\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5822 - val_loss: 0.5765\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5807 - val_loss: 0.5765\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5800 - val_loss: 0.5764\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5807 - val_loss: 0.5761\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5793 - val_loss: 0.5855\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5829 - val_loss: 0.5760\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5782 - val_loss: 0.5790\n",
      "16\n",
      "robustx|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_78 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6986 - val_loss: 0.6508\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6477 - val_loss: 0.6377\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6335 - val_loss: 0.6267\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6256 - val_loss: 0.6218\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6190 - val_loss: 0.6157\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6126 - val_loss: 0.6094\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6085 - val_loss: 0.6050\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6051 - val_loss: 0.6024\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6026 - val_loss: 0.5984\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6005 - val_loss: 0.5969\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5979 - val_loss: 0.5932\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5962 - val_loss: 0.5916\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5942 - val_loss: 0.5911\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5934 - val_loss: 0.5890\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5915 - val_loss: 0.5902\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5910 - val_loss: 0.5872\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5897 - val_loss: 0.5880\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5891 - val_loss: 0.5871\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5878 - val_loss: 0.5848\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5872 - val_loss: 0.5835\n",
      "17\n",
      "robustT|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_85 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6699 - val_loss: 0.6489\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6397 - val_loss: 0.6261\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6243 - val_loss: 0.6152\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6166 - val_loss: 0.6082\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6102 - val_loss: 0.6032\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6058 - val_loss: 0.5994\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6026 - val_loss: 0.5968\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6004 - val_loss: 0.5952\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5987 - val_loss: 0.5941\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5975 - val_loss: 0.5934\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5962 - val_loss: 0.5923\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5951 - val_loss: 0.5914\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5942 - val_loss: 0.5904\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5933 - val_loss: 0.5895\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5925 - val_loss: 0.5891\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5917 - val_loss: 0.5883\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5911 - val_loss: 0.5874\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5904 - val_loss: 0.5867\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5898 - val_loss: 0.5865\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5894 - val_loss: 0.5857\n",
      "18\n",
      "minmaxK|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6813 - val_loss: 0.6528\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6359 - val_loss: 0.6163\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6119 - val_loss: 0.5987\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6007 - val_loss: 0.5898\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5935 - val_loss: 0.5879\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5910 - val_loss: 0.5829\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5894 - val_loss: 0.5864\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5907 - val_loss: 0.5809\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5873 - val_loss: 0.5810\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.5817\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.5790\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5858 - val_loss: 0.5787\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5854 - val_loss: 0.5792\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5859 - val_loss: 0.5836\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.5781\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5845 - val_loss: 0.5802\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5852 - val_loss: 0.5781\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5848 - val_loss: 0.5780\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5847 - val_loss: 0.5785\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5846 - val_loss: 0.5852\n",
      "19\n",
      "minmaxJ|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_94 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6930 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "20\n",
      "robustt|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_97 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6805 - val_loss: 0.6449\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6238 - val_loss: 0.6112\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6115 - val_loss: 0.6039\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6053 - val_loss: 0.5998\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6012 - val_loss: 0.5979\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5981 - val_loss: 0.5945\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5957 - val_loss: 0.5928\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5939 - val_loss: 0.5908\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5916 - val_loss: 0.5887\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5903 - val_loss: 0.5882\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5892 - val_loss: 0.5866\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5882 - val_loss: 0.5856\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5879 - val_loss: 0.5841\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5862 - val_loss: 0.5828\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5854 - val_loss: 0.5823\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5851 - val_loss: 0.5814\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5841 - val_loss: 0.5805\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5836 - val_loss: 0.5805\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5832 - val_loss: 0.5791\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5821 - val_loss: 0.5784\n",
      "21\n",
      "minmaxO|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_104 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6891 - val_loss: 0.6849\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6813 - val_loss: 0.6752\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6696 - val_loss: 0.6605\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6539 - val_loss: 0.6440\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6411 - val_loss: 0.6325\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6316 - val_loss: 0.6238\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6249 - val_loss: 0.6176\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6200 - val_loss: 0.6131\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6166 - val_loss: 0.6098\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6138 - val_loss: 0.6073\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6119 - val_loss: 0.6061\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6103 - val_loss: 0.6039\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6087 - val_loss: 0.6022\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6077 - val_loss: 0.6011\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6065 - val_loss: 0.6000\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6054 - val_loss: 0.5991\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6048 - val_loss: 0.5986\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6038 - val_loss: 0.5974\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6029 - val_loss: 0.5967\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6023 - val_loss: 0.5960\n",
      "22\n",
      "normalizeo|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6688 - val_loss: 0.6270\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6090 - val_loss: 0.5924\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5961 - val_loss: 0.5883\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5921 - val_loss: 0.5885\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5923 - val_loss: 0.5841\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5902 - val_loss: 0.5815\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5876 - val_loss: 0.5887\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5875 - val_loss: 0.5826\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5872 - val_loss: 0.5793\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5871 - val_loss: 0.5806\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5850 - val_loss: 0.5849\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.5821\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5848 - val_loss: 0.5809\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5847 - val_loss: 0.5812\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5856 - val_loss: 0.5779\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5857 - val_loss: 0.5792\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5838 - val_loss: 0.5782\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5840 - val_loss: 0.5771\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5832 - val_loss: 0.5774\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5832 - val_loss: 0.5805\n",
      "23\n",
      "robustN|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_114 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6646 - val_loss: 0.6475\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6438 - val_loss: 0.6372\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6319 - val_loss: 0.6226\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6190 - val_loss: 0.6116\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6108 - val_loss: 0.6053\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6053 - val_loss: 0.6011\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6023 - val_loss: 0.5996\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6004 - val_loss: 0.5972\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5984 - val_loss: 0.5949\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5967 - val_loss: 0.5942\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5963 - val_loss: 0.5936\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5956 - val_loss: 0.5928\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5951 - val_loss: 0.5925\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5943 - val_loss: 0.5924\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5934 - val_loss: 0.5912\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5927 - val_loss: 0.5905\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5924 - val_loss: 0.5901\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5917 - val_loss: 0.5900\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5915 - val_loss: 0.5898\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5911 - val_loss: 0.5888\n",
      "24\n",
      "standardizeb|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_119 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6621 - val_loss: 0.6269\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6095 - val_loss: 0.5932\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5963 - val_loss: 0.5874\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5904 - val_loss: 0.5837\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5872 - val_loss: 0.5809\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5848 - val_loss: 0.5791\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5830 - val_loss: 0.5771\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5809 - val_loss: 0.5750\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5797 - val_loss: 0.5745\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5787 - val_loss: 0.5727\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5774 - val_loss: 0.5731\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5773 - val_loss: 0.5735\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5761 - val_loss: 0.5707\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5752 - val_loss: 0.5706\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5745 - val_loss: 0.5698\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5744 - val_loss: 0.5699\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5737 - val_loss: 0.5705\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5733 - val_loss: 0.5706\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5732 - val_loss: 0.5694\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5730 - val_loss: 0.5691\n",
      "25\n",
      "standardizex|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6930 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "26\n",
      "robustm|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_132 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.7031 - val_loss: 0.6943\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6919 - val_loss: 0.6901\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6861 - val_loss: 0.6812\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6768 - val_loss: 0.6724\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6710 - val_loss: 0.6688\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6683 - val_loss: 0.6670\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6669 - val_loss: 0.6645\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6648 - val_loss: 0.6621\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6624 - val_loss: 0.6590\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6588 - val_loss: 0.6542\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6532 - val_loss: 0.6479\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6467 - val_loss: 0.6421\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6408 - val_loss: 0.6345\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6347 - val_loss: 0.6284\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6297 - val_loss: 0.6234\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6251 - val_loss: 0.6189\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6215 - val_loss: 0.6154\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6184 - val_loss: 0.6124\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6164 - val_loss: 0.6105\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6148 - val_loss: 0.6085\n",
      "27\n",
      "standardizec|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_138 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6953 - val_loss: 0.6906\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6876 - val_loss: 0.6831\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6766 - val_loss: 0.6675\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6573 - val_loss: 0.6449\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6380 - val_loss: 0.6297\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6259 - val_loss: 0.6205\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6186 - val_loss: 0.6148\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6136 - val_loss: 0.6108\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6101 - val_loss: 0.6075\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6070 - val_loss: 0.6047\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6042 - val_loss: 0.6022\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6020 - val_loss: 0.6003\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6002 - val_loss: 0.5985\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5985 - val_loss: 0.5968\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5971 - val_loss: 0.5953\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5960 - val_loss: 0.5942\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5951 - val_loss: 0.5933\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5943 - val_loss: 0.5925\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5935 - val_loss: 0.5918\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5930 - val_loss: 0.5912\n",
      "28\n",
      "maxabsU|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6598 - val_loss: 0.6282\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6192 - val_loss: 0.6033\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6001 - val_loss: 0.5895\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5924 - val_loss: 0.5878\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5897 - val_loss: 0.5856\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5883 - val_loss: 0.5856\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5878 - val_loss: 0.5850\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5854 - val_loss: 0.5832\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5843 - val_loss: 0.5804\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5830 - val_loss: 0.5804\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5851 - val_loss: 0.5821\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5825 - val_loss: 0.5788\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5822 - val_loss: 0.5808\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5836 - val_loss: 0.5789\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5808 - val_loss: 0.5783\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5805 - val_loss: 0.5824\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5808 - val_loss: 0.5775\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5805 - val_loss: 0.5775\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5798 - val_loss: 0.5768\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5793 - val_loss: 0.5782\n",
      "29\n",
      "minmaxX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6734 - val_loss: 0.6339\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6155 - val_loss: 0.6117\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5987 - val_loss: 0.5855\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5925 - val_loss: 0.5949\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5931 - val_loss: 0.5838\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5875 - val_loss: 0.5795\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5867 - val_loss: 0.5768\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5846 - val_loss: 0.5807\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5843 - val_loss: 0.5775\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5840 - val_loss: 0.5842\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5851 - val_loss: 0.5769\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5830 - val_loss: 0.5794\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5823 - val_loss: 0.5817\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5852 - val_loss: 0.5944\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5835 - val_loss: 0.5783\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5847 - val_loss: 0.5753\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5818 - val_loss: 0.5768\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5817 - val_loss: 0.5806\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5834 - val_loss: 0.5815\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5822 - val_loss: 0.5751\n",
      "0\n",
      "robustV|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6999 - val_loss: 0.6924\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6886 - val_loss: 0.6765\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6552 - val_loss: 0.6289\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6154 - val_loss: 0.6004\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5995 - val_loss: 0.5953\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5943 - val_loss: 0.5877\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5899 - val_loss: 0.5846\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5877 - val_loss: 0.5831\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5857 - val_loss: 0.5815\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5842 - val_loss: 0.5806\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5833 - val_loss: 0.5795\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5825 - val_loss: 0.5786\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5814 - val_loss: 0.5779\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5808 - val_loss: 0.5781\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5802 - val_loss: 0.5765\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5799 - val_loss: 0.5761\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5794 - val_loss: 0.5750\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5785 - val_loss: 0.5748\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5789 - val_loss: 0.5742\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5780 - val_loss: 0.5754\n",
      "1\n",
      "normalizeg|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6930 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "2\n",
      "robustC|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_163 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.7902 - val_loss: 0.6665\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6439 - val_loss: 0.6255\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6214 - val_loss: 0.6122\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6123 - val_loss: 0.6041\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6065 - val_loss: 0.6002\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6033 - val_loss: 0.5983\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6016 - val_loss: 0.5966\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5999 - val_loss: 0.5953\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5985 - val_loss: 0.5942\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5972 - val_loss: 0.5933\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5962 - val_loss: 0.5926\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5952 - val_loss: 0.5913\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5944 - val_loss: 0.5909\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5935 - val_loss: 0.5902\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5926 - val_loss: 0.5893\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5920 - val_loss: 0.5888\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5912 - val_loss: 0.5882\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5908 - val_loss: 0.5871\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5899 - val_loss: 0.5866\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5893 - val_loss: 0.5858\n",
      "3\n",
      "maxabsK|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6808 - val_loss: 0.6406\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6135 - val_loss: 0.5906\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5918 - val_loss: 0.5816\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5873 - val_loss: 0.5799\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5844 - val_loss: 0.5783\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5837 - val_loss: 0.5815\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5826 - val_loss: 0.5762\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5820 - val_loss: 0.5782\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5821 - val_loss: 0.5755\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5815 - val_loss: 0.5753\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5809 - val_loss: 0.5770\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5819 - val_loss: 0.5737\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5803 - val_loss: 0.5756\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5800 - val_loss: 0.5770\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5802 - val_loss: 0.5762\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5799 - val_loss: 0.5744\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 0.5738\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5793 - val_loss: 0.5733\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5789 - val_loss: 0.5771\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5788 - val_loss: 0.5734\n",
      "4\n",
      "maxabsT|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_174 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6800 - val_loss: 0.6558\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6374 - val_loss: 0.6160\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6130 - val_loss: 0.6036\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6048 - val_loss: 0.5976\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5994 - val_loss: 0.5930\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5951 - val_loss: 0.5892\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5914 - val_loss: 0.5863\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5885 - val_loss: 0.5838\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5863 - val_loss: 0.5822\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5850 - val_loss: 0.5806\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5833 - val_loss: 0.5795\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5824 - val_loss: 0.5785\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5814 - val_loss: 0.5776\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5806 - val_loss: 0.5766\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5801 - val_loss: 0.5759\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5794 - val_loss: 0.5755\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5787 - val_loss: 0.5759\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5785 - val_loss: 0.5746\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5778 - val_loss: 0.5742\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5774 - val_loss: 0.5741\n",
      "5\n",
      "robustD|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_177 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6888 - val_loss: 0.6790\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6671 - val_loss: 0.6524\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6457 - val_loss: 0.6363\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6358 - val_loss: 0.6281\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6298 - val_loss: 0.6223\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6250 - val_loss: 0.6177\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6211 - val_loss: 0.6137\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6177 - val_loss: 0.6104\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6147 - val_loss: 0.6069\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6118 - val_loss: 0.6036\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6095 - val_loss: 0.6018\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6075 - val_loss: 0.6001\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6053 - val_loss: 0.5969\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6006 - val_loss: 0.5930\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5976 - val_loss: 0.5910\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5959 - val_loss: 0.5902\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5946 - val_loss: 0.5885\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5932 - val_loss: 0.5877\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5927 - val_loss: 0.5875\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5916 - val_loss: 0.5868\n",
      "6\n",
      "normalizev|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_183 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6653 - val_loss: 0.6122\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6076 - val_loss: 0.5955\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5952 - val_loss: 0.5899\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5897 - val_loss: 0.5858\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5865 - val_loss: 0.5814\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5852 - val_loss: 0.5856\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5838 - val_loss: 0.5799\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5821 - val_loss: 0.5780\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5802 - val_loss: 0.5774\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5808 - val_loss: 0.5768\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5789 - val_loss: 0.5747\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5789 - val_loss: 0.5738\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5785 - val_loss: 0.5754\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5785 - val_loss: 0.5734\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5778 - val_loss: 0.5725\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5776 - val_loss: 0.5724\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5771 - val_loss: 0.5815\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5791 - val_loss: 0.5749\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5758 - val_loss: 0.5716\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5752 - val_loss: 0.5728\n",
      "7\n",
      "minmaxF|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6931 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6926\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6924 - val_loss: 0.6915\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6886 - val_loss: 0.6816\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6634 - val_loss: 0.6388\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6264 - val_loss: 0.6129\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6111 - val_loss: 0.6029\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6037 - val_loss: 0.5967\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5994 - val_loss: 0.5935\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5970 - val_loss: 0.5914\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5949 - val_loss: 0.5898\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5936 - val_loss: 0.5889\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5925 - val_loss: 0.5889\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5919 - val_loss: 0.5872\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5914 - val_loss: 0.5866\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5910 - val_loss: 0.5873\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5904 - val_loss: 0.5863\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5904 - val_loss: 0.5859\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5897 - val_loss: 0.5851\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5897 - val_loss: 0.5850\n",
      "8\n",
      "normalizej|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6831 - val_loss: 0.6642\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6449 - val_loss: 0.6232\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6152 - val_loss: 0.6018\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6036 - val_loss: 0.5953\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5994 - val_loss: 0.5932\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5971 - val_loss: 0.5912\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5956 - val_loss: 0.5896\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5938 - val_loss: 0.5885\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5928 - val_loss: 0.5887\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5922 - val_loss: 0.5869\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5913 - val_loss: 0.5861\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5914 - val_loss: 0.5860\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5902 - val_loss: 0.5856\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5899 - val_loss: 0.5845\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5893 - val_loss: 0.5844\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5891 - val_loss: 0.5834\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5883 - val_loss: 0.5833\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5895 - val_loss: 0.5847\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5880 - val_loss: 0.5815\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5872 - val_loss: 0.5817\n",
      "9\n",
      "standardizeY|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_197 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6988 - val_loss: 0.6678\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6407 - val_loss: 0.6118\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6076 - val_loss: 0.5980\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5994 - val_loss: 0.5928\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5953 - val_loss: 0.5903\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5925 - val_loss: 0.5878\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5904 - val_loss: 0.5859\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5886 - val_loss: 0.5846\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5872 - val_loss: 0.5834\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5861 - val_loss: 0.5822\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5849 - val_loss: 0.5814\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5843 - val_loss: 0.5804\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5834 - val_loss: 0.5797\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5826 - val_loss: 0.5785\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5817 - val_loss: 0.5784\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5811 - val_loss: 0.5775\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5805 - val_loss: 0.5772\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5801 - val_loss: 0.5765\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5794 - val_loss: 0.5768\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5791 - val_loss: 0.5759\n",
      "10\n",
      "minmaxB|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.6935 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "11\n",
      "standardizeL|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_207 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6738 - val_loss: 0.6470\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6314 - val_loss: 0.6120\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6084 - val_loss: 0.5982\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5998 - val_loss: 0.5923\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5945 - val_loss: 0.5886\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5914 - val_loss: 0.5861\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5890 - val_loss: 0.5840\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5875 - val_loss: 0.5826\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5859 - val_loss: 0.5815\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5849 - val_loss: 0.5805\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5840 - val_loss: 0.5794\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5831 - val_loss: 0.5789\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5825 - val_loss: 0.5782\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5817 - val_loss: 0.5775\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5811 - val_loss: 0.5769\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5805 - val_loss: 0.5763\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5801 - val_loss: 0.5757\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5796 - val_loss: 0.5758\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5793 - val_loss: 0.5751\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5786 - val_loss: 0.5748\n",
      "12\n",
      "normalizeS|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.6929 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "13\n",
      "normalizeT|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6918 - val_loss: 0.6830\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6604 - val_loss: 0.6366\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6309 - val_loss: 0.6211\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6194 - val_loss: 0.6120\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6113 - val_loss: 0.6068\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6071 - val_loss: 0.6024\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6022 - val_loss: 0.5985\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5990 - val_loss: 0.5964\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5961 - val_loss: 0.5925\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5937 - val_loss: 0.5907\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5926 - val_loss: 0.5907\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5907 - val_loss: 0.5870\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5891 - val_loss: 0.5878\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5886 - val_loss: 0.5871\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.5872\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5873 - val_loss: 0.5826\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5853 - val_loss: 0.5822\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5849 - val_loss: 0.5811\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5837 - val_loss: 0.5806\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5837 - val_loss: 0.5797\n",
      "14\n",
      "robustt|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6671 - val_loss: 0.6457\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6394 - val_loss: 0.6300\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6295 - val_loss: 0.6221\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6247 - val_loss: 0.6216\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6223 - val_loss: 0.6189\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6213 - val_loss: 0.6171\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6190 - val_loss: 0.6168\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6174 - val_loss: 0.6167\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6177 - val_loss: 0.6159\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6155 - val_loss: 0.6151\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6146 - val_loss: 0.6125\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6136 - val_loss: 0.6110\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6121 - val_loss: 0.6128\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6112 - val_loss: 0.6077\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6086 - val_loss: 0.6063\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6076 - val_loss: 0.6028\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6031 - val_loss: 0.5989\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6002 - val_loss: 0.5952\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5986 - val_loss: 0.5934\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5973 - val_loss: 0.5930\n",
      "15\n",
      "maxabsM|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6930 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "16\n",
      "robustl|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6457 - val_loss: 0.6150\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6043 - val_loss: 0.5840\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5870 - val_loss: 0.5752\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5806 - val_loss: 0.5755\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 0.5747\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5775 - val_loss: 0.5734\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5766 - val_loss: 0.5724\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5757 - val_loss: 0.5714\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5751 - val_loss: 0.5731\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5742 - val_loss: 0.5715\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5741 - val_loss: 0.5705\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5748 - val_loss: 0.5726\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5734 - val_loss: 0.5700\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5723 - val_loss: 0.5697\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5720 - val_loss: 0.5737\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5739 - val_loss: 0.5702\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5715 - val_loss: 0.5715\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5725 - val_loss: 0.5693\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5719 - val_loss: 0.5681\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5716 - val_loss: 0.5712\n",
      "17\n",
      "robustY|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_239 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3558 - val_loss: 0.9574\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.7863 - val_loss: 0.6944\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6739 - val_loss: 0.6551\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6482 - val_loss: 0.6392\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6357 - val_loss: 0.6302\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6277 - val_loss: 0.6233\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6216 - val_loss: 0.6179\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6169 - val_loss: 0.6140\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6133 - val_loss: 0.6114\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6107 - val_loss: 0.6087\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6082 - val_loss: 0.6064\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6062 - val_loss: 0.6043\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6044 - val_loss: 0.6022\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6028 - val_loss: 0.6007\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6012 - val_loss: 0.5989\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6003 - val_loss: 0.5975\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5992 - val_loss: 0.5966\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5984 - val_loss: 0.5960\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5978 - val_loss: 0.5950\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5971 - val_loss: 0.5940\n",
      "18\n",
      "minmaxp|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_242 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6647 - val_loss: 0.6305\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6161 - val_loss: 0.5997\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6006 - val_loss: 0.5933\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5965 - val_loss: 0.5941\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5955 - val_loss: 0.5916\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5946 - val_loss: 0.5896\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5932 - val_loss: 0.5884\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5926 - val_loss: 0.5878\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5909 - val_loss: 0.5878\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5897 - val_loss: 0.5856\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5889 - val_loss: 0.5846\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5885 - val_loss: 0.5863\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5871 - val_loss: 0.5827\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5866 - val_loss: 0.5815\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5860 - val_loss: 0.5831\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5857 - val_loss: 0.5806\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5844 - val_loss: 0.5799\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5843 - val_loss: 0.5792\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5837 - val_loss: 0.5788\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5834 - val_loss: 0.5786\n",
      "19\n",
      "standardized|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_246 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6652 - val_loss: 0.6127\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6085 - val_loss: 0.5966\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5972 - val_loss: 0.5901\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5918 - val_loss: 0.5862\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5885 - val_loss: 0.5833\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5865 - val_loss: 0.5815\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5846 - val_loss: 0.5794\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5827 - val_loss: 0.5789\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5814 - val_loss: 0.5780\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5807 - val_loss: 0.5759\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5799 - val_loss: 0.5751\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5784 - val_loss: 0.5751\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5780 - val_loss: 0.5744\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5774 - val_loss: 0.5739\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5766 - val_loss: 0.5735\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5758 - val_loss: 0.5742\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5755 - val_loss: 0.5726\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5746 - val_loss: 0.5722\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5744 - val_loss: 0.5723\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5743 - val_loss: 0.5712\n",
      "20\n",
      "robustP|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6921 - val_loss: 0.6911\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6896 - val_loss: 0.6866\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6816 - val_loss: 0.6735\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6644 - val_loss: 0.6517\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6445 - val_loss: 0.6338\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6317 - val_loss: 0.6240\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6250 - val_loss: 0.6185\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6208 - val_loss: 0.6147\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6179 - val_loss: 0.6120\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6156 - val_loss: 0.6098\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6139 - val_loss: 0.6079\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6122 - val_loss: 0.6064\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6108 - val_loss: 0.6050\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6095 - val_loss: 0.6039\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6085 - val_loss: 0.6024\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6071 - val_loss: 0.6014\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6061 - val_loss: 0.6004\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6049 - val_loss: 0.5995\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6041 - val_loss: 0.5985\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6030 - val_loss: 0.5975\n",
      "21\n",
      "normalizeD|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6923 - val_loss: 0.6866\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6555 - val_loss: 0.6172\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5995 - val_loss: 0.5850\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5872 - val_loss: 0.5864\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5882 - val_loss: 0.5780\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5836 - val_loss: 0.5765\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5846 - val_loss: 0.5756\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5815 - val_loss: 0.5749\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5807 - val_loss: 0.5734\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5805 - val_loss: 0.5739\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5809 - val_loss: 0.5740\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5799 - val_loss: 0.5747\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5802 - val_loss: 0.5718\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 0.5721\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5797 - val_loss: 0.5730\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5801 - val_loss: 0.5734\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5790 - val_loss: 0.5724\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5785 - val_loss: 0.5742\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5791 - val_loss: 0.5735\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5784 - val_loss: 0.5709\n",
      "22\n",
      "maxabsI|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_262 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6845 - val_loss: 0.6726\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6560 - val_loss: 0.6340\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6212 - val_loss: 0.6090\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6075 - val_loss: 0.6016\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6027 - val_loss: 0.5980\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5993 - val_loss: 0.5955\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5975 - val_loss: 0.5931\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5955 - val_loss: 0.5912\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5939 - val_loss: 0.5900\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5930 - val_loss: 0.5886\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5917 - val_loss: 0.5879\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5907 - val_loss: 0.5875\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5901 - val_loss: 0.5859\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5894 - val_loss: 0.5848\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5885 - val_loss: 0.5842\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5880 - val_loss: 0.5835\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5876 - val_loss: 0.5833\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5873 - val_loss: 0.5828\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5867 - val_loss: 0.5824\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5864 - val_loss: 0.5826\n",
      "23\n",
      "minmaxZ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6818 - val_loss: 0.6433\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6142 - val_loss: 0.5893\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5933 - val_loss: 0.5878\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5882 - val_loss: 0.5796\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5856 - val_loss: 0.5785\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5852 - val_loss: 0.5793\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5845 - val_loss: 0.5819\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5859 - val_loss: 0.5792\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5850 - val_loss: 0.5766\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5821 - val_loss: 0.5764\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5819 - val_loss: 0.5815\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5830 - val_loss: 0.5784\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5808 - val_loss: 0.5770\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5814 - val_loss: 0.5792\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5803 - val_loss: 0.5823\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5805 - val_loss: 0.5860\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5825 - val_loss: 0.5744\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5801 - val_loss: 0.5744\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5787 - val_loss: 0.5755\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5791 - val_loss: 0.5740\n",
      "24\n",
      "minmaxb|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6790 - val_loss: 0.6641\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6525 - val_loss: 0.6356\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6274 - val_loss: 0.6135\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6127 - val_loss: 0.6018\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6049 - val_loss: 0.5958\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6000 - val_loss: 0.5923\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5976 - val_loss: 0.5893\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5946 - val_loss: 0.5872\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5923 - val_loss: 0.5873\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5907 - val_loss: 0.5843\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5900 - val_loss: 0.5839\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5886 - val_loss: 0.5871\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5882 - val_loss: 0.5835\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5870 - val_loss: 0.5847\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5868 - val_loss: 0.5818\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5858 - val_loss: 0.5813\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5855 - val_loss: 0.5805\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5853 - val_loss: 0.5801\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5846 - val_loss: 0.5797\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5843 - val_loss: 0.5796\n",
      "25\n",
      "normalizec|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6840 - val_loss: 0.6649\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6341 - val_loss: 0.6089\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6040 - val_loss: 0.5960\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5973 - val_loss: 0.5919\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5942 - val_loss: 0.5907\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5929 - val_loss: 0.5903\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5916 - val_loss: 0.5879\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5894 - val_loss: 0.5851\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5886 - val_loss: 0.5841\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5890 - val_loss: 0.5854\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5882 - val_loss: 0.5851\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5878 - val_loss: 0.5836\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5873 - val_loss: 0.5829\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5873 - val_loss: 0.5823\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5866 - val_loss: 0.5826\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5872 - val_loss: 0.5820\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5866 - val_loss: 0.5820\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.5816\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5856 - val_loss: 0.5846\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5859 - val_loss: 0.5812\n",
      "26\n",
      "minmaxa|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6687 - val_loss: 0.6294\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6199 - val_loss: 0.6020\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5997 - val_loss: 0.5927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5902 - val_loss: 0.5803\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5861 - val_loss: 0.5780\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5821 - val_loss: 0.5761\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5822 - val_loss: 0.5769\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5811 - val_loss: 0.5738\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5791 - val_loss: 0.5770\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5796 - val_loss: 0.5760\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5805 - val_loss: 0.5731\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5784 - val_loss: 0.5730\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5786 - val_loss: 0.5738\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5810 - val_loss: 0.5726\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5773 - val_loss: 0.5715\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5770 - val_loss: 0.5728\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5780 - val_loss: 0.5724\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5773 - val_loss: 0.5811\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5769 - val_loss: 0.5717\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5760 - val_loss: 0.5757\n",
      "27\n",
      "robustF|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_286 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6577 - val_loss: 0.6193\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6068 - val_loss: 0.5950\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5950 - val_loss: 0.5881\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5903 - val_loss: 0.5836\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5858 - val_loss: 0.5803\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5827 - val_loss: 0.5797\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5818 - val_loss: 0.5780\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5803 - val_loss: 0.5766\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5792 - val_loss: 0.5766\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5778 - val_loss: 0.5753\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5772 - val_loss: 0.5750\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5766 - val_loss: 0.5749\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5763 - val_loss: 0.5731\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5754 - val_loss: 0.5738\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5746 - val_loss: 0.5734\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5745 - val_loss: 0.5724\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5740 - val_loss: 0.5741\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5737 - val_loss: 0.5718\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5733 - val_loss: 0.5713\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5724 - val_loss: 0.5710\n",
      "28\n",
      "robustT|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_291 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6985 - val_loss: 0.6679\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6543 - val_loss: 0.6418\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6344 - val_loss: 0.6253\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6218 - val_loss: 0.6143\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6130 - val_loss: 0.6053\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6057 - val_loss: 0.5992\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6006 - val_loss: 0.5946\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5969 - val_loss: 0.5917\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5941 - val_loss: 0.5891\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5922 - val_loss: 0.5868\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5900 - val_loss: 0.5849\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5883 - val_loss: 0.5835\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5872 - val_loss: 0.5823\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5857 - val_loss: 0.5811\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5847 - val_loss: 0.5796\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5837 - val_loss: 0.5786\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5831 - val_loss: 0.5781\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5827 - val_loss: 0.5771\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5819 - val_loss: 0.5768\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5812 - val_loss: 0.5764\n",
      "29\n",
      "robustN|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_295 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6614 - val_loss: 0.6224\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6080 - val_loss: 0.5960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5941 - val_loss: 0.5907\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5897 - val_loss: 0.5861\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5868 - val_loss: 0.5832\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5848 - val_loss: 0.5817\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5834 - val_loss: 0.5802\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5829 - val_loss: 0.5800\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5816 - val_loss: 0.5782\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5806 - val_loss: 0.5766\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5799 - val_loss: 0.5765\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5798 - val_loss: 0.5753\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5784 - val_loss: 0.5746\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5780 - val_loss: 0.5747\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5783 - val_loss: 0.5740\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5774 - val_loss: 0.5738\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5773 - val_loss: 0.5734\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5771 - val_loss: 0.5729\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5761 - val_loss: 0.5730\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5757 - val_loss: 0.5725\n",
      "0\n",
      "maxabsT|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6930 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6925 - val_loss: 0.6917\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6869 - val_loss: 0.6770\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6628 - val_loss: 0.6456\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6383 - val_loss: 0.6281\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6269 - val_loss: 0.6198\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6209 - val_loss: 0.6151\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6173 - val_loss: 0.6116\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6143 - val_loss: 0.6089\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6120 - val_loss: 0.6067\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6101 - val_loss: 0.6050\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6085 - val_loss: 0.6034\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6067 - val_loss: 0.6023\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6057 - val_loss: 0.6012\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6045 - val_loss: 0.6002\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6035 - val_loss: 0.5994\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6026 - val_loss: 0.5985\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6019 - val_loss: 0.5978\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6013 - val_loss: 0.5970\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6006 - val_loss: 0.5965\n",
      "1\n",
      "robustp|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6963 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6927 - val_loss: 0.6925\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6923 - val_loss: 0.6914\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6810 - val_loss: 0.6593\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6418 - val_loss: 0.6265\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6233 - val_loss: 0.6123\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6149 - val_loss: 0.6048\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6086 - val_loss: 0.6007\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6045 - val_loss: 0.5975\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6009 - val_loss: 0.5923\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5980 - val_loss: 0.5899\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5955 - val_loss: 0.5878\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5942 - val_loss: 0.5874\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5926 - val_loss: 0.5846\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5916 - val_loss: 0.5834\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5902 - val_loss: 0.5824\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5890 - val_loss: 0.5811\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5879 - val_loss: 0.5801\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5875 - val_loss: 0.5807\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5861 - val_loss: 0.5785\n",
      "2\n",
      "robustW|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "3\n",
      "normalizeW|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_312 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6668 - val_loss: 0.6350\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6219 - val_loss: 0.6057\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6021 - val_loss: 0.5948\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5947 - val_loss: 0.5899\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5918 - val_loss: 0.5872\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5889 - val_loss: 0.5870\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5885 - val_loss: 0.5875\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.5845\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5853 - val_loss: 0.5828\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5839 - val_loss: 0.5817\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5833 - val_loss: 0.5813\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5831 - val_loss: 0.5805\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5822 - val_loss: 0.5807\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5820 - val_loss: 0.5808\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5823 - val_loss: 0.5809\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5809 - val_loss: 0.5795\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5803 - val_loss: 0.5783\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5797 - val_loss: 0.5818\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5798 - val_loss: 0.5789\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5794 - val_loss: 0.5780\n",
      "4\n",
      "standardizec|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_318 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6803 - val_loss: 0.6300\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6211 - val_loss: 0.6073\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6072 - val_loss: 0.5985\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6002 - val_loss: 0.5932\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5954 - val_loss: 0.5893\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5920 - val_loss: 0.5867\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5896 - val_loss: 0.5845\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5873 - val_loss: 0.5829\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5860 - val_loss: 0.5817\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5847 - val_loss: 0.5806\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5836 - val_loss: 0.5790\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5822 - val_loss: 0.5787\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5819 - val_loss: 0.5778\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5809 - val_loss: 0.5765\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5802 - val_loss: 0.5761\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5795 - val_loss: 0.5755\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5789 - val_loss: 0.5749\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5784 - val_loss: 0.5754\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5781 - val_loss: 0.5741\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5772 - val_loss: 0.5734\n",
      "5\n",
      "normalizem|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6879 - val_loss: 0.6796\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6641 - val_loss: 0.6435\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6340 - val_loss: 0.6230\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6219 - val_loss: 0.6139\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6153 - val_loss: 0.6085\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6100 - val_loss: 0.6025\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6061 - val_loss: 0.6010\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6042 - val_loss: 0.5986\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6018 - val_loss: 0.5938\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5987 - val_loss: 0.5922\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5976 - val_loss: 0.5902\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5960 - val_loss: 0.5901\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5955 - val_loss: 0.5877\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5938 - val_loss: 0.5866\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5933 - val_loss: 0.5859\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5924 - val_loss: 0.5854\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5913 - val_loss: 0.5852\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5905 - val_loss: 0.5842\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5904 - val_loss: 0.5842\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5899 - val_loss: 0.5828\n",
      "6\n",
      "normalizeg|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_326 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6841 - val_loss: 0.6712\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6546 - val_loss: 0.6341\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6229 - val_loss: 0.6088\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6073 - val_loss: 0.6000\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6012 - val_loss: 0.5945\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5969 - val_loss: 0.5913\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5936 - val_loss: 0.5898\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5917 - val_loss: 0.5872\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5901 - val_loss: 0.5858\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5889 - val_loss: 0.5852\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5882 - val_loss: 0.5840\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5872 - val_loss: 0.5832\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5866 - val_loss: 0.5826\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5862 - val_loss: 0.5821\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5855 - val_loss: 0.5815\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5851 - val_loss: 0.5810\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5847 - val_loss: 0.5813\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5841 - val_loss: 0.5800\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5837 - val_loss: 0.5797\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5832 - val_loss: 0.5792\n",
      "7\n",
      "normalizef|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6930 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "8\n",
      "minmaxm|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6742 - val_loss: 0.6366\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6169 - val_loss: 0.6056\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5989 - val_loss: 0.5908\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5946 - val_loss: 0.5878\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5930 - val_loss: 0.5882\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5910 - val_loss: 0.5828\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5882 - val_loss: 0.5857\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5873 - val_loss: 0.5828\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5861 - val_loss: 0.5802\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5846 - val_loss: 0.5786\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5854 - val_loss: 0.5780\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5853 - val_loss: 0.5790\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5831 - val_loss: 0.5773\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5821 - val_loss: 0.5774\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5818 - val_loss: 0.5785\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5818 - val_loss: 0.5768\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5815 - val_loss: 0.5816\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5825 - val_loss: 0.5766\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5810 - val_loss: 0.5760\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5806 - val_loss: 0.5761\n",
      "9\n",
      "standardizei|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_339 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6998 - val_loss: 0.6931\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6902 - val_loss: 0.6868\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6832 - val_loss: 0.6777\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6710 - val_loss: 0.6616\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6532 - val_loss: 0.6423\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6368 - val_loss: 0.6270\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6252 - val_loss: 0.6174\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6179 - val_loss: 0.6109\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6124 - val_loss: 0.6047\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6070 - val_loss: 0.5993\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6025 - val_loss: 0.5949\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5986 - val_loss: 0.5916\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5961 - val_loss: 0.5895\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5943 - val_loss: 0.5880\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5930 - val_loss: 0.5866\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5918 - val_loss: 0.5854\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5910 - val_loss: 0.5848\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5902 - val_loss: 0.5841\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5895 - val_loss: 0.5835\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5890 - val_loss: 0.5830\n",
      "10\n",
      "standardizeO|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 0.6930 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6921 - val_loss: 0.6881\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6654 - val_loss: 0.6263\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6097 - val_loss: 0.5940\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5953 - val_loss: 0.5879\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5912 - val_loss: 0.5853\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5888 - val_loss: 0.5858\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5880 - val_loss: 0.5821\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.5810\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5851 - val_loss: 0.5800\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5858 - val_loss: 0.5799\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5838 - val_loss: 0.5790\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5835 - val_loss: 0.5784\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5829 - val_loss: 0.5786\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5823 - val_loss: 0.5794\n",
      "11\n",
      "minmaxR|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6590 - val_loss: 0.6312\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6137 - val_loss: 0.5933\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5939 - val_loss: 0.5949\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5898 - val_loss: 0.5909\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5903 - val_loss: 0.5807\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5888 - val_loss: 0.5773\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5848 - val_loss: 0.5763\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5844 - val_loss: 0.5755\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5828 - val_loss: 0.5752\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5824 - val_loss: 0.5857\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5834 - val_loss: 0.5751\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5832 - val_loss: 0.5845\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5833 - val_loss: 0.5883\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5833 - val_loss: 0.5745\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5813 - val_loss: 0.5740\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5807 - val_loss: 0.5730\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5827 - val_loss: 0.5753\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5811 - val_loss: 0.5809\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5820 - val_loss: 0.5756\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5801 - val_loss: 0.5728\n",
      "12\n",
      "robustM|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_353 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6618 - val_loss: 0.6119\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6066 - val_loss: 0.5913\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5949 - val_loss: 0.5852\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5901 - val_loss: 0.5816\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5870 - val_loss: 0.5803\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5859 - val_loss: 0.5792\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5835 - val_loss: 0.5784\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5827 - val_loss: 0.5774\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5815 - val_loss: 0.5766\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5807 - val_loss: 0.5760\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5801 - val_loss: 0.5745\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5796 - val_loss: 0.5745\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5792 - val_loss: 0.5740\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5786 - val_loss: 0.5736\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5781 - val_loss: 0.5731\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5774 - val_loss: 0.5729\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5770 - val_loss: 0.5724\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5771 - val_loss: 0.5721\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5765 - val_loss: 0.5718\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5764 - val_loss: 0.5718\n",
      "13\n",
      "standardizem|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6939 - val_loss: 0.6906\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6821 - val_loss: 0.6646\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6474 - val_loss: 0.6288\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6244 - val_loss: 0.6147\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6154 - val_loss: 0.6076\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6101 - val_loss: 0.6030\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6061 - val_loss: 0.5995\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6033 - val_loss: 0.5967\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6011 - val_loss: 0.5945\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5989 - val_loss: 0.5927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5974 - val_loss: 0.5912\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5961 - val_loss: 0.5901\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5949 - val_loss: 0.5890\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5938 - val_loss: 0.5879\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5929 - val_loss: 0.5872\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5922 - val_loss: 0.5867\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5915 - val_loss: 0.5861\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5909 - val_loss: 0.5855\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5904 - val_loss: 0.5851\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5899 - val_loss: 0.5848\n",
      "14\n",
      "maxabsK|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6931 - val_loss: 0.6922\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6681 - val_loss: 0.6305\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6209 - val_loss: 0.6031\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6013 - val_loss: 0.5919\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5938 - val_loss: 0.5861\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5927 - val_loss: 0.5840\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5895 - val_loss: 0.5817\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5881 - val_loss: 0.5821\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5870 - val_loss: 0.5828\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5867 - val_loss: 0.5789\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5850 - val_loss: 0.5818\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5843 - val_loss: 0.5828\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5840 - val_loss: 0.5816\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5838 - val_loss: 0.5782\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5830 - val_loss: 0.5771\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5815 - val_loss: 0.5759\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5831 - val_loss: 0.5761\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5812 - val_loss: 0.5753\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5805 - val_loss: 0.5777\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5800 - val_loss: 0.5749\n",
      "15\n",
      "maxabsO|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_369 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6521 - val_loss: 0.6160\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6059 - val_loss: 0.5918\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5922 - val_loss: 0.5854\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5886 - val_loss: 0.5849\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.5799\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5831 - val_loss: 0.5802\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5818 - val_loss: 0.5794\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5807 - val_loss: 0.5763\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5797 - val_loss: 0.5784\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5805 - val_loss: 0.5780\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5787 - val_loss: 0.5774\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5777 - val_loss: 0.5753\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5776 - val_loss: 0.5746\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5764 - val_loss: 0.5744\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5764 - val_loss: 0.5746\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5756 - val_loss: 0.5736\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5751 - val_loss: 0.5727\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5754 - val_loss: 0.5728\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5748 - val_loss: 0.5728\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5740 - val_loss: 0.5727\n",
      "16\n",
      "normalized|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6924 - val_loss: 0.6916\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6893 - val_loss: 0.6835\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6726 - val_loss: 0.6581\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6475 - val_loss: 0.6343\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6283 - val_loss: 0.6198\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6176 - val_loss: 0.6112\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6113 - val_loss: 0.6067\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6079 - val_loss: 0.6041\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6055 - val_loss: 0.6016\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6033 - val_loss: 0.5999\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6015 - val_loss: 0.5984\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5999 - val_loss: 0.5966\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5984 - val_loss: 0.5954\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5972 - val_loss: 0.5941\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5960 - val_loss: 0.5928\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5950 - val_loss: 0.5929\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5939 - val_loss: 0.5907\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5930 - val_loss: 0.5896\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5921 - val_loss: 0.5889\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5914 - val_loss: 0.5878\n",
      "17\n",
      "maxabsu|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6436 - val_loss: 0.6123\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6039 - val_loss: 0.5836\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5888 - val_loss: 0.5792\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5849 - val_loss: 0.5754\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5828 - val_loss: 0.5761\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5835 - val_loss: 0.5746\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5808 - val_loss: 0.5741\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5803 - val_loss: 0.5738\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5791 - val_loss: 0.5731\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5787 - val_loss: 0.5725\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5779 - val_loss: 0.5716\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5768 - val_loss: 0.5731\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5777 - val_loss: 0.5711\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5768 - val_loss: 0.5738\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5764 - val_loss: 0.5718\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5751 - val_loss: 0.5741\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5753 - val_loss: 0.5706\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5752 - val_loss: 0.5720\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5745 - val_loss: 0.5736\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5755 - val_loss: 0.5717\n",
      "18\n",
      "standardizek|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6609 - val_loss: 0.6251\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6083 - val_loss: 0.5911\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5907 - val_loss: 0.5814\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5853 - val_loss: 0.5787\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5830 - val_loss: 0.5774\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5814 - val_loss: 0.5766\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5804 - val_loss: 0.5752\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5798 - val_loss: 0.5748\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5798 - val_loss: 0.5744\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5787 - val_loss: 0.5735\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5779 - val_loss: 0.5735\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5775 - val_loss: 0.5734\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5772 - val_loss: 0.5729\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5770 - val_loss: 0.5734\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5767 - val_loss: 0.5726\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5767 - val_loss: 0.5727\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5760 - val_loss: 0.5727\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5760 - val_loss: 0.5729\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5757 - val_loss: 0.5725\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5756 - val_loss: 0.5726\n",
      "19\n",
      "normalizeh|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6932 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "20\n",
      "minmaxS|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6764 - val_loss: 0.6412\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6230 - val_loss: 0.6116\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6040 - val_loss: 0.5917\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5926 - val_loss: 0.5873\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5889 - val_loss: 0.5832\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.5800\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5853 - val_loss: 0.5792\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5844 - val_loss: 0.5789\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5837 - val_loss: 0.5769\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5834 - val_loss: 0.5766\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5824 - val_loss: 0.5764\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5820 - val_loss: 0.5765\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5816 - val_loss: 0.5761\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5812 - val_loss: 0.5808\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5841 - val_loss: 0.5767\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5815 - val_loss: 0.5753\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5804 - val_loss: 0.5757\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5810 - val_loss: 0.5746\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5803 - val_loss: 0.5754\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5806 - val_loss: 0.5753\n",
      "21\n",
      "minmaxI|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_398 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6843 - val_loss: 0.6666\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6439 - val_loss: 0.6218\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6187 - val_loss: 0.6087\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6102 - val_loss: 0.6036\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6067 - val_loss: 0.6014\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6043 - val_loss: 0.6003\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6043 - val_loss: 0.5978\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6018 - val_loss: 0.5970\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6018 - val_loss: 0.5970\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6009 - val_loss: 0.5960\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6003 - val_loss: 0.5954\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6006 - val_loss: 0.5957\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5992 - val_loss: 0.5957\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5993 - val_loss: 0.5951\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5986 - val_loss: 0.5953\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5983 - val_loss: 0.5955\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5983 - val_loss: 0.5941\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5975 - val_loss: 0.5940\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5978 - val_loss: 0.5955\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5975 - val_loss: 0.5933\n",
      "22\n",
      "minmaxL|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_405 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6723 - val_loss: 0.6289\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6030 - val_loss: 0.5844\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5901 - val_loss: 0.5839\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5886 - val_loss: 0.5804\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5873 - val_loss: 0.5804\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.5914\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5876 - val_loss: 0.5824\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5871 - val_loss: 0.5792\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5847 - val_loss: 0.5804\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5844 - val_loss: 0.5775\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5833 - val_loss: 0.5766\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5824 - val_loss: 0.5777\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5824 - val_loss: 0.5801\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5832 - val_loss: 0.5828\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5828 - val_loss: 0.5777\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5815 - val_loss: 0.5770\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5816 - val_loss: 0.5756\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5813 - val_loss: 0.5762\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5805 - val_loss: 0.5746\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5805 - val_loss: 0.5752\n",
      "23\n",
      "standardizet|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6606 - val_loss: 0.6170\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6037 - val_loss: 0.5860\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5873 - val_loss: 0.5835\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5837 - val_loss: 0.5768\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5807 - val_loss: 0.5738\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5789 - val_loss: 0.5776\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5780 - val_loss: 0.5727\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5770 - val_loss: 0.5736\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5758 - val_loss: 0.5736\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5757 - val_loss: 0.5718\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5742 - val_loss: 0.5721\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5734 - val_loss: 0.5705\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5739 - val_loss: 0.5711\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5738 - val_loss: 0.5700\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5726 - val_loss: 0.5766\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5724 - val_loss: 0.5693\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5714 - val_loss: 0.5701\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5717 - val_loss: 0.5684\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5708 - val_loss: 0.5691\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5715 - val_loss: 0.5731\n",
      "24\n",
      "maxabsI|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "25\n",
      "normalizeL|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6930 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6928\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "26\n",
      "minmaxq|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6824 - val_loss: 0.6583\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6370 - val_loss: 0.6213\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6137 - val_loss: 0.6073\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5999 - val_loss: 0.5894\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5920 - val_loss: 0.5829\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5894 - val_loss: 0.5787\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5858 - val_loss: 0.5772\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5846 - val_loss: 0.5780\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5844 - val_loss: 0.5878\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5852 - val_loss: 0.5885\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5851 - val_loss: 0.5928\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5856 - val_loss: 0.5779\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5828 - val_loss: 0.5833\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5823 - val_loss: 0.5760\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5821 - val_loss: 0.5738\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5817 - val_loss: 0.5740\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5817 - val_loss: 0.5793\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5819 - val_loss: 0.5740\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5809 - val_loss: 0.5726\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5834 - val_loss: 0.5809\n",
      "27\n",
      "minmaxs|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_436 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6915 - val_loss: 0.6887\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6841 - val_loss: 0.6771\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6691 - val_loss: 0.6590\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6556 - val_loss: 0.6486\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6469 - val_loss: 0.6408\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6392 - val_loss: 0.6321\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6296 - val_loss: 0.6229\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6226 - val_loss: 0.6175\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6181 - val_loss: 0.6139\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6151 - val_loss: 0.6108\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6131 - val_loss: 0.6094\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6126 - val_loss: 0.6075\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6102 - val_loss: 0.6068\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6089 - val_loss: 0.6056\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6082 - val_loss: 0.6048\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6072 - val_loss: 0.6041\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6068 - val_loss: 0.6028\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6059 - val_loss: 0.6022\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6058 - val_loss: 0.6017\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6049 - val_loss: 0.6012\n",
      "28\n",
      "standardizeX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6927 - val_loss: 0.6878\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6739 - val_loss: 0.6562\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6413 - val_loss: 0.6234\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6147 - val_loss: 0.6027\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6025 - val_loss: 0.5945\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5973 - val_loss: 0.5931\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5933 - val_loss: 0.5895\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5912 - val_loss: 0.5839\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5882 - val_loss: 0.5879\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5870 - val_loss: 0.5817\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5870 - val_loss: 0.5793\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5850 - val_loss: 0.5817\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5838 - val_loss: 0.5784\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5832 - val_loss: 0.5782\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5827 - val_loss: 0.5771\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.5825 - val_loss: 0.5768\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5813 - val_loss: 0.5758\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5812 - val_loss: 0.5775\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5816 - val_loss: 0.5746\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5803 - val_loss: 0.5747\n",
      "29\n",
      "robustx|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6889 - val_loss: 0.6773\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6541 - val_loss: 0.6239\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6082 - val_loss: 0.5911\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5906 - val_loss: 0.5844\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.5842\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5853 - val_loss: 0.5784\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5848 - val_loss: 0.5779\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5834 - val_loss: 0.5778\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5819 - val_loss: 0.5796\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5810 - val_loss: 0.5770\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5820 - val_loss: 0.5774\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5810 - val_loss: 0.5770\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5799 - val_loss: 0.5758\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5799 - val_loss: 0.5784\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5804 - val_loss: 0.5752\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5791 - val_loss: 0.5781\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 0.5761\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5802 - val_loss: 0.5751\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5785 - val_loss: 0.5754\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5785 - val_loss: 0.5740\n",
      "0\n",
      "minmaxd|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.7120 - val_loss: 0.6934\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6931 - val_loss: 0.6928\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6926\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6927 - val_loss: 0.6926\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6927 - val_loss: 0.6925\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6926 - val_loss: 0.6925\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6925 - val_loss: 0.6922\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6919 - val_loss: 0.6908\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6857 - val_loss: 0.6755\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6596 - val_loss: 0.6413\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6324 - val_loss: 0.6216\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6198 - val_loss: 0.6126\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6126 - val_loss: 0.6072\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6081 - val_loss: 0.6036\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6051 - val_loss: 0.5998\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6022 - val_loss: 0.5972\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6004 - val_loss: 0.5955\n",
      "1\n",
      "standardizeb|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6930 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "2\n",
      "standardizeA|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6576 - val_loss: 0.6246\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6119 - val_loss: 0.5956\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5949 - val_loss: 0.5861\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5884 - val_loss: 0.5836\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5873 - val_loss: 0.5800\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5833 - val_loss: 0.5782\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5813 - val_loss: 0.5781\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5806 - val_loss: 0.5775\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5798 - val_loss: 0.5756\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5786 - val_loss: 0.5746\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5781 - val_loss: 0.5760\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5774 - val_loss: 0.5761\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5769 - val_loss: 0.5765\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5766 - val_loss: 0.5716\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5752 - val_loss: 0.5785\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5765 - val_loss: 0.5747\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5749 - val_loss: 0.5715\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5748 - val_loss: 0.5698\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5741 - val_loss: 0.5708\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5733 - val_loss: 0.5706\n",
      "3\n",
      "standardizec|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_460 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6535 - val_loss: 0.6097\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6024 - val_loss: 0.5920\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5920 - val_loss: 0.5846\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5876 - val_loss: 0.5811\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5848 - val_loss: 0.5782\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5821 - val_loss: 0.5761\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5806 - val_loss: 0.5746\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5787 - val_loss: 0.5739\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5778 - val_loss: 0.5734\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5770 - val_loss: 0.5716\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5758 - val_loss: 0.5733\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5752 - val_loss: 0.5719\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5744 - val_loss: 0.5729\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5738 - val_loss: 0.5701\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5734 - val_loss: 0.5720\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5738 - val_loss: 0.5695\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5728 - val_loss: 0.5695\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5719 - val_loss: 0.5688\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5713 - val_loss: 0.5691\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5712 - val_loss: 0.5679\n",
      "4\n",
      "standardizeV|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6937 - val_loss: 0.6866\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6686 - val_loss: 0.6438\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6268 - val_loss: 0.6136\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6096 - val_loss: 0.6042\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6046 - val_loss: 0.5993\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6010 - val_loss: 0.5964\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5983 - val_loss: 0.5934\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5958 - val_loss: 0.5912\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5941 - val_loss: 0.5890\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5921 - val_loss: 0.5869\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5901 - val_loss: 0.5856\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5886 - val_loss: 0.5832\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5869 - val_loss: 0.5812\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5852 - val_loss: 0.5793\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5845 - val_loss: 0.5785\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5832 - val_loss: 0.5774\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5829 - val_loss: 0.5762\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5817 - val_loss: 0.5752\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5812 - val_loss: 0.5749\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5807 - val_loss: 0.5744\n",
      "5\n",
      "standardizeq|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6525 - val_loss: 0.5991\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5894 - val_loss: 0.5770\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5786 - val_loss: 0.5752\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5761 - val_loss: 0.5727\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5744 - val_loss: 0.5712\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5749 - val_loss: 0.5705\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5729 - val_loss: 0.5730\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5726 - val_loss: 0.5750\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5724 - val_loss: 0.5715\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5724 - val_loss: 0.5710\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5714 - val_loss: 0.5703\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5717 - val_loss: 0.5724\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5718 - val_loss: 0.5740\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5702 - val_loss: 0.5700\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5700 - val_loss: 0.5703\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5700 - val_loss: 0.5825\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5713 - val_loss: 0.5704\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5694 - val_loss: 0.5713\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5703 - val_loss: 0.5768\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5691 - val_loss: 0.5700\n",
      "6\n",
      "maxabsG|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6930 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "7\n",
      "standardizeP|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6611 - val_loss: 0.6048\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5880 - val_loss: 0.5748\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5733 - val_loss: 0.5701\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5729 - val_loss: 0.5679\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5704 - val_loss: 0.5714\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5696 - val_loss: 0.5763\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5704 - val_loss: 0.5699\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5712 - val_loss: 0.5675\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5703 - val_loss: 0.5753\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5705 - val_loss: 0.5682\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5686 - val_loss: 0.5684\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5682 - val_loss: 0.5685\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5673 - val_loss: 0.5739\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5679 - val_loss: 0.5663\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5675 - val_loss: 0.5691\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5672 - val_loss: 0.5686\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5669 - val_loss: 0.5679\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5671 - val_loss: 0.5662\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5663 - val_loss: 0.5666\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5664 - val_loss: 0.5672\n",
      "8\n",
      "robustF|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6928\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "9\n",
      "minmaxz|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6635 - val_loss: 0.6248\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6116 - val_loss: 0.5923\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5912 - val_loss: 0.5915\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5860 - val_loss: 0.5767\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5825 - val_loss: 0.5782\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5829 - val_loss: 0.5775\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5818 - val_loss: 0.5786\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5824 - val_loss: 0.5756\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5813 - val_loss: 0.5804\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5809 - val_loss: 0.5778\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5803 - val_loss: 0.5746\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5805 - val_loss: 0.5747\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5804 - val_loss: 0.5917\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5805 - val_loss: 0.5746\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5798 - val_loss: 0.5764\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5796 - val_loss: 0.5767\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5797 - val_loss: 0.5800\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5804 - val_loss: 0.5794\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5798 - val_loss: 0.5747\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 0.5750\n",
      "10\n",
      "robustx|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6937 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "11\n",
      "normalizeE|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6989 - val_loss: 0.6953\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6942 - val_loss: 0.6934\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6933 - val_loss: 0.6930\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6930 - val_loss: 0.6929\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6929 - val_loss: 0.6928\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6929 - val_loss: 0.6928\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6929 - val_loss: 0.6928\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "12\n",
      "maxabsB|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6695 - val_loss: 0.6160\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6034 - val_loss: 0.5860\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5874 - val_loss: 0.5776\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5837 - val_loss: 0.5774\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5851 - val_loss: 0.5746\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5818 - val_loss: 0.5746\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5813 - val_loss: 0.5756\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5794 - val_loss: 0.5902\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5824 - val_loss: 0.5735\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5780 - val_loss: 0.5742\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5786 - val_loss: 0.5786\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5772 - val_loss: 0.5716\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5767 - val_loss: 0.5714\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5760 - val_loss: 0.5748\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5758 - val_loss: 0.5716\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5759 - val_loss: 0.5717\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5772 - val_loss: 0.5719\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5751 - val_loss: 0.5727\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5741 - val_loss: 0.5860\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5766 - val_loss: 0.5714\n",
      "13\n",
      "maxabsm|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6940 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "14\n",
      "standardizeW|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.6620 - val_loss: 0.6282\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6215 - val_loss: 0.6066\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6001 - val_loss: 0.5983\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5912 - val_loss: 0.5808\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5871 - val_loss: 0.5801\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5827 - val_loss: 0.5760\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5828 - val_loss: 0.5794\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5821 - val_loss: 0.6028\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5813 - val_loss: 0.5766\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5787 - val_loss: 0.5735\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5795 - val_loss: 0.5736\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5775 - val_loss: 0.5747\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5774 - val_loss: 0.5728\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5783 - val_loss: 0.5727\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5791 - val_loss: 0.5776\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5775 - val_loss: 0.5758\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5775 - val_loss: 0.5715\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5757 - val_loss: 0.5791\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5769 - val_loss: 0.5756\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5756 - val_loss: 0.5749\n",
      "15\n",
      "normalizeS|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6923 - val_loss: 0.6906\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6862 - val_loss: 0.6793\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6718 - val_loss: 0.6614\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6532 - val_loss: 0.6432\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6373 - val_loss: 0.6290\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6252 - val_loss: 0.6187\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6175 - val_loss: 0.6118\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6121 - val_loss: 0.6074\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6081 - val_loss: 0.6039\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6052 - val_loss: 0.6008\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6030 - val_loss: 0.5983\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6007 - val_loss: 0.5963\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5989 - val_loss: 0.5944\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5973 - val_loss: 0.5931\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5964 - val_loss: 0.5919\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5951 - val_loss: 0.5911\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5943 - val_loss: 0.5900\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5933 - val_loss: 0.5889\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5926 - val_loss: 0.5887\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5919 - val_loss: 0.5879\n",
      "16\n",
      "minmaxY|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6496 - val_loss: 0.5991\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5983 - val_loss: 0.5983\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5891 - val_loss: 0.5840\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5845 - val_loss: 0.5796\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5820 - val_loss: 0.5785\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5821 - val_loss: 0.5771\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5799 - val_loss: 0.5817\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5802 - val_loss: 0.5890\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5806 - val_loss: 0.5753\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5780 - val_loss: 0.5743\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5779 - val_loss: 0.5798\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5774 - val_loss: 0.5756\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5778 - val_loss: 0.5868\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5785 - val_loss: 0.5775\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5766 - val_loss: 0.5775\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5770 - val_loss: 0.5732\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5760 - val_loss: 0.5726\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5774 - val_loss: 0.5829\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5755 - val_loss: 0.5730\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5750 - val_loss: 0.5744\n",
      "17\n",
      "normalizeK|rf\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6927 - val_loss: 0.6920\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6853 - val_loss: 0.6653\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6455 - val_loss: 0.6291\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6261 - val_loss: 0.6154\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6150 - val_loss: 0.6061\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6080 - val_loss: 0.6010\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6043 - val_loss: 0.5993\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6025 - val_loss: 0.5978\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6022 - val_loss: 0.5968\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6010 - val_loss: 0.5966\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6005 - val_loss: 0.5963\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6000 - val_loss: 0.5972\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6000 - val_loss: 0.5957\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5995 - val_loss: 0.5986\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6001 - val_loss: 0.5962\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5994 - val_loss: 0.5950\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5986 - val_loss: 0.5946\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5989 - val_loss: 0.5945\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5988 - val_loss: 0.5944\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5983 - val_loss: 0.5949\n",
      "18\n",
      "maxabsi|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_539 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6592 - val_loss: 0.6087\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6088 - val_loss: 0.5960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6002 - val_loss: 0.5908\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5964 - val_loss: 0.5879\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5932 - val_loss: 0.5886\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5893 - val_loss: 0.5927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5902 - val_loss: 0.5810\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.6110\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5899 - val_loss: 0.5792\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5858 - val_loss: 0.5790\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5842 - val_loss: 0.5785\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5838 - val_loss: 0.5776\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5839 - val_loss: 0.5773\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5828 - val_loss: 0.5822\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5830 - val_loss: 0.5774\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5820 - val_loss: 0.5789\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5829 - val_loss: 0.5757\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5811 - val_loss: 0.5791\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5806 - val_loss: 0.5744\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5809 - val_loss: 0.5752\n",
      "19\n",
      "standardizeU|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6735 - val_loss: 0.6353\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6183 - val_loss: 0.5993\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5960 - val_loss: 0.5835\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5850 - val_loss: 0.5774\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5798 - val_loss: 0.5751\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5782 - val_loss: 0.5739\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5776 - val_loss: 0.5738\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5766 - val_loss: 0.5734\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5775 - val_loss: 0.5726\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5757 - val_loss: 0.5731\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5753 - val_loss: 0.5727\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5749 - val_loss: 0.5717\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5749 - val_loss: 0.5714\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5748 - val_loss: 0.5724\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5740 - val_loss: 0.5717\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5743 - val_loss: 0.5712\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5738 - val_loss: 0.5726\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5737 - val_loss: 0.5724\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5741 - val_loss: 0.5728\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5737 - val_loss: 0.5711\n",
      "20\n",
      "robustX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6867 - val_loss: 0.6761\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6578 - val_loss: 0.6354\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6282 - val_loss: 0.6192\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6123 - val_loss: 0.6050\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6031 - val_loss: 0.5946\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5950 - val_loss: 0.5885\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5914 - val_loss: 0.5866\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5891 - val_loss: 0.5900\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5871 - val_loss: 0.5840\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5850 - val_loss: 0.5806\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5821 - val_loss: 0.5818\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5815 - val_loss: 0.5809\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5801 - val_loss: 0.5794\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5809 - val_loss: 0.5762\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5787 - val_loss: 0.5759\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5780 - val_loss: 0.5771\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5791 - val_loss: 0.5781\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5774 - val_loss: 0.5761\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5772 - val_loss: 0.5745\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5765 - val_loss: 0.5746\n",
      "21\n",
      "robustm|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6910 - val_loss: 0.6880\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6808 - val_loss: 0.6655\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6524 - val_loss: 0.6377\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6297 - val_loss: 0.6187\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6158 - val_loss: 0.6085\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6092 - val_loss: 0.6034\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6057 - val_loss: 0.6004\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6032 - val_loss: 0.5987\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6012 - val_loss: 0.5972\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5999 - val_loss: 0.5961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5986 - val_loss: 0.5946\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5970 - val_loss: 0.5932\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5959 - val_loss: 0.5920\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5946 - val_loss: 0.5917\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5938 - val_loss: 0.5906\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5931 - val_loss: 0.5897\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5925 - val_loss: 0.5896\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5917 - val_loss: 0.5895\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5914 - val_loss: 0.5883\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5910 - val_loss: 0.5882\n",
      "22\n",
      "minmaxw|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6568 - val_loss: 0.6071\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5954 - val_loss: 0.5888\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5891 - val_loss: 0.5916\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5875 - val_loss: 0.5804\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5845 - val_loss: 0.5782\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5835 - val_loss: 0.5789\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5830 - val_loss: 0.5853\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5864 - val_loss: 0.5778\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5832 - val_loss: 0.5798\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5823 - val_loss: 0.5783\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5828 - val_loss: 0.5897\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5855 - val_loss: 0.5803\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5818 - val_loss: 0.5775\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5818 - val_loss: 0.5768\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5821 - val_loss: 0.5769\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5812 - val_loss: 0.5766\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5823 - val_loss: 0.5760\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5811 - val_loss: 0.5807\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5832 - val_loss: 0.5762\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5803 - val_loss: 0.5756\n",
      "23\n",
      "maxabso|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6850 - val_loss: 0.6607\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6391 - val_loss: 0.6175\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6106 - val_loss: 0.5940\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5953 - val_loss: 0.5836\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5881 - val_loss: 0.5811\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5875 - val_loss: 0.5821\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5853 - val_loss: 0.5779\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5839 - val_loss: 0.5769\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5843 - val_loss: 0.5795\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5826 - val_loss: 0.5777\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5830 - val_loss: 0.5769\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5820 - val_loss: 0.5828\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5841 - val_loss: 0.5756\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5813 - val_loss: 0.5747\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5808 - val_loss: 0.5897\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5812 - val_loss: 0.5775\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5804 - val_loss: 0.5789\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5802 - val_loss: 0.5756\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5805 - val_loss: 0.5810\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5816 - val_loss: 0.5745\n",
      "24\n",
      "maxabsV|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6929 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "25\n",
      "standardized|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6384 - val_loss: 0.5936\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.5764\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5772 - val_loss: 0.5760\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5750 - val_loss: 0.5702\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5738 - val_loss: 0.5690\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5726 - val_loss: 0.5682\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5721 - val_loss: 0.5717\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5724 - val_loss: 0.5692\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5713 - val_loss: 0.5675\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5706 - val_loss: 0.5694\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5711 - val_loss: 0.5675\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5700 - val_loss: 0.5682\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5697 - val_loss: 0.5675\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5689 - val_loss: 0.5707\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5697 - val_loss: 0.5730\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5700 - val_loss: 0.5675\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5691 - val_loss: 0.5673\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5681 - val_loss: 0.5703\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5691 - val_loss: 0.5681\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5675 - val_loss: 0.5709\n",
      "26\n",
      "standardizea|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6793 - val_loss: 0.6574\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6301 - val_loss: 0.6101\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6036 - val_loss: 0.5980\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5956 - val_loss: 0.5892\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5905 - val_loss: 0.5847\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5867 - val_loss: 0.5810\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5849 - val_loss: 0.5784\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5832 - val_loss: 0.5769\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5815 - val_loss: 0.5765\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5813 - val_loss: 0.5752\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5791 - val_loss: 0.5763\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5782 - val_loss: 0.5756\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5776 - val_loss: 0.5769\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5784 - val_loss: 0.5750\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5764 - val_loss: 0.5733\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5757 - val_loss: 0.5734\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5749 - val_loss: 0.5746\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5761 - val_loss: 0.5726\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5750 - val_loss: 0.5749\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5746 - val_loss: 0.5739\n",
      "27\n",
      "standardizey|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6536 - val_loss: 0.6059\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5966 - val_loss: 0.5859\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5866 - val_loss: 0.5825\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5835 - val_loss: 0.5771\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5811 - val_loss: 0.5824\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5815 - val_loss: 0.5747\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5780 - val_loss: 0.5743\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5776 - val_loss: 0.5739\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5776 - val_loss: 0.5733\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5754 - val_loss: 0.5726\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5749 - val_loss: 0.5733\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5748 - val_loss: 0.5737\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5731 - val_loss: 0.5717\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5730 - val_loss: 0.5706\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5723 - val_loss: 0.5704\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5724 - val_loss: 0.5705\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5722 - val_loss: 0.5708\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5708 - val_loss: 0.5717\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5714 - val_loss: 0.5712\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5701 - val_loss: 0.5715\n",
      "28\n",
      "normalized|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6472 - val_loss: 0.6010\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5915 - val_loss: 0.5820\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5818 - val_loss: 0.5753\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 0.5743\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5784 - val_loss: 0.5741\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5757 - val_loss: 0.5719\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5753 - val_loss: 0.5725\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5749 - val_loss: 0.5723\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5745 - val_loss: 0.5707\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5739 - val_loss: 0.5705\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5724 - val_loss: 0.5709\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5722 - val_loss: 0.5736\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5730 - val_loss: 0.5709\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5713 - val_loss: 0.5700\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5710 - val_loss: 0.5710\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5728 - val_loss: 0.5705\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5713 - val_loss: 0.5709\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5722 - val_loss: 0.5699\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5706 - val_loss: 0.5697\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5704 - val_loss: 0.5717\n",
      "29\n",
      "normalizeg|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6930 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "0\n",
      "normalizem|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_598 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6701 - val_loss: 0.6287\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6112 - val_loss: 0.5992\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5974 - val_loss: 0.5910\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5922 - val_loss: 0.5868\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5887 - val_loss: 0.5835\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5864 - val_loss: 0.5808\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5841 - val_loss: 0.5786\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5825 - val_loss: 0.5777\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5815 - val_loss: 0.5769\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5807 - val_loss: 0.5760\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5796 - val_loss: 0.5760\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5795 - val_loss: 0.5746\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5786 - val_loss: 0.5748\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5781 - val_loss: 0.5734\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5774 - val_loss: 0.5731\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5769 - val_loss: 0.5728\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5766 - val_loss: 0.5726\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5759 - val_loss: 0.5722\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5758 - val_loss: 0.5721\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5751 - val_loss: 0.5728\n",
      "1\n",
      "robustK|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.6856 - val_loss: 0.6523\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6178 - val_loss: 0.5912\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.5858 - val_loss: 0.5829\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5837 - val_loss: 0.5739\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5798 - val_loss: 0.5946\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5796 - val_loss: 0.5796\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.5790 - val_loss: 0.5723\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5762 - val_loss: 0.5724\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5752 - val_loss: 0.5716\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5749 - val_loss: 0.5745\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.5767 - val_loss: 0.5735\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.5763 - val_loss: 0.5723\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.5733 - val_loss: 0.5756\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.5730 - val_loss: 0.5706\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.5754 - val_loss: 0.5759\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.5732 - val_loss: 0.5706\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5731 - val_loss: 0.5717\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5738 - val_loss: 0.5708\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5728 - val_loss: 0.5747\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5738 - val_loss: 0.5703\n",
      "2\n",
      "maxabsF|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6933 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6926\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6898 - val_loss: 0.6784\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6548 - val_loss: 0.6244\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6109 - val_loss: 0.5960\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5951 - val_loss: 0.5856\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5883 - val_loss: 0.5806\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5844 - val_loss: 0.5773\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5817 - val_loss: 0.5755\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5805 - val_loss: 0.5764\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5799 - val_loss: 0.5733\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5784 - val_loss: 0.5729\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5774 - val_loss: 0.5721\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5772 - val_loss: 0.5738\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5764 - val_loss: 0.5719\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5764 - val_loss: 0.5716\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5752 - val_loss: 0.5705\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5758 - val_loss: 0.5715\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5746 - val_loss: 0.5706\n",
      "3\n",
      "robustD|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6928\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "4\n",
      "minmaxh|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6945 - val_loss: 0.6931\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "5\n",
      "minmaxV|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "6\n",
      "minmaxP|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6936 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "7\n",
      "robustx|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.6905 - val_loss: 0.6838\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6597 - val_loss: 0.6256\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6131 - val_loss: 0.6025\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.5972 - val_loss: 0.5896\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.5889 - val_loss: 0.5836\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5839 - val_loss: 0.5838\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5818 - val_loss: 0.5863\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5786 - val_loss: 0.5792\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5779 - val_loss: 0.5846\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5778 - val_loss: 0.5779\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5757 - val_loss: 0.5757\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5749 - val_loss: 0.5753\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5743 - val_loss: 0.5763\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5742 - val_loss: 0.5764\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5739 - val_loss: 0.5769\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5740 - val_loss: 0.5755\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5746 - val_loss: 0.5837\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5762 - val_loss: 0.5753\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5733 - val_loss: 0.5732\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5720 - val_loss: 0.5752\n",
      "8\n",
      "robustO|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6928 - val_loss: 0.6928\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "9\n",
      "minmaxC|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6930 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6928\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "10\n",
      "maxabsr|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6939 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "11\n",
      "maxabsv|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6930 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "12\n",
      "maxabsY|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "13\n",
      "minmaxX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6538 - val_loss: 0.6161\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6063 - val_loss: 0.5912\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5910 - val_loss: 0.5867\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5870 - val_loss: 0.5801\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5839 - val_loss: 0.5781\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5833 - val_loss: 0.5809\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5822 - val_loss: 0.5765\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5821 - val_loss: 0.5752\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5807 - val_loss: 0.5768\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5804 - val_loss: 0.5756\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5803 - val_loss: 0.5827\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5812 - val_loss: 0.5747\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5793 - val_loss: 0.5745\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5795 - val_loss: 0.5796\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 0.5742\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5784 - val_loss: 0.5740\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5787 - val_loss: 0.5733\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5784 - val_loss: 0.5738\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5790 - val_loss: 0.5731\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5774 - val_loss: 0.5744\n",
      "14\n",
      "normalizeZ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.6932 - val_loss: 0.6927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6928\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "15\n",
      "minmaxi|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6930 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "16\n",
      "normalizeC|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6698 - val_loss: 0.6291\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6141 - val_loss: 0.6050\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5977 - val_loss: 0.5932\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5903 - val_loss: 0.5857\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5915 - val_loss: 0.5805\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5863 - val_loss: 0.5810\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5854 - val_loss: 0.5785\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5842 - val_loss: 0.5854\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5838 - val_loss: 0.5797\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5827 - val_loss: 0.5758\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5817 - val_loss: 0.5820\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5830 - val_loss: 0.5768\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5825 - val_loss: 0.5756\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5805 - val_loss: 0.5753\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5812 - val_loss: 0.5813\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5825 - val_loss: 0.5760\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5808 - val_loss: 0.5744\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5788 - val_loss: 0.5750\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5799 - val_loss: 0.5752\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5786 - val_loss: 0.5776\n",
      "17\n",
      "minmaxM|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6915\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6826 - val_loss: 0.6606\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6317 - val_loss: 0.6059\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6025 - val_loss: 0.5926\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5965 - val_loss: 0.5903\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5924 - val_loss: 0.5856\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5901 - val_loss: 0.5836\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5887 - val_loss: 0.5862\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5892 - val_loss: 0.5815\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5869 - val_loss: 0.5803\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5862 - val_loss: 0.5806\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5859 - val_loss: 0.5797\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5855 - val_loss: 0.5807\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5887 - val_loss: 0.5814\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5843 - val_loss: 0.5788\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5847 - val_loss: 0.5783\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5840 - val_loss: 0.5799\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5839 - val_loss: 0.5810\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5840 - val_loss: 0.5807\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5840 - val_loss: 0.5809\n",
      "18\n",
      "maxabsQ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6930 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "19\n",
      "robustB|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6530 - val_loss: 0.6198\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6111 - val_loss: 0.5955\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5973 - val_loss: 0.5868\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5916 - val_loss: 0.5829\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5888 - val_loss: 0.5826\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5864 - val_loss: 0.5799\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5846 - val_loss: 0.5803\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5840 - val_loss: 0.5780\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5819 - val_loss: 0.5764\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5814 - val_loss: 0.5752\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5803 - val_loss: 0.5787\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5801 - val_loss: 0.5740\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5789 - val_loss: 0.5733\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5783 - val_loss: 0.5729\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5780 - val_loss: 0.5734\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5775 - val_loss: 0.5727\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5770 - val_loss: 0.5725\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5766 - val_loss: 0.5711\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5767 - val_loss: 0.5712\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5757 - val_loss: 0.5713\n",
      "20\n",
      "robustZ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6634 - val_loss: 0.6176\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6086 - val_loss: 0.5878\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5900 - val_loss: 0.5785\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5830 - val_loss: 0.5777\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5806 - val_loss: 0.5719\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5798 - val_loss: 0.5717\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5783 - val_loss: 0.5719\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5790 - val_loss: 0.5727\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5774 - val_loss: 0.5720\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5771 - val_loss: 0.5725\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5766 - val_loss: 0.5719\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5765 - val_loss: 0.5741\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5766 - val_loss: 0.5727\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5763 - val_loss: 0.5757\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5767 - val_loss: 0.5710\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5749 - val_loss: 0.5730\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5749 - val_loss: 0.5696\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5743 - val_loss: 0.5706\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5743 - val_loss: 0.5699\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5746 - val_loss: 0.5711\n",
      "21\n",
      "robusta|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6810 - val_loss: 0.6379\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6026 - val_loss: 0.5738\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5784 - val_loss: 0.5738\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5762 - val_loss: 0.5740\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5760 - val_loss: 0.5824\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5756 - val_loss: 0.5700\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5746 - val_loss: 0.5732\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5750 - val_loss: 0.5690\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5735 - val_loss: 0.5725\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5745 - val_loss: 0.5732\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5745 - val_loss: 0.5680\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5725 - val_loss: 0.5682\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5730 - val_loss: 0.5698\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5728 - val_loss: 0.5675\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5716 - val_loss: 0.5734\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5725 - val_loss: 0.5677\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5721 - val_loss: 0.5684\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5721 - val_loss: 0.5748\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5724 - val_loss: 0.5689\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.5714 - val_loss: 0.5678\n",
      "22\n",
      "robustw|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6845 - val_loss: 0.6625\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6335 - val_loss: 0.6126\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6093 - val_loss: 0.6012\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6032 - val_loss: 0.5965\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5994 - val_loss: 0.5950\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5977 - val_loss: 0.5919\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5950 - val_loss: 0.5899\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5936 - val_loss: 0.5888\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5920 - val_loss: 0.5880\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5904 - val_loss: 0.5863\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5897 - val_loss: 0.5838\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5877 - val_loss: 0.5851\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5872 - val_loss: 0.5817\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5856 - val_loss: 0.5815\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5850 - val_loss: 0.5805\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5840 - val_loss: 0.5796\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5835 - val_loss: 0.5792\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5833 - val_loss: 0.5808\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5834 - val_loss: 0.5789\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5837 - val_loss: 0.5786\n",
      "23\n",
      "robustk|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6816 - val_loss: 0.6740\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6655 - val_loss: 0.6548\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6462 - val_loss: 0.6362\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6311 - val_loss: 0.6244\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6225 - val_loss: 0.6176\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6172 - val_loss: 0.6139\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6141 - val_loss: 0.6104\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6110 - val_loss: 0.6085\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6095 - val_loss: 0.6070\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6083 - val_loss: 0.6061\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6069 - val_loss: 0.6040\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6060 - val_loss: 0.6037\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6049 - val_loss: 0.6031\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6043 - val_loss: 0.6018\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6039 - val_loss: 0.6026\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6037 - val_loss: 0.6003\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6025 - val_loss: 0.6001\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6022 - val_loss: 0.5993\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6018 - val_loss: 0.6028\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6018 - val_loss: 0.5987\n",
      "24\n",
      "standardizeJ|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_715 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6588 - val_loss: 0.6297\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6157 - val_loss: 0.6003\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5998 - val_loss: 0.5940\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5944 - val_loss: 0.5895\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5915 - val_loss: 0.5874\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5888 - val_loss: 0.5860\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5877 - val_loss: 0.5844\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5861 - val_loss: 0.5855\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5849 - val_loss: 0.5867\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5846 - val_loss: 0.5839\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5833 - val_loss: 0.5809\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5823 - val_loss: 0.5809\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5822 - val_loss: 0.5803\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5813 - val_loss: 0.5802\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5804 - val_loss: 0.5787\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5802 - val_loss: 0.5791\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5796 - val_loss: 0.5798\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5795 - val_loss: 0.5779\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5792 - val_loss: 0.5808\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5788 - val_loss: 0.5768\n",
      "25\n",
      "robustl|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.7073 - val_loss: 0.6941\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6908 - val_loss: 0.6894\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6869 - val_loss: 0.6830\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6773 - val_loss: 0.6691\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6602 - val_loss: 0.6488\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6420 - val_loss: 0.6323\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6294 - val_loss: 0.6223\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6216 - val_loss: 0.6164\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6170 - val_loss: 0.6121\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6137 - val_loss: 0.6095\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6114 - val_loss: 0.6072\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6094 - val_loss: 0.6049\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6073 - val_loss: 0.6032\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6055 - val_loss: 0.6016\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6041 - val_loss: 0.6008\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6037 - val_loss: 0.5996\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6017 - val_loss: 0.5970\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.6003 - val_loss: 0.5960\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5990 - val_loss: 0.5947\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.5979 - val_loss: 0.5935\n",
      "26\n",
      "robustN|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6582 - val_loss: 0.6154\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6003 - val_loss: 0.5921\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5854 - val_loss: 0.5785\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5814 - val_loss: 0.5738\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5789 - val_loss: 0.5752\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5781 - val_loss: 0.5800\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5793 - val_loss: 0.5733\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5775 - val_loss: 0.5745\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5761 - val_loss: 0.5818\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5787 - val_loss: 0.5709\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5748 - val_loss: 0.5702\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5749 - val_loss: 0.5731\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5744 - val_loss: 0.5708\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5747 - val_loss: 0.5725\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5757 - val_loss: 0.5715\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5739 - val_loss: 0.5699\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5736 - val_loss: 0.5701\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5738 - val_loss: 0.5700\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5729 - val_loss: 0.5714\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5733 - val_loss: 0.5693\n",
      "27\n",
      "maxabsg|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.6702 - val_loss: 0.6404\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6295 - val_loss: 0.6168\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6147 - val_loss: 0.6030\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6034 - val_loss: 0.5939\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5963 - val_loss: 0.5894\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.5906 - val_loss: 0.5821\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5875 - val_loss: 0.5790\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5855 - val_loss: 0.5770\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5829 - val_loss: 0.5761\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5818 - val_loss: 0.5756\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5811 - val_loss: 0.5740\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5805 - val_loss: 0.5754\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5794 - val_loss: 0.5732\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5783 - val_loss: 0.5726\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5782 - val_loss: 0.5741\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5796 - val_loss: 0.5724\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5779 - val_loss: 0.5735\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5773 - val_loss: 0.5743\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5773 - val_loss: 0.5727\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.5774 - val_loss: 0.5722\n",
      "28\n",
      "minmaxm|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.6929 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "29\n",
      "maxabsm|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.6929 - val_loss: 0.6928\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6928\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6929 - val_loss: 0.6927\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6928\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.6928 - val_loss: 0.6927\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"n_runs\": 5,\n",
    "    \"transforms\": [\"normalize\", \"standardize\", \"robust\", \"maxabs\", \"minmax\"] * 30,\n",
    "    \"metrics\": [\"accuracy\", \"pd\", \"prec\"],\n",
    "    \"random\": True,\n",
    "    \"learners\": [FeedforwardDL(random={'n_layers': (2, 6), 'n_units': (3, 20)}, n_epochs=20)],\n",
    "    \"log_path\": \"./\",\n",
    "    \"data\": [data],\n",
    "    \"name\": \"firefox-acc\"\n",
    "}\n",
    "for _ in range(50):\n",
    "    config[\"learners\"].append(\n",
    "        FeedforwardDL(random={'n_layers': (2, 6), 'n_units': (3, 20)}, n_epochs=20))\n",
    "\n",
    "dodge = DODGE(config)\n",
    "dodge.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = DODGEInterpreter(files=['./firefox-acc.txt'], max_by=0, \n",
    "                          metrics=['accuracy', 'pd', 'prec'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = interp.interpret()['firefox-acc.txt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': array([0.7027027 , 0.70373122, 0.69990286]),\n",
       " 'pd': array([0.73373778, 0.75362319, 0.74081564]),\n",
       " 'prec': array([0.69745835, 0.69154639, 0.69126743])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raise_utils.learners import MulticlassDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(*train_test_split(x, y))\n",
    "data.y_train = np.where(data.y_train < 2, 0, np.where(data.y_train < 6, 1, 2))\n",
    "data.y_test = np.where(data.y_test < 2, 0, np.where(data.y_test < 6, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 2, ..., 2, 1, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.y_train = to_categorical(data.y_train, num_classes=3)\n",
    "data.y_test = to_categorical(data.y_test, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raise_utils.hooks import Hook\n",
    "\n",
    "def top_2(model, x_test, y_test):\n",
    "    probs = model.model.predict(x_test)\n",
    "    best_n = np.argsort(probs, axis=-1)[:, -2:]\n",
    "    correct = 0\n",
    "    total = len(y_test)\n",
    "    \n",
    "    for i, pred in enumerate(best_n):\n",
    "        if np.argmax(y_test[i], axis=-1) in pred:\n",
    "            correct += 1\n",
    "    print('Top-2 accuracy =', round(correct / total, 3))\n",
    "\n",
    "top2_hook = Hook(name='top2', function=top_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x146d6b0d0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 6, 'n_units': 8, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x146d6bf40>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 6, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147628e20>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 6, 'n_units': 11, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147628d30>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 4, 'n_units': 16, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147628040>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 3, 'n_units': 15, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147628af0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 4, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147628160>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 4, 'n_units': 19, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147628ee0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 6, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147628850>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 3, 'n_units': 18, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1477ac2b0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 2, 'n_units': 20, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148fc09d0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 2, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148fc0820>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 3, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148fc0f40>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 3, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148fc0b50>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 6, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x146d704f0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 2, 'n_units': 15, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x146d70040>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 2, 'n_units': 6, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x146d700d0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 5, 'n_units': 12, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x146d70880>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 2, 'n_units': 20, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x146d70ca0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 3, 'n_units': 18, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x146d70a00>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 3, 'n_units': 16, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x146d708b0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 4, 'n_units': 14, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147468790>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 5, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147468520>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 3, 'n_units': 19, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147468220>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 2, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1474683a0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 4, 'n_units': 20, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147468d90>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 4, 'n_units': 4, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147468b20>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 2, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1472cfb50>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 5, 'n_units': 16, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1472cf370>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 3, 'n_units': 11, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1472cfa00>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 4, 'n_units': 12, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1472cf1f0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 2, 'n_units': 12, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1472cfe80>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 5, 'n_units': 9, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1472cf130>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 2, 'n_units': 8, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1472cf040>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 5, 'n_units': 12, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148c7b6a0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 6, 'n_units': 18, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148c7bd60>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 2, 'n_units': 20, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148c7b4c0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 5, 'n_units': 14, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148c7b400>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 3, 'n_units': 6, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148c7bfa0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 3, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148c7bca0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 6, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148c7b490>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 3, 'n_units': 4, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148cbc970>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 5, 'n_units': 6, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148cbc0d0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 5, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148cbc940>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 5, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148cbc340>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 4, 'n_units': 11, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148cbcd00>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 4, 'n_units': 11, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148cbc7f0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 4, 'n_units': 15, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1491ba730>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 4, 'n_units': 14, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1478292e0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 3, 'n_units': 14, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148d2eca0>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 6, 'n_units': 6, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x148d2e460>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 4, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardizeN|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_122 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0362 - val_loss: 0.9970\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9690 - val_loss: 0.9637\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9511 - val_loss: 0.9537\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9441 - val_loss: 0.9485\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9397 - val_loss: 0.9445\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9369 - val_loss: 0.9428\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9345 - val_loss: 0.9398\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9327 - val_loss: 0.9393\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9310 - val_loss: 0.9379\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9299 - val_loss: 0.9370\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9289 - val_loss: 0.9366\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9279 - val_loss: 0.9375\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9270 - val_loss: 0.9350\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9264 - val_loss: 0.9335\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9256 - val_loss: 0.9343\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9254 - val_loss: 0.9334\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9248 - val_loss: 0.9338\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9249 - val_loss: 0.9334\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9249 - val_loss: 0.9325\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9240 - val_loss: 0.9325\n",
      "Top-2 accuracy = 0.86\n",
      "1\n",
      "maxabsU|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_128 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0627 - val_loss: 1.0152\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9877 - val_loss: 0.9817\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9653 - val_loss: 0.9662\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9535 - val_loss: 0.9565\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9463 - val_loss: 0.9505\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9414 - val_loss: 0.9481\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9388 - val_loss: 0.9432\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9360 - val_loss: 0.9415\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9338 - val_loss: 0.9395\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9325 - val_loss: 0.9374\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9306 - val_loss: 0.9358\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9286 - val_loss: 0.9349\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9275 - val_loss: 0.9340\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9261 - val_loss: 0.9326\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9250 - val_loss: 0.9321\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9243 - val_loss: 0.9315\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9230 - val_loss: 0.9299\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9224 - val_loss: 0.9291\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9213 - val_loss: 0.9287\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9208 - val_loss: 0.9282\n",
      "Top-2 accuracy = 0.86\n",
      "2\n",
      "normalizes|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_131 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0969 - val_loss: 1.0959\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0926 - val_loss: 1.0872\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0688 - val_loss: 1.0480\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0210 - val_loss: 1.0012\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9831 - val_loss: 0.9807\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9693 - val_loss: 0.9731\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9634 - val_loss: 0.9680\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9593 - val_loss: 0.9643\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9566 - val_loss: 0.9612\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9539 - val_loss: 0.9603\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9526 - val_loss: 0.9563\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9506 - val_loss: 0.9542\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9495 - val_loss: 0.9535\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9483 - val_loss: 0.9522\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9475 - val_loss: 0.9511\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9467 - val_loss: 0.9508\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9460 - val_loss: 0.9498\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9452 - val_loss: 0.9496\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9447 - val_loss: 0.9484\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9443 - val_loss: 0.9483\n",
      "Top-2 accuracy = 0.85\n",
      "3\n",
      "standardizeG|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_135 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0376 - val_loss: 0.9823\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9678 - val_loss: 0.9582\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9511 - val_loss: 0.9500\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9427 - val_loss: 0.9452\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9374 - val_loss: 0.9418\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9339 - val_loss: 0.9402\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9314 - val_loss: 0.9393\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9298 - val_loss: 0.9368\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9282 - val_loss: 0.9354\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9263 - val_loss: 0.9348\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9250 - val_loss: 0.9337\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9237 - val_loss: 0.9328\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9228 - val_loss: 0.9322\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9216 - val_loss: 0.9315\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9206 - val_loss: 0.9304\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9199 - val_loss: 0.9290\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9190 - val_loss: 0.9288\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9176 - val_loss: 0.9281\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9170 - val_loss: 0.9282\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9165 - val_loss: 0.9277\n",
      "Top-2 accuracy = 0.85\n",
      "4\n",
      "robustu|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_139 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.1167 - val_loss: 1.0809\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0468 - val_loss: 1.0061\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9771 - val_loss: 0.9670\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9548 - val_loss: 0.9562\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9458 - val_loss: 0.9492\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9411 - val_loss: 0.9461\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9388 - val_loss: 0.9437\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9363 - val_loss: 0.9418\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9350 - val_loss: 0.9440\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9338 - val_loss: 0.9430\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9324 - val_loss: 0.9378\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9307 - val_loss: 0.9386\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9290 - val_loss: 0.9353\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9282 - val_loss: 0.9343\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9271 - val_loss: 0.9343\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9266 - val_loss: 0.9325\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9257 - val_loss: 0.9341\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9247 - val_loss: 0.9328\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9241 - val_loss: 0.9326\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9232 - val_loss: 0.9300\n",
      "Top-2 accuracy = 0.86\n",
      "5\n",
      "normalizeP|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_145 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0495 - val_loss: 0.9929\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9773 - val_loss: 0.9718\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9652 - val_loss: 0.9634\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9584 - val_loss: 0.9584\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9527 - val_loss: 0.9542\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9481 - val_loss: 0.9502\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9441 - val_loss: 0.9481\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9405 - val_loss: 0.9439\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9372 - val_loss: 0.9447\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9348 - val_loss: 0.9413\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9327 - val_loss: 0.9371\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9296 - val_loss: 0.9345\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9277 - val_loss: 0.9341\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9264 - val_loss: 0.9326\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9250 - val_loss: 0.9343\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9243 - val_loss: 0.9298\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9226 - val_loss: 0.9303\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9212 - val_loss: 0.9297\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9208 - val_loss: 0.9288\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9197 - val_loss: 0.9280\n",
      "Top-2 accuracy = 0.85\n",
      "6\n",
      "robustw|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0371 - val_loss: 0.9911\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9718 - val_loss: 0.9674\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9586 - val_loss: 0.9617\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9514 - val_loss: 0.9549\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9477 - val_loss: 0.9521\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9451 - val_loss: 0.9512\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9439 - val_loss: 0.9493\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9431 - val_loss: 0.9509\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9420 - val_loss: 0.9501\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9410 - val_loss: 0.9475\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9403 - val_loss: 0.9478\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9401 - val_loss: 0.9463\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9390 - val_loss: 0.9496\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9393 - val_loss: 0.9456\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9373 - val_loss: 0.9449\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9382 - val_loss: 0.9440\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9361 - val_loss: 0.9450\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9360 - val_loss: 0.9431\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9349 - val_loss: 0.9441\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9362 - val_loss: 0.9439\n",
      "Top-2 accuracy = 0.85\n",
      "7\n",
      "maxabse|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_155 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0903 - val_loss: 1.0829\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0706 - val_loss: 1.0575\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0409 - val_loss: 1.0283\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0161 - val_loss: 1.0090\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0014 - val_loss: 0.9980\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9920 - val_loss: 0.9904\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9855 - val_loss: 0.9853\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9807 - val_loss: 0.9810\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9768 - val_loss: 0.9781\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9739 - val_loss: 0.9753\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9711 - val_loss: 0.9735\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9690 - val_loss: 0.9711\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9673 - val_loss: 0.9693\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9655 - val_loss: 0.9680\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9640 - val_loss: 0.9666\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9624 - val_loss: 0.9657\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9612 - val_loss: 0.9638\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9597 - val_loss: 0.9625\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9587 - val_loss: 0.9612\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9574 - val_loss: 0.9601\n",
      "Top-2 accuracy = 0.84\n",
      "8\n",
      "minmaxa|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_160 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.1057 - val_loss: 1.0797\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0590 - val_loss: 1.0426\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0221 - val_loss: 1.0116\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9968 - val_loss: 0.9945\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9850 - val_loss: 0.9853\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9777 - val_loss: 0.9810\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9735 - val_loss: 0.9772\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9713 - val_loss: 0.9746\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9689 - val_loss: 0.9719\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9668 - val_loss: 0.9717\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9657 - val_loss: 0.9719\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9641 - val_loss: 0.9685\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9630 - val_loss: 0.9689\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9616 - val_loss: 0.9653\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9607 - val_loss: 0.9653\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9596 - val_loss: 0.9646\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9588 - val_loss: 0.9646\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9581 - val_loss: 0.9619\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9577 - val_loss: 0.9659\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9576 - val_loss: 0.9614\n",
      "Top-2 accuracy = 0.84\n",
      "9\n",
      "robustq|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_163 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0468 - val_loss: 0.9945\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9718 - val_loss: 0.9660\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9521 - val_loss: 0.9555\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9434 - val_loss: 0.9480\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9382 - val_loss: 0.9433\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9344 - val_loss: 0.9405\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9312 - val_loss: 0.9384\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9285 - val_loss: 0.9352\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9273 - val_loss: 0.9344\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9253 - val_loss: 0.9350\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9232 - val_loss: 0.9373\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9236 - val_loss: 0.9357\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9218 - val_loss: 0.9310\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9200 - val_loss: 0.9310\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9203 - val_loss: 0.9303\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9194 - val_loss: 0.9297\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9182 - val_loss: 0.9305\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9174 - val_loss: 0.9294\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9174 - val_loss: 0.9302\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9172 - val_loss: 0.9281\n",
      "Top-2 accuracy = 0.86\n",
      "10\n",
      "minmaxv|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_168 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0783 - val_loss: 1.0535\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0184 - val_loss: 0.9975\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9845 - val_loss: 0.9777\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9718 - val_loss: 0.9699\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9639 - val_loss: 0.9632\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9589 - val_loss: 0.9699\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9553 - val_loss: 0.9545\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9505 - val_loss: 0.9556\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9487 - val_loss: 0.9501\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9472 - val_loss: 0.9578\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9462 - val_loss: 0.9464\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9429 - val_loss: 0.9490\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9423 - val_loss: 0.9463\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9411 - val_loss: 0.9479\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9400 - val_loss: 0.9417\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9382 - val_loss: 0.9468\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9394 - val_loss: 0.9414\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9386 - val_loss: 0.9420\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9367 - val_loss: 0.9425\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9356 - val_loss: 0.9408\n",
      "Top-2 accuracy = 0.85\n",
      "11\n",
      "maxabsI|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_174 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0766 - val_loss: 1.0441\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0153 - val_loss: 1.0051\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9912 - val_loss: 0.9890\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9828 - val_loss: 0.9875\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9785 - val_loss: 0.9786\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9743 - val_loss: 0.9751\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9716 - val_loss: 0.9722\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9675 - val_loss: 0.9713\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9649 - val_loss: 0.9655\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9619 - val_loss: 0.9636\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9593 - val_loss: 0.9607\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9569 - val_loss: 0.9588\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9549 - val_loss: 0.9586\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9532 - val_loss: 0.9564\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9518 - val_loss: 0.9544\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9511 - val_loss: 0.9552\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9495 - val_loss: 0.9530\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9484 - val_loss: 0.9540\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9479 - val_loss: 0.9547\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9471 - val_loss: 0.9556\n",
      "Top-2 accuracy = 0.84\n",
      "12\n",
      "maxabsJ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0742 - val_loss: 1.0279\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9936 - val_loss: 0.9831\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9730 - val_loss: 0.9781\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9691 - val_loss: 0.9782\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9646 - val_loss: 0.9662\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9644 - val_loss: 0.9799\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9605 - val_loss: 0.9783\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9558 - val_loss: 0.9691\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9524 - val_loss: 0.9536\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9511 - val_loss: 0.9533\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9494 - val_loss: 0.9590\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9531 - val_loss: 0.9524\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9479 - val_loss: 0.9507\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9471 - val_loss: 0.9530\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9512 - val_loss: 0.9597\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9464 - val_loss: 0.9600\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9487 - val_loss: 0.9821\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9509 - val_loss: 0.9676\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9502 - val_loss: 0.9566\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9478 - val_loss: 0.9655\n",
      "Top-2 accuracy = 0.84\n",
      "13\n",
      "maxabsF|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_185 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.1038 - val_loss: 1.0846\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0736 - val_loss: 1.0636\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0485 - val_loss: 1.0365\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0216 - val_loss: 1.0144\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0017 - val_loss: 0.9974\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9876 - val_loss: 0.9859\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9791 - val_loss: 0.9800\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9746 - val_loss: 0.9761\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9716 - val_loss: 0.9722\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9694 - val_loss: 0.9737\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9681 - val_loss: 0.9711\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9666 - val_loss: 0.9689\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9660 - val_loss: 0.9682\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9657 - val_loss: 0.9678\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9650 - val_loss: 0.9666\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9642 - val_loss: 0.9674\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9641 - val_loss: 0.9657\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9631 - val_loss: 0.9652\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9630 - val_loss: 0.9645\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9625 - val_loss: 0.9652\n",
      "Top-2 accuracy = 0.83\n",
      "14\n",
      "robustX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0569 - val_loss: 0.9812\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9487 - val_loss: 0.9407\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9293 - val_loss: 0.9330\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9250 - val_loss: 0.9313\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9223 - val_loss: 0.9325\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9198 - val_loss: 0.9278\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9191 - val_loss: 0.9314\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9192 - val_loss: 0.9261\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9169 - val_loss: 0.9274\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9180 - val_loss: 0.9276\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9166 - val_loss: 0.9298\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9169 - val_loss: 0.9259\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9154 - val_loss: 0.9250\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9149 - val_loss: 0.9257\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9155 - val_loss: 0.9258\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9162 - val_loss: 0.9233\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9143 - val_loss: 0.9232\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9142 - val_loss: 0.9250\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9137 - val_loss: 0.9251\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9137 - val_loss: 0.9238\n",
      "Top-2 accuracy = 0.86\n",
      "15\n",
      "normalizeW|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_192 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.1067 - val_loss: 1.0951\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0885 - val_loss: 1.0821\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0691 - val_loss: 1.0593\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0424 - val_loss: 1.0299\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0108 - val_loss: 0.9986\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9869 - val_loss: 0.9826\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9756 - val_loss: 0.9755\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9703 - val_loss: 0.9712\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9666 - val_loss: 0.9681\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9639 - val_loss: 0.9655\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9617 - val_loss: 0.9635\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9597 - val_loss: 0.9620\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9582 - val_loss: 0.9608\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9568 - val_loss: 0.9597\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9559 - val_loss: 0.9590\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9551 - val_loss: 0.9585\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9543 - val_loss: 0.9576\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9537 - val_loss: 0.9570\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9532 - val_loss: 0.9567\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9526 - val_loss: 0.9565\n",
      "Top-2 accuracy = 0.84\n",
      "16\n",
      "robusts|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_195 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9350 - val_loss: 1.2372\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0896 - val_loss: 1.0400\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0151 - val_loss: 1.0106\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9934 - val_loss: 0.9941\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9822 - val_loss: 0.9837\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9721 - val_loss: 0.9763\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9633 - val_loss: 0.9687\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9571 - val_loss: 0.9615\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9525 - val_loss: 0.9605\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9497 - val_loss: 0.9602\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9470 - val_loss: 0.9623\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9471 - val_loss: 0.9600\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9434 - val_loss: 0.9635\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9407 - val_loss: 0.9443\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9402 - val_loss: 0.9466\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9392 - val_loss: 0.9452\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9365 - val_loss: 0.9451\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9358 - val_loss: 0.9510\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9374 - val_loss: 0.9402\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9329 - val_loss: 0.9428\n",
      "Top-2 accuracy = 0.85\n",
      "17\n",
      "minmaxF|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_199 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0882 - val_loss: 1.0733\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0585 - val_loss: 1.0431\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0274 - val_loss: 1.0149\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0056 - val_loss: 0.9968\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9906 - val_loss: 0.9857\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9830 - val_loss: 0.9842\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9783 - val_loss: 0.9826\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9762 - val_loss: 0.9752\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9732 - val_loss: 0.9737\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9717 - val_loss: 0.9726\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9700 - val_loss: 0.9713\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9684 - val_loss: 0.9706\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9676 - val_loss: 0.9692\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9667 - val_loss: 0.9703\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9659 - val_loss: 0.9688\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9651 - val_loss: 0.9680\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9653 - val_loss: 0.9682\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9645 - val_loss: 0.9699\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9651 - val_loss: 0.9674\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9635 - val_loss: 0.9685\n",
      "Top-2 accuracy = 0.84\n",
      "18\n",
      "minmaxs|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_206 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0954 - val_loss: 1.0854\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0767 - val_loss: 1.0697\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0541 - val_loss: 1.0411\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0235 - val_loss: 1.0151\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9987 - val_loss: 0.9944\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9836 - val_loss: 0.9831\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9748 - val_loss: 0.9755\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9687 - val_loss: 0.9713\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9646 - val_loss: 0.9674\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9624 - val_loss: 0.9657\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9607 - val_loss: 0.9659\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9596 - val_loss: 0.9636\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9594 - val_loss: 0.9659\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9581 - val_loss: 0.9623\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9568 - val_loss: 0.9606\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9559 - val_loss: 0.9601\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9555 - val_loss: 0.9594\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9550 - val_loss: 0.9580\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9541 - val_loss: 0.9576\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9535 - val_loss: 0.9580\n",
      "Top-2 accuracy = 0.84\n",
      "19\n",
      "minmaxk|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_209 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0880 - val_loss: 1.0671\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0343 - val_loss: 1.0117\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9921 - val_loss: 0.9872\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9779 - val_loss: 0.9741\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9691 - val_loss: 0.9701\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9661 - val_loss: 0.9691\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9642 - val_loss: 0.9676\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9623 - val_loss: 0.9628\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9604 - val_loss: 0.9609\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9588 - val_loss: 0.9591\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9583 - val_loss: 0.9625\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9575 - val_loss: 0.9566\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9551 - val_loss: 0.9594\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9564 - val_loss: 0.9538\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9534 - val_loss: 0.9534\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9526 - val_loss: 0.9558\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9527 - val_loss: 0.9513\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9524 - val_loss: 0.9505\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9505 - val_loss: 0.9513\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9507 - val_loss: 0.9491\n",
      "Top-2 accuracy = 0.84\n",
      "20\n",
      "robustI|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_216 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.2916 - val_loss: 1.1043\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0756 - val_loss: 1.0552\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0401 - val_loss: 1.0284\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0196 - val_loss: 1.0103\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0059 - val_loss: 0.9992\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9957 - val_loss: 0.9914\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9876 - val_loss: 0.9817\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9783 - val_loss: 0.9755\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9723 - val_loss: 0.9728\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9678 - val_loss: 0.9678\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9640 - val_loss: 0.9633\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9596 - val_loss: 0.9612\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9569 - val_loss: 0.9589\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9544 - val_loss: 0.9571\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9518 - val_loss: 0.9550\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9501 - val_loss: 0.9539\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9485 - val_loss: 0.9532\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9465 - val_loss: 0.9506\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9457 - val_loss: 0.9509\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9445 - val_loss: 0.9505\n",
      "Top-2 accuracy = 0.85\n",
      "21\n",
      "maxabsd|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_220 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0968 - val_loss: 1.0962\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0953 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "22\n",
      "maxabsV|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_227 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0512 - val_loss: 0.9967\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9748 - val_loss: 0.9643\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9554 - val_loss: 0.9530\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9457 - val_loss: 0.9478\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9410 - val_loss: 0.9442\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9364 - val_loss: 0.9401\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9334 - val_loss: 0.9378\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9311 - val_loss: 0.9372\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9286 - val_loss: 0.9343\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9270 - val_loss: 0.9332\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9254 - val_loss: 0.9309\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9238 - val_loss: 0.9318\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9231 - val_loss: 0.9299\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9223 - val_loss: 0.9284\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9204 - val_loss: 0.9281\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9199 - val_loss: 0.9274\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9190 - val_loss: 0.9266\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9184 - val_loss: 0.9264\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9173 - val_loss: 0.9247\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9168 - val_loss: 0.9276\n",
      "Top-2 accuracy = 0.86\n",
      "23\n",
      "normalizeg|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_231 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.1012 - val_loss: 1.0931\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0903 - val_loss: 1.0882\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0835 - val_loss: 1.0737\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0384 - val_loss: 1.0021\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9847 - val_loss: 0.9747\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9678 - val_loss: 0.9670\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9616 - val_loss: 0.9619\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9573 - val_loss: 0.9589\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9544 - val_loss: 0.9563\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9523 - val_loss: 0.9548\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9506 - val_loss: 0.9536\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9491 - val_loss: 0.9519\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9477 - val_loss: 0.9507\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9466 - val_loss: 0.9499\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9456 - val_loss: 0.9491\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9446 - val_loss: 0.9480\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9438 - val_loss: 0.9470\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9427 - val_loss: 0.9461\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9419 - val_loss: 0.9452\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9411 - val_loss: 0.9445\n",
      "Top-2 accuracy = 0.85\n",
      "24\n",
      "maxabsL|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_235 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0581 - val_loss: 1.0012\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9734 - val_loss: 0.9643\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9539 - val_loss: 0.9543\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9462 - val_loss: 0.9521\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9409 - val_loss: 0.9445\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9370 - val_loss: 0.9412\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9338 - val_loss: 0.9389\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9313 - val_loss: 0.9374\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9302 - val_loss: 0.9368\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9290 - val_loss: 0.9340\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9269 - val_loss: 0.9325\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9251 - val_loss: 0.9330\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9250 - val_loss: 0.9301\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9228 - val_loss: 0.9300\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9223 - val_loss: 0.9296\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9215 - val_loss: 0.9288\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9207 - val_loss: 0.9287\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9201 - val_loss: 0.9277\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9198 - val_loss: 0.9269\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9193 - val_loss: 0.9275\n",
      "Top-2 accuracy = 0.86\n",
      "25\n",
      "minmaxE|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0961 - val_loss: 1.0960\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0953 - val_loss: 1.0963\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Top-2 accuracy = 0.7\n",
      "26\n",
      "robustP|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_246 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0976 - val_loss: 1.0950\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0931 - val_loss: 1.0919\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0891 - val_loss: 1.0873\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0823 - val_loss: 1.0790\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0722 - val_loss: 1.0703\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0630 - val_loss: 1.0620\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0517 - val_loss: 1.0485\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0351 - val_loss: 1.0306\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0176 - val_loss: 1.0172\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0045 - val_loss: 1.0055\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9933 - val_loss: 0.9965\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9851 - val_loss: 0.9892\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9782 - val_loss: 0.9822\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9731 - val_loss: 0.9780\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9698 - val_loss: 0.9742\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9672 - val_loss: 0.9715\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9650 - val_loss: 0.9694\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9637 - val_loss: 0.9677\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9621 - val_loss: 0.9663\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9611 - val_loss: 0.9652\n",
      "Top-2 accuracy = 0.84\n",
      "27\n",
      "normalizew|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_250 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0949 - val_loss: 1.0882\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0743 - val_loss: 1.0562\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0335 - val_loss: 1.0182\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9993 - val_loss: 0.9932\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9783 - val_loss: 0.9782\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9657 - val_loss: 0.9707\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9601 - val_loss: 0.9666\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9565 - val_loss: 0.9645\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9542 - val_loss: 0.9628\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9526 - val_loss: 0.9613\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9517 - val_loss: 0.9602\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9509 - val_loss: 0.9595\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9502 - val_loss: 0.9599\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9499 - val_loss: 0.9580\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9489 - val_loss: 0.9590\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9488 - val_loss: 0.9575\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9484 - val_loss: 0.9574\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9480 - val_loss: 0.9572\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9479 - val_loss: 0.9575\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9475 - val_loss: 0.9562\n",
      "Top-2 accuracy = 0.85\n",
      "28\n",
      "maxabsi|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0897 - val_loss: 1.0800\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0639 - val_loss: 1.0442\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0111 - val_loss: 0.9861\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9713 - val_loss: 0.9719\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9643 - val_loss: 0.9684\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9607 - val_loss: 0.9655\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9575 - val_loss: 0.9622\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9547 - val_loss: 0.9582\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9516 - val_loss: 0.9556\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9501 - val_loss: 0.9540\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9482 - val_loss: 0.9532\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9472 - val_loss: 0.9513\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9457 - val_loss: 0.9508\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9445 - val_loss: 0.9494\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9434 - val_loss: 0.9483\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9425 - val_loss: 0.9480\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9415 - val_loss: 0.9463\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9406 - val_loss: 0.9457\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9398 - val_loss: 0.9459\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9392 - val_loss: 0.9448\n",
      "Top-2 accuracy = 0.85\n",
      "29\n",
      "standardized|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_258 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0961 - val_loss: 1.0954\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0914 - val_loss: 1.0868\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0647 - val_loss: 1.0469\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0264 - val_loss: 1.0159\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0049 - val_loss: 1.0024\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9943 - val_loss: 0.9949\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9875 - val_loss: 0.9888\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9823 - val_loss: 0.9840\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9764 - val_loss: 0.9764\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9705 - val_loss: 0.9722\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9666 - val_loss: 0.9687\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9642 - val_loss: 0.9668\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9617 - val_loss: 0.9646\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9599 - val_loss: 0.9626\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9583 - val_loss: 0.9616\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9573 - val_loss: 0.9598\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9560 - val_loss: 0.9584\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9550 - val_loss: 0.9569\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9539 - val_loss: 0.9563\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9531 - val_loss: 0.9553\n",
      "Top-2 accuracy = 0.85\n",
      "0\n",
      "normalizeQ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0970 - val_loss: 1.0964\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0955 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "1\n",
      "normalizep|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_267 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0897 - val_loss: 1.0715\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0337 - val_loss: 0.9939\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9799 - val_loss: 0.9743\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9651 - val_loss: 0.9646\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9573 - val_loss: 0.9587\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9522 - val_loss: 0.9544\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9483 - val_loss: 0.9518\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9457 - val_loss: 0.9506\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9439 - val_loss: 0.9480\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9421 - val_loss: 0.9464\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9405 - val_loss: 0.9466\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9396 - val_loss: 0.9444\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9384 - val_loss: 0.9437\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9377 - val_loss: 0.9434\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9364 - val_loss: 0.9437\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9360 - val_loss: 0.9427\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9355 - val_loss: 0.9419\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9347 - val_loss: 0.9415\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9338 - val_loss: 0.9406\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9332 - val_loss: 0.9406\n",
      "Top-2 accuracy = 0.85\n",
      "2\n",
      "maxabsm|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0680 - val_loss: 1.0185\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9811 - val_loss: 0.9672\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9588 - val_loss: 0.9573\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9518 - val_loss: 0.9529\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9478 - val_loss: 0.9505\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9458 - val_loss: 0.9505\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9448 - val_loss: 0.9474\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9423 - val_loss: 0.9462\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9419 - val_loss: 0.9442\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9402 - val_loss: 0.9448\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9403 - val_loss: 0.9428\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9386 - val_loss: 0.9420\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9377 - val_loss: 0.9411\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9374 - val_loss: 0.9415\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9371 - val_loss: 0.9406\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9364 - val_loss: 0.9394\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9351 - val_loss: 0.9427\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9357 - val_loss: 0.9399\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9343 - val_loss: 0.9397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9342 - val_loss: 0.9381\n",
      "Top-2 accuracy = 0.85\n",
      "3\n",
      "minmaxY|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0708 - val_loss: 1.0315\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0106 - val_loss: 0.9912\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9749 - val_loss: 0.9646\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9611 - val_loss: 0.9665\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9565 - val_loss: 0.9569\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9551 - val_loss: 0.9549\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9531 - val_loss: 0.9611\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9536 - val_loss: 0.9545\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9519 - val_loss: 0.9590\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9534 - val_loss: 0.9541\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9510 - val_loss: 0.9528\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9515 - val_loss: 0.9526\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9515 - val_loss: 0.9513\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9498 - val_loss: 0.9507\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9491 - val_loss: 0.9509\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9495 - val_loss: 0.9514\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9491 - val_loss: 0.9515\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9496 - val_loss: 0.9530\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9490 - val_loss: 0.9496\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9491 - val_loss: 0.9602\n",
      "Top-2 accuracy = 0.84\n",
      "4\n",
      "normalizek|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_284 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0781 - val_loss: 1.0459\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0150 - val_loss: 1.0065\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9902 - val_loss: 0.9870\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9761 - val_loss: 0.9772\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9691 - val_loss: 0.9725\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9642 - val_loss: 0.9651\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9602 - val_loss: 0.9615\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9567 - val_loss: 0.9585\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9544 - val_loss: 0.9643\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9522 - val_loss: 0.9625\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9498 - val_loss: 0.9515\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9473 - val_loss: 0.9572\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9456 - val_loss: 0.9492\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9442 - val_loss: 0.9477\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9422 - val_loss: 0.9474\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9423 - val_loss: 0.9481\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9414 - val_loss: 0.9474\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9401 - val_loss: 0.9567\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9399 - val_loss: 0.9497\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9424 - val_loss: 0.9508\n",
      "Top-2 accuracy = 0.85\n",
      "5\n",
      "normalizeU|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0970 - val_loss: 1.0964\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0955 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "6\n",
      "maxabsK|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_294 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0711 - val_loss: 1.0505\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0249 - val_loss: 1.0143\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9909 - val_loss: 0.9851\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9702 - val_loss: 0.9690\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9594 - val_loss: 0.9626\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9520 - val_loss: 0.9665\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9486 - val_loss: 0.9546\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9466 - val_loss: 0.9580\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9446 - val_loss: 0.9540\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9439 - val_loss: 0.9545\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9427 - val_loss: 0.9481\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9408 - val_loss: 0.9533\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9411 - val_loss: 0.9469\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9396 - val_loss: 0.9492\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9405 - val_loss: 0.9525\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9393 - val_loss: 0.9473\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9393 - val_loss: 0.9445\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9378 - val_loss: 0.9434\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9377 - val_loss: 0.9464\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9370 - val_loss: 0.9427\n",
      "Top-2 accuracy = 0.85\n",
      "7\n",
      "normalizeH|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0804 - val_loss: 1.0462\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0029 - val_loss: 0.9763\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9631 - val_loss: 0.9622\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9565 - val_loss: 0.9617\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9532 - val_loss: 0.9555\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9510 - val_loss: 0.9582\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9493 - val_loss: 0.9537\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9464 - val_loss: 0.9504\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9472 - val_loss: 0.9483\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9477 - val_loss: 0.9479\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9449 - val_loss: 0.9566\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9429 - val_loss: 0.9498\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9422 - val_loss: 0.9465\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9436 - val_loss: 0.9448\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9408 - val_loss: 0.9543\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9417 - val_loss: 0.9455\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9401 - val_loss: 0.9447\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9403 - val_loss: 0.9438\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9390 - val_loss: 0.9449\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9390 - val_loss: 0.9433\n",
      "Top-2 accuracy = 0.85\n",
      "8\n",
      "robuste|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_304 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0661 - val_loss: 1.0291\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0017 - val_loss: 0.9817\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9629 - val_loss: 0.9574\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9479 - val_loss: 0.9495\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9419 - val_loss: 0.9470\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9386 - val_loss: 0.9448\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9362 - val_loss: 0.9413\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9344 - val_loss: 0.9394\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9330 - val_loss: 0.9376\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9311 - val_loss: 0.9368\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9298 - val_loss: 0.9350\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9289 - val_loss: 0.9396\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9284 - val_loss: 0.9349\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9269 - val_loss: 0.9344\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9260 - val_loss: 0.9334\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9244 - val_loss: 0.9319\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9242 - val_loss: 0.9322\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9241 - val_loss: 0.9319\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9224 - val_loss: 0.9319\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9218 - val_loss: 0.9305\n",
      "Top-2 accuracy = 0.85\n",
      "9\n",
      "robusth|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0898 - val_loss: 1.0779\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0619 - val_loss: 1.0477\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0362 - val_loss: 1.0280\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0213 - val_loss: 1.0174\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0122 - val_loss: 1.0105\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0064 - val_loss: 1.0063\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0029 - val_loss: 1.0039\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0006 - val_loss: 1.0015\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9984 - val_loss: 0.9995\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9968 - val_loss: 0.9977\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9948 - val_loss: 0.9958\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9929 - val_loss: 0.9944\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9906 - val_loss: 0.9907\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9877 - val_loss: 0.9880\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9846 - val_loss: 0.9846\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9805 - val_loss: 0.9802\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9763 - val_loss: 0.9766\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9728 - val_loss: 0.9728\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9694 - val_loss: 0.9692\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9672 - val_loss: 0.9665\n",
      "Top-2 accuracy = 0.84\n",
      "10\n",
      "minmaxa|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_314 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0597 - val_loss: 1.0038\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9756 - val_loss: 0.9650\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9540 - val_loss: 0.9535\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9479 - val_loss: 0.9515\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9438 - val_loss: 0.9476\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9405 - val_loss: 0.9432\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9388 - val_loss: 0.9553\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9391 - val_loss: 0.9404\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9347 - val_loss: 0.9388\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9357 - val_loss: 0.9400\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9337 - val_loss: 0.9400\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9327 - val_loss: 0.9407\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9318 - val_loss: 0.9390\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9306 - val_loss: 0.9362\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9301 - val_loss: 0.9336\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9276 - val_loss: 0.9313\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9287 - val_loss: 0.9320\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9287 - val_loss: 0.9374\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9261 - val_loss: 0.9364\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9265 - val_loss: 0.9378\n",
      "Top-2 accuracy = 0.85\n",
      "11\n",
      "minmaxv|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_321 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0791 - val_loss: 1.0450\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0228 - val_loss: 1.0150\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0006 - val_loss: 0.9970\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9837 - val_loss: 0.9817\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9716 - val_loss: 0.9714\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9644 - val_loss: 0.9679\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9585 - val_loss: 0.9598\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9543 - val_loss: 0.9579\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9512 - val_loss: 0.9545\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9490 - val_loss: 0.9514\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9468 - val_loss: 0.9514\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9460 - val_loss: 0.9542\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9458 - val_loss: 0.9478\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9450 - val_loss: 0.9473\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9433 - val_loss: 0.9499\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9433 - val_loss: 0.9466\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9415 - val_loss: 0.9466\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9407 - val_loss: 0.9451\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9396 - val_loss: 0.9421\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9385 - val_loss: 0.9486\n",
      "Top-2 accuracy = 0.85\n",
      "12\n",
      "minmaxp|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0883 - val_loss: 1.0650\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0393 - val_loss: 1.0262\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0135 - val_loss: 1.0082\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9994 - val_loss: 0.9960\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9894 - val_loss: 0.9883\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9825 - val_loss: 0.9832\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9771 - val_loss: 0.9779\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9743 - val_loss: 0.9816\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9732 - val_loss: 0.9741\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9708 - val_loss: 0.9728\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9711 - val_loss: 0.9723\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9685 - val_loss: 0.9710\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9674 - val_loss: 0.9699\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9666 - val_loss: 0.9720\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9669 - val_loss: 0.9698\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9659 - val_loss: 0.9695\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9651 - val_loss: 0.9666\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9647 - val_loss: 0.9672\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9643 - val_loss: 0.9662\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9635 - val_loss: 0.9675\n",
      "Top-2 accuracy = 0.84\n",
      "13\n",
      "minmaxh|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_331 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0858 - val_loss: 1.0541\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0223 - val_loss: 1.0066\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9890 - val_loss: 0.9885\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9714 - val_loss: 0.9671\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9621 - val_loss: 0.9607\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9560 - val_loss: 0.9588\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9524 - val_loss: 0.9596\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9517 - val_loss: 0.9514\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9480 - val_loss: 0.9534\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9483 - val_loss: 0.9496\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9466 - val_loss: 0.9478\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9456 - val_loss: 0.9663\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9463 - val_loss: 0.9509\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9441 - val_loss: 0.9468\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9432 - val_loss: 0.9462\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9424 - val_loss: 0.9450\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9417 - val_loss: 0.9436\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9411 - val_loss: 0.9432\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9399 - val_loss: 0.9418\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9394 - val_loss: 0.9456\n",
      "Top-2 accuracy = 0.85\n",
      "14\n",
      "maxabsz|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0968 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0954 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "15\n",
      "maxabsn|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_343 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0886\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0811 - val_loss: 1.0733\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0526 - val_loss: 1.0434\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0184 - val_loss: 1.0133\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9983 - val_loss: 0.9964\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9835 - val_loss: 0.9823\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9736 - val_loss: 0.9754\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9679 - val_loss: 0.9721\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9648 - val_loss: 0.9668\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9619 - val_loss: 0.9707\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9614 - val_loss: 0.9644\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9590 - val_loss: 0.9681\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9594 - val_loss: 0.9645\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9575 - val_loss: 0.9609\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9563 - val_loss: 0.9617\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9561 - val_loss: 0.9598\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9552 - val_loss: 0.9600\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9539 - val_loss: 0.9593\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9536 - val_loss: 0.9594\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9534 - val_loss: 0.9594\n",
      "Top-2 accuracy = 0.85\n",
      "16\n",
      "standardizeo|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_349 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0672 - val_loss: 1.0253\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0017 - val_loss: 0.9850\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9699 - val_loss: 0.9642\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9555 - val_loss: 0.9544\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9469 - val_loss: 0.9481\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9419 - val_loss: 0.9450\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9377 - val_loss: 0.9442\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9356 - val_loss: 0.9401\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9329 - val_loss: 0.9373\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9307 - val_loss: 0.9374\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9293 - val_loss: 0.9362\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9280 - val_loss: 0.9352\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9270 - val_loss: 0.9338\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9257 - val_loss: 0.9328\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9246 - val_loss: 0.9314\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9237 - val_loss: 0.9319\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9230 - val_loss: 0.9304\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9222 - val_loss: 0.9308\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9210 - val_loss: 0.9297\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9207 - val_loss: 0.9295\n",
      "Top-2 accuracy = 0.86\n",
      "17\n",
      "normalizeI|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0575 - val_loss: 1.0066\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9776 - val_loss: 0.9599\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9488 - val_loss: 0.9473\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9413 - val_loss: 0.9416\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9370 - val_loss: 0.9390\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9351 - val_loss: 0.9413\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9341 - val_loss: 0.9357\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9326 - val_loss: 0.9346\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9310 - val_loss: 0.9333\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9304 - val_loss: 0.9347\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9307 - val_loss: 0.9375\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9301 - val_loss: 0.9319\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9273 - val_loss: 0.9309\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9260 - val_loss: 0.9308\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9248 - val_loss: 0.9286\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9233 - val_loss: 0.9280\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9217 - val_loss: 0.9284\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9201 - val_loss: 0.9235\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9194 - val_loss: 0.9240\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9199 - val_loss: 0.9259\n",
      "Top-2 accuracy = 0.85\n",
      "18\n",
      "standardizeA|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0375 - val_loss: 0.9987\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9783 - val_loss: 0.9674\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9610 - val_loss: 0.9583\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9533 - val_loss: 0.9534\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9496 - val_loss: 0.9514\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9478 - val_loss: 0.9504\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9451 - val_loss: 0.9528\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9442 - val_loss: 0.9481\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9430 - val_loss: 0.9507\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9413 - val_loss: 0.9485\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9408 - val_loss: 0.9473\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9394 - val_loss: 0.9448\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9382 - val_loss: 0.9443\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9371 - val_loss: 0.9446\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9364 - val_loss: 0.9431\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9352 - val_loss: 0.9454\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9343 - val_loss: 0.9425\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9339 - val_loss: 0.9424\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9331 - val_loss: 0.9405\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9317 - val_loss: 0.9437\n",
      "Top-2 accuracy = 0.85\n",
      "19\n",
      "robustT|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_365 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0288 - val_loss: 0.9821\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9626 - val_loss: 0.9559\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9450 - val_loss: 0.9450\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9376 - val_loss: 0.9421\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9344 - val_loss: 0.9409\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9317 - val_loss: 0.9352\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9290 - val_loss: 0.9362\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9273 - val_loss: 0.9333\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9252 - val_loss: 0.9318\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9237 - val_loss: 0.9318\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9230 - val_loss: 0.9316\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9215 - val_loss: 0.9287\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9202 - val_loss: 0.9308\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9199 - val_loss: 0.9270\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9185 - val_loss: 0.9261\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9169 - val_loss: 0.9257\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9168 - val_loss: 0.9272\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9159 - val_loss: 0.9255\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9148 - val_loss: 0.9252\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9143 - val_loss: 0.9235\n",
      "Top-2 accuracy = 0.86\n",
      "20\n",
      "normalizeK|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0556 - val_loss: 0.9908\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9644 - val_loss: 0.9573\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9450 - val_loss: 0.9752\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9442 - val_loss: 0.9506\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9398 - val_loss: 0.9574\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9414 - val_loss: 0.9420\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9360 - val_loss: 0.9447\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9366 - val_loss: 0.9419\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9342 - val_loss: 0.9398\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9334 - val_loss: 0.9471\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9329 - val_loss: 0.9482\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9335 - val_loss: 0.9384\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9310 - val_loss: 0.9377\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9310 - val_loss: 0.9385\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9304 - val_loss: 0.9379\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9295 - val_loss: 0.9375\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9298 - val_loss: 0.9368\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9292 - val_loss: 0.9400\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9285 - val_loss: 0.9408\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9293 - val_loss: 0.9373\n",
      "Top-2 accuracy = 0.85\n",
      "21\n",
      "standardizeq|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_376 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0798 - val_loss: 1.0519\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0093 - val_loss: 0.9927\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9689 - val_loss: 0.9662\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9542 - val_loss: 0.9563\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9478 - val_loss: 0.9485\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9430 - val_loss: 0.9473\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9398 - val_loss: 0.9428\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9388 - val_loss: 0.9417\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9359 - val_loss: 0.9386\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9340 - val_loss: 0.9385\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9328 - val_loss: 0.9358\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9312 - val_loss: 0.9365\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9302 - val_loss: 0.9349\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9294 - val_loss: 0.9348\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9291 - val_loss: 0.9348\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9281 - val_loss: 0.9335\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9274 - val_loss: 0.9336\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9261 - val_loss: 0.9337\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9260 - val_loss: 0.9337\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9254 - val_loss: 0.9332\n",
      "Top-2 accuracy = 0.85\n",
      "22\n",
      "standardizeJ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0931 - val_loss: 1.0879\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0766 - val_loss: 1.0641\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0433 - val_loss: 1.0254\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0100 - val_loss: 1.0035\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9927 - val_loss: 0.9893\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9829 - val_loss: 0.9832\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9765 - val_loss: 0.9762\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9721 - val_loss: 0.9738\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9698 - val_loss: 0.9710\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9671 - val_loss: 0.9678\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9651 - val_loss: 0.9665\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9636 - val_loss: 0.9657\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9627 - val_loss: 0.9638\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9609 - val_loss: 0.9637\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9600 - val_loss: 0.9623\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9590 - val_loss: 0.9618\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9580 - val_loss: 0.9595\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9573 - val_loss: 0.9591\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9570 - val_loss: 0.9586\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9561 - val_loss: 0.9588\n",
      "Top-2 accuracy = 0.84\n",
      "23\n",
      "maxabsE|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0866 - val_loss: 1.0685\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0413 - val_loss: 1.0250\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0093 - val_loss: 1.0061\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9937 - val_loss: 0.9951\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9839 - val_loss: 0.9847\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9763 - val_loss: 0.9786\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9707 - val_loss: 0.9726\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9673 - val_loss: 0.9691\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9634 - val_loss: 0.9660\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9610 - val_loss: 0.9642\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9590 - val_loss: 0.9640\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9571 - val_loss: 0.9612\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9557 - val_loss: 0.9596\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9546 - val_loss: 0.9585\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9535 - val_loss: 0.9583\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9528 - val_loss: 0.9573\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9520 - val_loss: 0.9565\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9511 - val_loss: 0.9589\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9508 - val_loss: 0.9554\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9501 - val_loss: 0.9553\n",
      "Top-2 accuracy = 0.85\n",
      "24\n",
      "standardizeF|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0428 - val_loss: 0.9737\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9481 - val_loss: 0.9494\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9362 - val_loss: 0.9349\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9297 - val_loss: 0.9327\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9264 - val_loss: 0.9392\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9256 - val_loss: 0.9366\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9231 - val_loss: 0.9348\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9216 - val_loss: 0.9300\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9214 - val_loss: 0.9295\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9193 - val_loss: 0.9312\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9192 - val_loss: 0.9272\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9180 - val_loss: 0.9274\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9160 - val_loss: 0.9269\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9160 - val_loss: 0.9294\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9157 - val_loss: 0.9251\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9139 - val_loss: 0.9267\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9129 - val_loss: 0.9290\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9131 - val_loss: 0.9293\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9126 - val_loss: 0.9361\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9145 - val_loss: 0.9285\n",
      "Top-2 accuracy = 0.86\n",
      "25\n",
      "minmaxQ|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_398 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 1.0796 - val_loss: 1.0509\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0162 - val_loss: 1.0018\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9883 - val_loss: 0.9923\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9771 - val_loss: 0.9777\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9698 - val_loss: 0.9715\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9646 - val_loss: 0.9657\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9605 - val_loss: 0.9699\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9581 - val_loss: 0.9601\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9551 - val_loss: 0.9581\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9540 - val_loss: 0.9658\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9534 - val_loss: 0.9598\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9522 - val_loss: 0.9579\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9501 - val_loss: 0.9556\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9502 - val_loss: 0.9533\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9493 - val_loss: 0.9539\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9475 - val_loss: 0.9531\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9476 - val_loss: 0.9529\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9488 - val_loss: 0.9528\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9459 - val_loss: 0.9503\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9455 - val_loss: 0.9520\n",
      "Top-2 accuracy = 0.84\n",
      "26\n",
      "robustq|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_403 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0314 - val_loss: 1.0004\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9786 - val_loss: 0.9737\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9595 - val_loss: 0.9603\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9499 - val_loss: 0.9530\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9449 - val_loss: 0.9492\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9413 - val_loss: 0.9473\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9394 - val_loss: 0.9443\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9365 - val_loss: 0.9428\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9353 - val_loss: 0.9428\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9339 - val_loss: 0.9402\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9325 - val_loss: 0.9398\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9313 - val_loss: 0.9387\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9305 - val_loss: 0.9368\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9294 - val_loss: 0.9371\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9282 - val_loss: 0.9346\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9272 - val_loss: 0.9345\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9268 - val_loss: 0.9345\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9253 - val_loss: 0.9333\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9246 - val_loss: 0.9331\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9238 - val_loss: 0.9312\n",
      "Top-2 accuracy = 0.85\n",
      "27\n",
      "minmaxL|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0758 - val_loss: 1.0181\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9839 - val_loss: 0.9755\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9697 - val_loss: 0.9716\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9672 - val_loss: 0.9692\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9649 - val_loss: 0.9753\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9663 - val_loss: 0.9695\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9608 - val_loss: 0.9660\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9598 - val_loss: 0.9620\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9579 - val_loss: 0.9644\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9591 - val_loss: 0.9712\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9566 - val_loss: 0.9609\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9525 - val_loss: 0.9571\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9505 - val_loss: 0.9655\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9495 - val_loss: 0.9555\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9489 - val_loss: 0.9530\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9464 - val_loss: 0.9573\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9464 - val_loss: 0.9562\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9474 - val_loss: 0.9509\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9440 - val_loss: 0.9464\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9433 - val_loss: 0.9452\n",
      "Top-2 accuracy = 0.85\n",
      "28\n",
      "standardizeG|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.1020 - val_loss: 1.0969\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0963\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "29\n",
      "minmaxg|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0439 - val_loss: 0.9902\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9636 - val_loss: 0.9651\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9556 - val_loss: 0.9537\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9474 - val_loss: 0.9474\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9441 - val_loss: 0.9452\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9416 - val_loss: 0.9495\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9402 - val_loss: 0.9434\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9388 - val_loss: 0.9490\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9390 - val_loss: 0.9488\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9374 - val_loss: 0.9385\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9359 - val_loss: 0.9544\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9364 - val_loss: 0.9396\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9353 - val_loss: 0.9375\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9324 - val_loss: 0.9397\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9322 - val_loss: 0.9358\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9328 - val_loss: 0.9389\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9319 - val_loss: 0.9363\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9312 - val_loss: 0.9403\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9320 - val_loss: 0.9581\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9340 - val_loss: 0.9338\n",
      "Top-2 accuracy = 0.85\n",
      "0\n",
      "normalizeu|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0973 - val_loss: 1.0964\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0954 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "1\n",
      "standardizem|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_426 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0792 - val_loss: 1.0624\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0445 - val_loss: 1.0277\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0155 - val_loss: 1.0135\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0046 - val_loss: 1.0065\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9983 - val_loss: 1.0017\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9926 - val_loss: 0.9978\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9876 - val_loss: 0.9943\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9837 - val_loss: 0.9914\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9796 - val_loss: 0.9886\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9759 - val_loss: 0.9839\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9727 - val_loss: 0.9809\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9702 - val_loss: 0.9780\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9677 - val_loss: 0.9763\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9659 - val_loss: 0.9742\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9636 - val_loss: 0.9729\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9624 - val_loss: 0.9717\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9609 - val_loss: 0.9701\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9594 - val_loss: 0.9696\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9585 - val_loss: 0.9684\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9579 - val_loss: 0.9685\n",
      "Top-2 accuracy = 0.84\n",
      "2\n",
      "robustO|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0629 - val_loss: 1.0377\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0133 - val_loss: 0.9987\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9873 - val_loss: 0.9835\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9773 - val_loss: 0.9775\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9712 - val_loss: 0.9723\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9681 - val_loss: 0.9692\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9652 - val_loss: 0.9661\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9632 - val_loss: 0.9650\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9616 - val_loss: 0.9622\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9598 - val_loss: 0.9618\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9586 - val_loss: 0.9610\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9570 - val_loss: 0.9590\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9559 - val_loss: 0.9578\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9540 - val_loss: 0.9569\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9531 - val_loss: 0.9579\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9524 - val_loss: 0.9545\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9509 - val_loss: 0.9567\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9508 - val_loss: 0.9543\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9500 - val_loss: 0.9524\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9486 - val_loss: 0.9529\n",
      "Top-2 accuracy = 0.85\n",
      "3\n",
      "minmaxM|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0947 - val_loss: 1.0927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0784 - val_loss: 1.0592\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0441 - val_loss: 1.0360\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0260 - val_loss: 1.0223\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0152 - val_loss: 1.0142\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0085 - val_loss: 1.0114\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0057 - val_loss: 1.0081\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0015 - val_loss: 1.0038\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9984 - val_loss: 1.0013\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9962 - val_loss: 1.0043\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9950 - val_loss: 0.9987\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9947 - val_loss: 0.9997\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9934 - val_loss: 0.9972\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9923 - val_loss: 0.9978\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9928 - val_loss: 0.9962\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9917 - val_loss: 0.9954\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9905 - val_loss: 0.9946\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9895 - val_loss: 0.9947\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9879 - val_loss: 0.9902\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9850 - val_loss: 0.9865\n",
      "Top-2 accuracy = 0.83\n",
      "4\n",
      "standardizeZ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0559 - val_loss: 1.0088\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9816 - val_loss: 0.9712\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9534 - val_loss: 0.9529\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9411 - val_loss: 0.9462\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9370 - val_loss: 0.9426\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9349 - val_loss: 0.9450\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9337 - val_loss: 0.9507\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9335 - val_loss: 0.9442\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9312 - val_loss: 0.9405\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9312 - val_loss: 0.9433\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9313 - val_loss: 0.9413\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9295 - val_loss: 0.9405\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9300 - val_loss: 0.9448\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9287 - val_loss: 0.9390\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9282 - val_loss: 0.9408\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9283 - val_loss: 0.9389\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9288 - val_loss: 0.9418\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9280 - val_loss: 0.9402\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9284 - val_loss: 0.9381\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9265 - val_loss: 0.9391\n",
      "Top-2 accuracy = 0.85\n",
      "5\n",
      "normalizeT|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0519 - val_loss: 1.0018\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9750 - val_loss: 0.9601\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9519 - val_loss: 0.9549\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9453 - val_loss: 0.9468\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9418 - val_loss: 0.9447\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9395 - val_loss: 0.9439\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9386 - val_loss: 0.9435\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9360 - val_loss: 0.9479\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9335 - val_loss: 0.9443\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9303 - val_loss: 0.9362\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9279 - val_loss: 0.9349\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9280 - val_loss: 0.9386\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9252 - val_loss: 0.9314\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9249 - val_loss: 0.9386\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9242 - val_loss: 0.9312\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9218 - val_loss: 0.9319\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9216 - val_loss: 0.9310\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9204 - val_loss: 0.9296\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9193 - val_loss: 0.9337\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9190 - val_loss: 0.9285\n",
      "Top-2 accuracy = 0.86\n",
      "6\n",
      "robustr|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0593 - val_loss: 1.0287\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0104 - val_loss: 1.0023\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9899 - val_loss: 0.9866\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9778 - val_loss: 0.9779\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9701 - val_loss: 0.9713\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9642 - val_loss: 0.9662\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9602 - val_loss: 0.9628\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9574 - val_loss: 0.9599\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9546 - val_loss: 0.9574\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9528 - val_loss: 0.9562\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9518 - val_loss: 0.9559\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9501 - val_loss: 0.9534\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9480 - val_loss: 0.9533\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9474 - val_loss: 0.9502\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9466 - val_loss: 0.9493\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9445 - val_loss: 0.9490\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9436 - val_loss: 0.9489\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9424 - val_loss: 0.9469\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9412 - val_loss: 0.9464\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9409 - val_loss: 0.9455\n",
      "Top-2 accuracy = 0.85\n",
      "7\n",
      "normalizeZ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0688 - val_loss: 1.0320\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9961 - val_loss: 0.9745\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9628 - val_loss: 0.9604\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9512 - val_loss: 0.9523\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9450 - val_loss: 0.9475\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9410 - val_loss: 0.9445\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9380 - val_loss: 0.9457\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9359 - val_loss: 0.9428\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9333 - val_loss: 0.9420\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9320 - val_loss: 0.9387\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9296 - val_loss: 0.9376\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9290 - val_loss: 0.9379\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9277 - val_loss: 0.9348\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9268 - val_loss: 0.9346\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9249 - val_loss: 0.9339\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9253 - val_loss: 0.9339\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9244 - val_loss: 0.9341\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9226 - val_loss: 0.9323\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9230 - val_loss: 0.9315\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9218 - val_loss: 0.9326\n",
      "Top-2 accuracy = 0.85\n",
      "8\n",
      "standardizeS|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0534 - val_loss: 1.0070\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9766 - val_loss: 0.9676\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9567 - val_loss: 0.9633\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9513 - val_loss: 0.9583\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9491 - val_loss: 0.9527\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9451 - val_loss: 0.9545\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9438 - val_loss: 0.9512\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9431 - val_loss: 0.9606\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9412 - val_loss: 0.9518\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9406 - val_loss: 0.9505\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9385 - val_loss: 0.9456\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9377 - val_loss: 0.9487\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9354 - val_loss: 0.9543\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9360 - val_loss: 0.9500\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9364 - val_loss: 0.9516\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9340 - val_loss: 0.9453\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9332 - val_loss: 0.9545\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9351 - val_loss: 0.9552\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9341 - val_loss: 0.9454\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9316 - val_loss: 0.9455\n",
      "Top-2 accuracy = 0.85\n",
      "9\n",
      "minmaxD|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_461 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.1034 - val_loss: 1.0810\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0609 - val_loss: 1.0456\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0276 - val_loss: 1.0227\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0096 - val_loss: 1.0087\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9971 - val_loss: 0.9980\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9882 - val_loss: 0.9915\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9830 - val_loss: 0.9878\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9788 - val_loss: 0.9819\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9759 - val_loss: 0.9802\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9750 - val_loss: 0.9791\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9727 - val_loss: 0.9789\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9710 - val_loss: 0.9788\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9696 - val_loss: 0.9737\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9684 - val_loss: 0.9726\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9679 - val_loss: 0.9759\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9679 - val_loss: 0.9701\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9660 - val_loss: 0.9707\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9652 - val_loss: 0.9698\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9644 - val_loss: 0.9759\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9644 - val_loss: 0.9770\n",
      "Top-2 accuracy = 0.82\n",
      "10\n",
      "minmaxh|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0957 - val_loss: 1.0902\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0700 - val_loss: 1.0503\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0270 - val_loss: 1.0112\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9894 - val_loss: 1.0854\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9856 - val_loss: 1.0045\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9745 - val_loss: 0.9865\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9707 - val_loss: 0.9711\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9719 - val_loss: 0.9726\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9656 - val_loss: 0.9754\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9650 - val_loss: 0.9684\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9637 - val_loss: 0.9772\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9637 - val_loss: 1.0125\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9663 - val_loss: 0.9772\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9638 - val_loss: 0.9756\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9618 - val_loss: 0.9685\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9603 - val_loss: 0.9985\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9620 - val_loss: 0.9688\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9595 - val_loss: 0.9899\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9619 - val_loss: 0.9772\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9598 - val_loss: 0.9724\n",
      "Top-2 accuracy = 0.84\n",
      "11\n",
      "normalizeX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0882 - val_loss: 1.0610\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0108 - val_loss: 0.9866\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9784 - val_loss: 0.9778\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9726 - val_loss: 0.9701\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9678 - val_loss: 0.9679\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9646 - val_loss: 0.9655\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9626 - val_loss: 0.9649\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9606 - val_loss: 0.9643\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9608 - val_loss: 0.9634\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9586 - val_loss: 0.9592\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9565 - val_loss: 0.9605\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9556 - val_loss: 0.9593\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9557 - val_loss: 0.9635\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9541 - val_loss: 0.9563\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9535 - val_loss: 0.9557\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9509 - val_loss: 0.9575\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9538 - val_loss: 0.9580\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9488 - val_loss: 0.9523\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9472 - val_loss: 0.9554\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9477 - val_loss: 0.9506\n",
      "Top-2 accuracy = 0.85\n",
      "12\n",
      "standardizeA|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0844 - val_loss: 1.0584\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0429 - val_loss: 1.0323\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0245 - val_loss: 1.0184\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0130 - val_loss: 1.0089\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0035 - val_loss: 1.0021\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9960 - val_loss: 0.9945\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9902 - val_loss: 0.9908\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9855 - val_loss: 0.9883\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9819 - val_loss: 0.9862\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9788 - val_loss: 0.9787\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9758 - val_loss: 0.9793\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9728 - val_loss: 0.9811\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9724 - val_loss: 0.9712\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9679 - val_loss: 0.9688\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9655 - val_loss: 0.9689\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9638 - val_loss: 0.9655\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9622 - val_loss: 0.9690\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9599 - val_loss: 0.9653\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9581 - val_loss: 0.9629\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9573 - val_loss: 0.9622\n",
      "Top-2 accuracy = 0.84\n",
      "13\n",
      "standardizei|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0719 - val_loss: 1.0204\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9901 - val_loss: 0.9758\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9667 - val_loss: 0.9647\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9574 - val_loss: 0.9616\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9517 - val_loss: 0.9553\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9465 - val_loss: 0.9527\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9432 - val_loss: 0.9503\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9397 - val_loss: 0.9464\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9388 - val_loss: 0.9446\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9365 - val_loss: 0.9433\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9339 - val_loss: 0.9420\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9335 - val_loss: 0.9402\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9322 - val_loss: 0.9413\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9306 - val_loss: 0.9411\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9296 - val_loss: 0.9399\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9286 - val_loss: 0.9385\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9282 - val_loss: 0.9407\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9278 - val_loss: 0.9384\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9264 - val_loss: 0.9375\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9258 - val_loss: 0.9358\n",
      "Top-2 accuracy = 0.85\n",
      "14\n",
      "standardizeX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0947 - val_loss: 1.0892\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0683 - val_loss: 1.0467\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0217 - val_loss: 1.0053\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9898 - val_loss: 0.9806\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9732 - val_loss: 0.9716\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9662 - val_loss: 0.9680\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9621 - val_loss: 0.9677\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9591 - val_loss: 0.9670\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9580 - val_loss: 0.9611\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9562 - val_loss: 0.9582\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9533 - val_loss: 0.9562\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9518 - val_loss: 0.9544\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9500 - val_loss: 0.9658\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9495 - val_loss: 0.9531\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9484 - val_loss: 0.9525\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9475 - val_loss: 0.9521\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9462 - val_loss: 0.9491\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9446 - val_loss: 0.9543\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9446 - val_loss: 0.9475\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9439 - val_loss: 0.9510\n",
      "Top-2 accuracy = 0.84\n",
      "15\n",
      "robustG|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0828 - val_loss: 1.0559\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0235 - val_loss: 0.9990\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9844 - val_loss: 0.9739\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9648 - val_loss: 0.9639\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9562 - val_loss: 0.9566\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9524 - val_loss: 0.9554\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9497 - val_loss: 0.9560\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9481 - val_loss: 0.9515\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9465 - val_loss: 0.9535\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9446 - val_loss: 0.9512\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9453 - val_loss: 0.9514\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9443 - val_loss: 0.9488\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9424 - val_loss: 0.9483\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9424 - val_loss: 0.9471\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9414 - val_loss: 0.9544\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9412 - val_loss: 0.9465\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9400 - val_loss: 0.9462\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9397 - val_loss: 0.9506\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9403 - val_loss: 0.9475\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9396 - val_loss: 0.9457\n",
      "Top-2 accuracy = 0.85\n",
      "16\n",
      "maxabsR|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0971 - val_loss: 1.0964\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0955 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "17\n",
      "standardizeL|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0838 - val_loss: 1.0553\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0213 - val_loss: 1.0011\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9909 - val_loss: 0.9868\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9782 - val_loss: 0.9774\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9692 - val_loss: 0.9709\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9634 - val_loss: 0.9694\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9596 - val_loss: 0.9633\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9570 - val_loss: 0.9623\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9550 - val_loss: 0.9609\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9533 - val_loss: 0.9603\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9536 - val_loss: 0.9569\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9519 - val_loss: 0.9563\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9505 - val_loss: 0.9558\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9505 - val_loss: 0.9553\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9489 - val_loss: 0.9545\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9483 - val_loss: 0.9629\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9498 - val_loss: 0.9561\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9473 - val_loss: 0.9533\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9472 - val_loss: 0.9557\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9477 - val_loss: 0.9524\n",
      "Top-2 accuracy = 0.85\n",
      "18\n",
      "maxabsY|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0796 - val_loss: 1.0322\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9936 - val_loss: 0.9718\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9580 - val_loss: 0.9568\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9498 - val_loss: 0.9539\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9471 - val_loss: 0.9515\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9452 - val_loss: 0.9516\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9458 - val_loss: 0.9543\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9448 - val_loss: 0.9480\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9424 - val_loss: 0.9477\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9412 - val_loss: 0.9478\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9406 - val_loss: 0.9485\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9409 - val_loss: 0.9465\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9394 - val_loss: 0.9470\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9383 - val_loss: 0.9467\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9379 - val_loss: 0.9466\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9384 - val_loss: 0.9459\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9373 - val_loss: 0.9456\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9375 - val_loss: 0.9450\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9370 - val_loss: 0.9483\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9381 - val_loss: 0.9440\n",
      "Top-2 accuracy = 0.85\n",
      "19\n",
      "minmaxQ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0694 - val_loss: 1.0341\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0021 - val_loss: 0.9850\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9762 - val_loss: 0.9828\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9685 - val_loss: 0.9707\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9647 - val_loss: 0.9652\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9605 - val_loss: 0.9637\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9575 - val_loss: 0.9780\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9575 - val_loss: 0.9643\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9539 - val_loss: 0.9687\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9543 - val_loss: 0.9675\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9522 - val_loss: 0.9552\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9497 - val_loss: 0.9530\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9489 - val_loss: 0.9525\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9474 - val_loss: 0.9572\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9474 - val_loss: 0.9637\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9462 - val_loss: 0.9488\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9454 - val_loss: 0.9547\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9440 - val_loss: 0.9466\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9438 - val_loss: 0.9501\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9446 - val_loss: 0.9472\n",
      "Top-2 accuracy = 0.85\n",
      "20\n",
      "standardizeQ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0971 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0955 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "21\n",
      "maxabsH|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0874 - val_loss: 1.0777\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0566 - val_loss: 1.0391\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0271 - val_loss: 1.0259\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0191 - val_loss: 1.0188\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0125 - val_loss: 1.0125\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0056 - val_loss: 1.0026\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9967 - val_loss: 0.9939\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9880 - val_loss: 0.9870\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9798 - val_loss: 0.9806\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9744 - val_loss: 0.9793\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9711 - val_loss: 0.9762\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9682 - val_loss: 0.9729\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9661 - val_loss: 0.9731\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9641 - val_loss: 0.9706\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9625 - val_loss: 0.9690\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9611 - val_loss: 0.9671\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9599 - val_loss: 0.9666\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9588 - val_loss: 0.9653\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9577 - val_loss: 0.9640\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9566 - val_loss: 0.9631\n",
      "Top-2 accuracy = 0.84\n",
      "22\n",
      "maxabsc|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0539 - val_loss: 0.9869\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9570 - val_loss: 1.0085\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9459 - val_loss: 0.9453\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9363 - val_loss: 0.9487\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9343 - val_loss: 0.9451\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9319 - val_loss: 0.9491\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9324 - val_loss: 0.9401\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9294 - val_loss: 0.9397\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9293 - val_loss: 0.9395\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9280 - val_loss: 0.9386\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9276 - val_loss: 0.9431\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9271 - val_loss: 0.9412\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9282 - val_loss: 0.9422\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9249 - val_loss: 0.9373\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9260 - val_loss: 0.9375\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9250 - val_loss: 0.9364\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9241 - val_loss: 0.9380\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9259 - val_loss: 0.9354\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9242 - val_loss: 0.9396\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9231 - val_loss: 0.9374\n",
      "Top-2 accuracy = 0.86\n",
      "23\n",
      "normalizeS|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 1.0903 - val_loss: 1.0728\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0324 - val_loss: 0.9989\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9692 - val_loss: 0.9665\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9542 - val_loss: 0.9595\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9497 - val_loss: 0.9512\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9478 - val_loss: 0.9619\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9498 - val_loss: 0.9616\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9453 - val_loss: 0.9488\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9438 - val_loss: 0.9492\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9439 - val_loss: 0.9489\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9431 - val_loss: 0.9471\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9424 - val_loss: 0.9475\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9432 - val_loss: 0.9466\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9416 - val_loss: 0.9500\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9417 - val_loss: 0.9481\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9421 - val_loss: 0.9535\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9417 - val_loss: 0.9480\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9429 - val_loss: 0.9453\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9412 - val_loss: 0.9461\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9431 - val_loss: 0.9487\n",
      "Top-2 accuracy = 0.85\n",
      "24\n",
      "minmaxU|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_531 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.1118 - val_loss: 1.0954\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0917 - val_loss: 1.0908\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0847 - val_loss: 1.0820\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0736 - val_loss: 1.0729\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0635 - val_loss: 1.0620\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0499 - val_loss: 1.0477\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0364 - val_loss: 1.0356\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0255 - val_loss: 1.0260\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0172 - val_loss: 1.0185\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0100 - val_loss: 1.0117\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0042 - val_loss: 1.0065\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9999 - val_loss: 1.0034\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9959 - val_loss: 0.9991\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9932 - val_loss: 0.9968\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9904 - val_loss: 0.9941\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9881 - val_loss: 0.9922\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9862 - val_loss: 0.9902\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9845 - val_loss: 0.9888\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9832 - val_loss: 0.9875\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9825 - val_loss: 0.9863\n",
      "Top-2 accuracy = 0.82\n",
      "25\n",
      "normalizev|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0918 - val_loss: 1.0855\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0647 - val_loss: 1.0466\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0349 - val_loss: 1.0332\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0256 - val_loss: 1.0264\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0187 - val_loss: 1.0208\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0113 - val_loss: 1.0068\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9946 - val_loss: 0.9990\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9843 - val_loss: 0.9926\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9751 - val_loss: 0.9728\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9661 - val_loss: 0.9684\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9643 - val_loss: 0.9665\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9579 - val_loss: 0.9616\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9547 - val_loss: 0.9616\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9544 - val_loss: 0.9585\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9524 - val_loss: 0.9602\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9507 - val_loss: 0.9612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9515 - val_loss: 0.9600\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9479 - val_loss: 0.9533\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9466 - val_loss: 0.9558\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9483 - val_loss: 0.9507\n",
      "Top-2 accuracy = 0.84\n",
      "26\n",
      "robustd|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_538 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.2148 - val_loss: 1.0776\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0478 - val_loss: 1.0349\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0156 - val_loss: 1.0107\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9946 - val_loss: 0.9918\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9794 - val_loss: 0.9791\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9699 - val_loss: 0.9701\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9632 - val_loss: 0.9643\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9580 - val_loss: 0.9611\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9550 - val_loss: 0.9597\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9518 - val_loss: 0.9559\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9501 - val_loss: 0.9535\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9473 - val_loss: 0.9546\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9454 - val_loss: 0.9510\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9427 - val_loss: 0.9495\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9417 - val_loss: 0.9488\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9403 - val_loss: 0.9459\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9396 - val_loss: 0.9487\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9383 - val_loss: 0.9456\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9370 - val_loss: 0.9427\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9343 - val_loss: 0.9426\n",
      "Top-2 accuracy = 0.85\n",
      "27\n",
      "robustI|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0405 - val_loss: 0.9875\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9645 - val_loss: 0.9688\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9507 - val_loss: 0.9490\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9400 - val_loss: 0.9460\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9368 - val_loss: 0.9503\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9358 - val_loss: 0.9450\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9349 - val_loss: 0.9422\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9318 - val_loss: 0.9445\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9320 - val_loss: 0.9467\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9294 - val_loss: 0.9438\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9278 - val_loss: 0.9587\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9302 - val_loss: 0.9392\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9283 - val_loss: 0.9383\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9275 - val_loss: 0.9390\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9311 - val_loss: 0.9409\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9279 - val_loss: 0.9378\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9249 - val_loss: 0.9423\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9254 - val_loss: 0.9383\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9237 - val_loss: 0.9386\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9246 - val_loss: 0.9353\n",
      "Top-2 accuracy = 0.85\n",
      "28\n",
      "minmaxA|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0969 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0955 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "29\n",
      "minmaxd|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0968 - val_loss: 1.0962\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0963\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0963\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "0\n",
      "normalizex|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0954 - val_loss: 1.0958\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0925 - val_loss: 1.0835\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0569 - val_loss: 1.0428\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0337 - val_loss: 1.0316\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0230 - val_loss: 1.0244\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0103 - val_loss: 1.0053\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9985 - val_loss: 0.9990\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9925 - val_loss: 0.9909\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9868 - val_loss: 0.9919\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9873 - val_loss: 0.9910\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9830 - val_loss: 0.9866\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9819 - val_loss: 0.9839\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9801 - val_loss: 0.9839\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9787 - val_loss: 0.9815\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9807 - val_loss: 0.9831\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9771 - val_loss: 0.9777\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9766 - val_loss: 0.9765\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9761 - val_loss: 0.9859\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9784 - val_loss: 0.9758\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9737 - val_loss: 0.9757\n",
      "Top-2 accuracy = 0.83\n",
      "1\n",
      "minmaxv|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0885 - val_loss: 1.0627\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0338 - val_loss: 1.0188\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0023 - val_loss: 0.9965\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9876 - val_loss: 1.0028\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9805 - val_loss: 0.9823\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9744 - val_loss: 0.9832\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9712 - val_loss: 0.9725\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9681 - val_loss: 0.9686\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9656 - val_loss: 0.9707\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9633 - val_loss: 0.9670\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9619 - val_loss: 0.9642\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9606 - val_loss: 0.9634\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9598 - val_loss: 0.9674\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9600 - val_loss: 0.9618\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9600 - val_loss: 0.9687\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9580 - val_loss: 0.9633\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9565 - val_loss: 0.9600\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9557 - val_loss: 0.9761\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9560 - val_loss: 0.9596\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9556 - val_loss: 0.9588\n",
      "Top-2 accuracy = 0.84\n",
      "2\n",
      "normalizer|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0543 - val_loss: 1.0359\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0176 - val_loss: 1.0086\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9971 - val_loss: 0.9944\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9849 - val_loss: 0.9864\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9790 - val_loss: 0.9794\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9752 - val_loss: 0.9753\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9723 - val_loss: 0.9732\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9707 - val_loss: 0.9716\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9698 - val_loss: 0.9716\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9689 - val_loss: 0.9693\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9675 - val_loss: 0.9688\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9669 - val_loss: 0.9677\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9658 - val_loss: 0.9658\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9648 - val_loss: 0.9656\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9643 - val_loss: 0.9659\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9638 - val_loss: 0.9646\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9626 - val_loss: 0.9654\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9623 - val_loss: 0.9626\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9616 - val_loss: 0.9625\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9612 - val_loss: 0.9619\n",
      "Top-2 accuracy = 0.83\n",
      "3\n",
      "standardizeR|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0958 - val_loss: 1.0962\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0959\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0948 - val_loss: 1.0931\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0463 - val_loss: 1.0064\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9842 - val_loss: 0.9801\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9677 - val_loss: 0.9698\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9624 - val_loss: 0.9679\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9594 - val_loss: 0.9657\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9573 - val_loss: 0.9608\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9542 - val_loss: 0.9592\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9530 - val_loss: 0.9585\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9513 - val_loss: 0.9600\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9502 - val_loss: 0.9566\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9489 - val_loss: 0.9541\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9482 - val_loss: 0.9544\n",
      "Top-2 accuracy = 0.84\n",
      "4\n",
      "standardizeh|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_582 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0677 - val_loss: 1.0267\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9939 - val_loss: 0.9766\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9670 - val_loss: 0.9633\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9563 - val_loss: 0.9571\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9501 - val_loss: 0.9510\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9452 - val_loss: 0.9474\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9421 - val_loss: 0.9460\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9402 - val_loss: 0.9442\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9378 - val_loss: 0.9420\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9362 - val_loss: 0.9396\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9352 - val_loss: 0.9395\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9337 - val_loss: 0.9379\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9329 - val_loss: 0.9384\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9314 - val_loss: 0.9385\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9313 - val_loss: 0.9368\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9305 - val_loss: 0.9353\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9293 - val_loss: 0.9354\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9288 - val_loss: 0.9348\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9277 - val_loss: 0.9345\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9267 - val_loss: 0.9316\n",
      "Top-2 accuracy = 0.85\n",
      "5\n",
      "standardizef|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0508 - val_loss: 1.0051\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9799 - val_loss: 0.9670\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9570 - val_loss: 0.9545\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9481 - val_loss: 0.9504\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9443 - val_loss: 0.9479\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9423 - val_loss: 0.9449\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9399 - val_loss: 0.9441\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9386 - val_loss: 0.9433\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9365 - val_loss: 0.9408\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9354 - val_loss: 0.9386\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9334 - val_loss: 0.9379\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9320 - val_loss: 0.9386\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9304 - val_loss: 0.9392\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9304 - val_loss: 0.9365\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9289 - val_loss: 0.9349\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9275 - val_loss: 0.9332\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9265 - val_loss: 0.9336\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9273 - val_loss: 0.9320\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9253 - val_loss: 0.9325\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9256 - val_loss: 0.9326\n",
      "Top-2 accuracy = 0.85\n",
      "6\n",
      "robusta|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0570 - val_loss: 1.0173\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9917 - val_loss: 0.9762\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9657 - val_loss: 0.9661\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9577 - val_loss: 0.9573\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9521 - val_loss: 0.9570\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9516 - val_loss: 0.9521\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9474 - val_loss: 0.9500\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9458 - val_loss: 0.9593\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9470 - val_loss: 0.9465\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9418 - val_loss: 0.9474\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9416 - val_loss: 0.9452\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9409 - val_loss: 0.9494\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9386 - val_loss: 0.9446\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9401 - val_loss: 0.9455\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9386 - val_loss: 0.9445\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9374 - val_loss: 0.9475\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9393 - val_loss: 0.9422\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9380 - val_loss: 0.9422\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9360 - val_loss: 0.9426\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9345 - val_loss: 0.9445\n",
      "Top-2 accuracy = 0.84\n",
      "7\n",
      "minmaxG|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0881 - val_loss: 1.0609\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0287 - val_loss: 1.0001\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9816 - val_loss: 0.9678\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9592 - val_loss: 0.9569\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9522 - val_loss: 0.9529\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9507 - val_loss: 0.9515\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9484 - val_loss: 0.9503\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9477 - val_loss: 0.9548\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9476 - val_loss: 0.9477\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9469 - val_loss: 0.9481\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9459 - val_loss: 0.9466\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9447 - val_loss: 0.9489\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9439 - val_loss: 0.9452\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9432 - val_loss: 0.9458\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9444 - val_loss: 0.9453\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9425 - val_loss: 0.9446\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9434 - val_loss: 0.9443\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9426 - val_loss: 0.9518\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9418 - val_loss: 0.9500\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9454 - val_loss: 0.9459\n",
      "Top-2 accuracy = 0.84\n",
      "8\n",
      "minmaxX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0626 - val_loss: 1.0206\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9873 - val_loss: 0.9670\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9582 - val_loss: 0.9602\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9511 - val_loss: 0.9509\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9488 - val_loss: 0.9545\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9468 - val_loss: 0.9476\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9453 - val_loss: 0.9496\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9441 - val_loss: 0.9492\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9446 - val_loss: 0.9548\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9432 - val_loss: 0.9514\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9412 - val_loss: 0.9531\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9409 - val_loss: 0.9509\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9418 - val_loss: 0.9575\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9424 - val_loss: 0.9428\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9397 - val_loss: 0.9458\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9392 - val_loss: 0.9440\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9390 - val_loss: 0.9490\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9389 - val_loss: 0.9419\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9372 - val_loss: 0.9414\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9381 - val_loss: 0.9404\n",
      "Top-2 accuracy = 0.85\n",
      "9\n",
      "normalizef|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0881 - val_loss: 1.0702\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0375 - val_loss: 1.0256\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0059 - val_loss: 1.0068\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0001 - val_loss: 1.0108\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9983 - val_loss: 1.0003\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9935 - val_loss: 0.9999\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9964 - val_loss: 0.9984\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9921 - val_loss: 0.9975\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9914 - val_loss: 0.9961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9915 - val_loss: 0.9949\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9901 - val_loss: 0.9943\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9900 - val_loss: 0.9939\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9907 - val_loss: 0.9958\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9881 - val_loss: 0.9957\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9876 - val_loss: 0.9975\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9870 - val_loss: 0.9942\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9882 - val_loss: 0.9909\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9854 - val_loss: 1.0016\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9865 - val_loss: 1.0050\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9885 - val_loss: 0.9918\n",
      "Top-2 accuracy = 0.83\n",
      "10\n",
      "robustX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0817 - val_loss: 1.0534\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0310 - val_loss: 1.0191\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0084 - val_loss: 1.0052\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9964 - val_loss: 0.9952\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9865 - val_loss: 0.9872\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9782 - val_loss: 0.9811\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9729 - val_loss: 0.9760\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9684 - val_loss: 0.9800\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9656 - val_loss: 0.9688\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9598 - val_loss: 0.9649\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9567 - val_loss: 0.9619\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9549 - val_loss: 0.9659\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9528 - val_loss: 0.9590\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9506 - val_loss: 0.9562\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9490 - val_loss: 0.9609\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9471 - val_loss: 0.9534\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9457 - val_loss: 0.9503\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9452 - val_loss: 0.9509\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9438 - val_loss: 0.9513\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9441 - val_loss: 0.9508\n",
      "Top-2 accuracy = 0.85\n",
      "11\n",
      "robuste|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0487 - val_loss: 1.0024\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9765 - val_loss: 0.9580\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9444 - val_loss: 0.9609\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9389 - val_loss: 0.9579\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9380 - val_loss: 0.9444\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9364 - val_loss: 0.9513\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9349 - val_loss: 0.9451\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9341 - val_loss: 0.9438\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9339 - val_loss: 0.9492\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9337 - val_loss: 0.9433\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9332 - val_loss: 0.9403\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9326 - val_loss: 0.9448\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9320 - val_loss: 0.9404\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9323 - val_loss: 0.9393\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9323 - val_loss: 0.9392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9324 - val_loss: 0.9464\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9310 - val_loss: 0.9395\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9304 - val_loss: 0.9499\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9305 - val_loss: 0.9386\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9301 - val_loss: 0.9386\n",
      "Top-2 accuracy = 0.85\n",
      "12\n",
      "maxabsN|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0860 - val_loss: 1.0591\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0282 - val_loss: 1.0168\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0021 - val_loss: 1.0074\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9907 - val_loss: 0.9919\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9838 - val_loss: 0.9911\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9799 - val_loss: 0.9862\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9785 - val_loss: 0.9895\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9773 - val_loss: 0.9861\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9757 - val_loss: 0.9833\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9756 - val_loss: 0.9821\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9753 - val_loss: 0.9827\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9736 - val_loss: 0.9806\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9730 - val_loss: 0.9792\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9718 - val_loss: 0.9787\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9711 - val_loss: 0.9788\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9706 - val_loss: 0.9773\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9708 - val_loss: 0.9776\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9706 - val_loss: 0.9771\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9691 - val_loss: 0.9762\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9687 - val_loss: 0.9829\n",
      "Top-2 accuracy = 0.83\n",
      "13\n",
      "robustR|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0695 - val_loss: 1.0273\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0032 - val_loss: 0.9930\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9801 - val_loss: 0.9748\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9652 - val_loss: 0.9636\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9581 - val_loss: 0.9584\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9538 - val_loss: 0.9550\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9513 - val_loss: 0.9564\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9495 - val_loss: 0.9518\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9474 - val_loss: 0.9520\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9455 - val_loss: 0.9507\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9450 - val_loss: 0.9483\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9436 - val_loss: 0.9480\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9419 - val_loss: 0.9481\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9416 - val_loss: 0.9454\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9409 - val_loss: 0.9471\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9407 - val_loss: 0.9448\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9403 - val_loss: 0.9441\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9389 - val_loss: 0.9461\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9392 - val_loss: 0.9447\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9379 - val_loss: 0.9437\n",
      "Top-2 accuracy = 0.85\n",
      "14\n",
      "standardizeO|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0621 - val_loss: 1.0047\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9694 - val_loss: 0.9555\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9494 - val_loss: 0.9577\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9447 - val_loss: 0.9472\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9411 - val_loss: 0.9468\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9387 - val_loss: 0.9476\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9387 - val_loss: 0.9519\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9394 - val_loss: 0.9487\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9377 - val_loss: 0.9442\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9393 - val_loss: 0.9417\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9347 - val_loss: 0.9444\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9351 - val_loss: 0.9404\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9339 - val_loss: 0.9444\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9360 - val_loss: 0.9396\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9329 - val_loss: 0.9415\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9322 - val_loss: 0.9417\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9320 - val_loss: 0.9386\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9327 - val_loss: 0.9398\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9321 - val_loss: 0.9381\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9313 - val_loss: 0.9375\n",
      "Top-2 accuracy = 0.86\n",
      "15\n",
      "minmaxx|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0703 - val_loss: 1.0211\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9850 - val_loss: 0.9958\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9696 - val_loss: 0.9631\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9588 - val_loss: 0.9631\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9564 - val_loss: 0.9606\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9558 - val_loss: 0.9569\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9549 - val_loss: 0.9578\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9536 - val_loss: 0.9693\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9557 - val_loss: 1.0019\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9667 - val_loss: 0.9571\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9552 - val_loss: 0.9670\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9526 - val_loss: 0.9724\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9527 - val_loss: 0.9550\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9517 - val_loss: 0.9557\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9551 - val_loss: 0.9534\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9497 - val_loss: 0.9560\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9499 - val_loss: 0.9598\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9495 - val_loss: 0.9726\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9527 - val_loss: 0.9557\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9523 - val_loss: 0.9571\n",
      "Top-2 accuracy = 0.84\n",
      "16\n",
      "maxabsc|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0947 - val_loss: 1.0941\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0570 - val_loss: 1.0229\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9985 - val_loss: 1.0178\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9803 - val_loss: 0.9725\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9675 - val_loss: 0.9683\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9616 - val_loss: 0.9857\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9609 - val_loss: 0.9576\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9544 - val_loss: 0.9924\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9575 - val_loss: 0.9518\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9518 - val_loss: 0.9511\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9511 - val_loss: 0.9509\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9500 - val_loss: 0.9519\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9487 - val_loss: 0.9523\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9486 - val_loss: 0.9508\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9479 - val_loss: 0.9473\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9489 - val_loss: 0.9528\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9483 - val_loss: 0.9550\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9475 - val_loss: 0.9475\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9451 - val_loss: 0.9516\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9453 - val_loss: 0.9459\n",
      "Top-2 accuracy = 0.85\n",
      "17\n",
      "robusth|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0614 - val_loss: 1.0155\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9838 - val_loss: 0.9697\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9584 - val_loss: 0.9555\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9481 - val_loss: 0.9477\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9419 - val_loss: 0.9428\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9384 - val_loss: 0.9414\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9351 - val_loss: 0.9395\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9333 - val_loss: 0.9386\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9309 - val_loss: 0.9357\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9298 - val_loss: 0.9372\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9289 - val_loss: 0.9349\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9269 - val_loss: 0.9346\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9266 - val_loss: 0.9349\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9258 - val_loss: 0.9332\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9252 - val_loss: 0.9342\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9254 - val_loss: 0.9329\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9238 - val_loss: 0.9319\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9227 - val_loss: 0.9314\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9220 - val_loss: 0.9312\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9214 - val_loss: 0.9337\n",
      "Top-2 accuracy = 0.85\n",
      "18\n",
      "normalizeM|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0273 - val_loss: 0.9697\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9479 - val_loss: 0.9452\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9374 - val_loss: 0.9413\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9337 - val_loss: 0.9412\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9324 - val_loss: 0.9392\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9314 - val_loss: 0.9374\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9299 - val_loss: 0.9385\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9291 - val_loss: 0.9363\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9282 - val_loss: 0.9365\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9264 - val_loss: 0.9366\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9266 - val_loss: 0.9343\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9247 - val_loss: 0.9350\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9240 - val_loss: 0.9318\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9224 - val_loss: 0.9313\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9215 - val_loss: 0.9285\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9198 - val_loss: 0.9298\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9197 - val_loss: 0.9294\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9192 - val_loss: 0.9310\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9196 - val_loss: 0.9269\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9185 - val_loss: 0.9325\n",
      "Top-2 accuracy = 0.85\n",
      "19\n",
      "minmaxn|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.1065 - val_loss: 1.0970\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0960 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0954 - val_loss: 1.0960\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0963\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0953 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "20\n",
      "maxabsA|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0728 - val_loss: 1.0014\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9776 - val_loss: 0.9889\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9634 - val_loss: 0.9682\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9605 - val_loss: 0.9673\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9542 - val_loss: 0.9529\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9514 - val_loss: 0.9519\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9509 - val_loss: 0.9640\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9493 - val_loss: 0.9533\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9491 - val_loss: 0.9764\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9482 - val_loss: 0.9628\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9460 - val_loss: 0.9487\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9451 - val_loss: 0.9495\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9456 - val_loss: 0.9499\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9449 - val_loss: 0.9491\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9435 - val_loss: 0.9454\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9441 - val_loss: 0.9547\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9430 - val_loss: 0.9512\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9424 - val_loss: 0.9494\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9404 - val_loss: 0.9522\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9413 - val_loss: 0.9430\n",
      "Top-2 accuracy = 0.85\n",
      "21\n",
      "minmaxR|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_662 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0933 - val_loss: 1.0897\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0842 - val_loss: 1.0796\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0717 - val_loss: 1.0654\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0557 - val_loss: 1.0497\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0379 - val_loss: 1.0330\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0236 - val_loss: 1.0195\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0101 - val_loss: 1.0088\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9993 - val_loss: 0.9978\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9895 - val_loss: 0.9889\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9826 - val_loss: 0.9810\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9764 - val_loss: 0.9812\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9715 - val_loss: 0.9769\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9663 - val_loss: 0.9668\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9624 - val_loss: 0.9639\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9610 - val_loss: 0.9610\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9586 - val_loss: 0.9601\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9582 - val_loss: 0.9604\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9563 - val_loss: 0.9559\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9536 - val_loss: 0.9559\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9541 - val_loss: 0.9570\n",
      "Top-2 accuracy = 0.84\n",
      "22\n",
      "normalizeY|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0826 - val_loss: 1.0586\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0176 - val_loss: 0.9967\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9772 - val_loss: 0.9671\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9625 - val_loss: 0.9709\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9605 - val_loss: 0.9581\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9585 - val_loss: 0.9577\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9541 - val_loss: 0.9571\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9534 - val_loss: 0.9565\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9528 - val_loss: 0.9563\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9523 - val_loss: 0.9615\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9521 - val_loss: 0.9538\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9507 - val_loss: 0.9538\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9505 - val_loss: 0.9559\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9510 - val_loss: 0.9542\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9489 - val_loss: 0.9525\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9493 - val_loss: 0.9541\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9487 - val_loss: 0.9535\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9488 - val_loss: 0.9655\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9486 - val_loss: 0.9515\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9473 - val_loss: 0.9603\n",
      "Top-2 accuracy = 0.84\n",
      "23\n",
      "normalizeR|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0972 - val_loss: 1.0956\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0904 - val_loss: 1.0787\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0393 - val_loss: 1.0062\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9892 - val_loss: 0.9789\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9763 - val_loss: 0.9834\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9695 - val_loss: 0.9678\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9651 - val_loss: 0.9722\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9633 - val_loss: 0.9620\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9605 - val_loss: 0.9609\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9597 - val_loss: 0.9587\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9585 - val_loss: 0.9567\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9576 - val_loss: 0.9565\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9550 - val_loss: 0.9613\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9546 - val_loss: 0.9583\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9538 - val_loss: 0.9535\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9520 - val_loss: 0.9512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9525 - val_loss: 0.9553\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9510 - val_loss: 0.9500\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9497 - val_loss: 0.9518\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9489 - val_loss: 0.9489\n",
      "Top-2 accuracy = 0.85\n",
      "24\n",
      "normalizeb|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0947 - val_loss: 1.0913\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0694 - val_loss: 1.0428\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0227 - val_loss: 1.0117\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0046 - val_loss: 1.0023\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0000 - val_loss: 0.9981\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9965 - val_loss: 0.9953\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9951 - val_loss: 0.9955\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9932 - val_loss: 0.9937\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9920 - val_loss: 0.9910\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9898 - val_loss: 0.9898\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9887 - val_loss: 0.9909\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9897 - val_loss: 0.9887\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9863 - val_loss: 0.9855\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9853 - val_loss: 0.9847\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9829 - val_loss: 0.9854\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9821 - val_loss: 0.9827\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9813 - val_loss: 0.9802\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9788 - val_loss: 0.9797\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9774 - val_loss: 0.9780\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9756 - val_loss: 0.9766\n",
      "Top-2 accuracy = 0.83\n",
      "25\n",
      "standardizez|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_684 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0797 - val_loss: 1.0504\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0145 - val_loss: 0.9913\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9749 - val_loss: 0.9707\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9600 - val_loss: 0.9603\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9526 - val_loss: 0.9555\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9479 - val_loss: 0.9518\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9434 - val_loss: 0.9492\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9413 - val_loss: 0.9478\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9385 - val_loss: 0.9464\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9365 - val_loss: 0.9429\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9351 - val_loss: 0.9421\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9341 - val_loss: 0.9412\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9340 - val_loss: 0.9398\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9332 - val_loss: 0.9408\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9314 - val_loss: 0.9379\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9307 - val_loss: 0.9378\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9307 - val_loss: 0.9389\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9296 - val_loss: 0.9408\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9298 - val_loss: 0.9386\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9292 - val_loss: 0.9372\n",
      "Top-2 accuracy = 0.85\n",
      "26\n",
      "standardizeS|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0595 - val_loss: 1.0155\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9924 - val_loss: 0.9848\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9736 - val_loss: 0.9729\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9649 - val_loss: 0.9661\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9592 - val_loss: 0.9615\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9547 - val_loss: 0.9557\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9507 - val_loss: 0.9527\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9475 - val_loss: 0.9503\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9454 - val_loss: 0.9497\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9435 - val_loss: 0.9471\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9430 - val_loss: 0.9466\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9409 - val_loss: 0.9452\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9403 - val_loss: 0.9469\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9389 - val_loss: 0.9453\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9378 - val_loss: 0.9429\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9366 - val_loss: 0.9419\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9358 - val_loss: 0.9427\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9345 - val_loss: 0.9411\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9342 - val_loss: 0.9431\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9335 - val_loss: 0.9397\n",
      "Top-2 accuracy = 0.85\n",
      "27\n",
      "maxabsl|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0707 - val_loss: 1.0191\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9856 - val_loss: 0.9739\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9612 - val_loss: 0.9616\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9527 - val_loss: 0.9552\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9480 - val_loss: 0.9527\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9460 - val_loss: 0.9491\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9427 - val_loss: 0.9474\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9411 - val_loss: 0.9463\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9406 - val_loss: 0.9466\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9394 - val_loss: 0.9455\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9389 - val_loss: 0.9430\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9374 - val_loss: 0.9424\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9359 - val_loss: 0.9424\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9352 - val_loss: 0.9400\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9349 - val_loss: 0.9417\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9344 - val_loss: 0.9410\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9327 - val_loss: 0.9380\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9315 - val_loss: 0.9372\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9307 - val_loss: 0.9365\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9302 - val_loss: 0.9396\n",
      "Top-2 accuracy = 0.85\n",
      "28\n",
      "maxabsi|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0954 - val_loss: 1.0901\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0623 - val_loss: 1.0370\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0124 - val_loss: 0.9975\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9836 - val_loss: 0.9778\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9685 - val_loss: 0.9670\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9615 - val_loss: 0.9640\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9596 - val_loss: 0.9597\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9542 - val_loss: 0.9560\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9527 - val_loss: 0.9618\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9511 - val_loss: 0.9589\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9507 - val_loss: 0.9514\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9475 - val_loss: 0.9586\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9487 - val_loss: 0.9546\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9467 - val_loss: 0.9513\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9449 - val_loss: 0.9514\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9441 - val_loss: 0.9545\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9459 - val_loss: 0.9484\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9433 - val_loss: 0.9492\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9432 - val_loss: 0.9484\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9431 - val_loss: 0.9479\n",
      "Top-2 accuracy = 0.85\n",
      "29\n",
      "robusto|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0931 - val_loss: 1.0877\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0768 - val_loss: 1.0648\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0488 - val_loss: 1.0381\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0287 - val_loss: 1.0254\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0183 - val_loss: 1.0158\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0101 - val_loss: 1.0093\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0047 - val_loss: 1.0049\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0002 - val_loss: 1.0021\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9960 - val_loss: 0.9995\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9915 - val_loss: 0.9910\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9868 - val_loss: 0.9877\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9838 - val_loss: 0.9846\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9793 - val_loss: 0.9823\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9781 - val_loss: 0.9805\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9755 - val_loss: 0.9777\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9738 - val_loss: 0.9768\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9718 - val_loss: 0.9739\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9708 - val_loss: 0.9724\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9694 - val_loss: 0.9750\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9685 - val_loss: 0.9708\n",
      "Top-2 accuracy = 0.84\n",
      "0\n",
      "robustq|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_711 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0781 - val_loss: 1.0136\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9924 - val_loss: 0.9838\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9704 - val_loss: 0.9688\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9589 - val_loss: 0.9606\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9515 - val_loss: 0.9557\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9461 - val_loss: 0.9513\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9426 - val_loss: 0.9494\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9396 - val_loss: 0.9463\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9364 - val_loss: 0.9452\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9348 - val_loss: 0.9436\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9330 - val_loss: 0.9422\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9314 - val_loss: 0.9401\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9297 - val_loss: 0.9388\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9291 - val_loss: 0.9395\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9283 - val_loss: 0.9369\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9267 - val_loss: 0.9367\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9257 - val_loss: 0.9361\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9248 - val_loss: 0.9357\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9244 - val_loss: 0.9353\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9236 - val_loss: 0.9351\n",
      "Top-2 accuracy = 0.85\n",
      "1\n",
      "maxabsL|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0968 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0953 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "2\n",
      "robustz|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_721 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0968 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0954 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "3\n",
      "maxabsX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0958 - val_loss: 1.0960\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0885 - val_loss: 1.0469\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9789 - val_loss: 0.9562\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9388 - val_loss: 0.9484\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9370 - val_loss: 0.9631\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9380 - val_loss: 0.9475\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9318 - val_loss: 0.9379\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9297 - val_loss: 0.9734\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9335 - val_loss: 0.9352\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9266 - val_loss: 0.9380\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9265 - val_loss: 0.9441\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9297 - val_loss: 0.9407\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9253 - val_loss: 0.9429\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9265 - val_loss: 0.9364\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9242 - val_loss: 0.9398\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9228 - val_loss: 0.9347\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9224 - val_loss: 0.9339\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9217 - val_loss: 0.9382\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9229 - val_loss: 0.9362\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9231 - val_loss: 0.9350\n",
      "Top-2 accuracy = 0.86\n",
      "4\n",
      "minmaxP|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0867 - val_loss: 1.0427\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0003 - val_loss: 0.9887\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9825 - val_loss: 0.9840\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9766 - val_loss: 0.9788\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9737 - val_loss: 0.9746\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9707 - val_loss: 0.9724\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9718 - val_loss: 0.9896\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9691 - val_loss: 0.9740\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9690 - val_loss: 0.9689\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9660 - val_loss: 0.9734\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9657 - val_loss: 0.9764\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9695 - val_loss: 0.9668\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9651 - val_loss: 0.9689\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9659 - val_loss: 0.9718\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9653 - val_loss: 0.9673\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9666 - val_loss: 0.9662\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9648 - val_loss: 0.9674\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9649 - val_loss: 0.9656\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9632 - val_loss: 0.9647\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9622 - val_loss: 0.9716\n",
      "Top-2 accuracy = 0.83\n",
      "5\n",
      "minmaxH|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0964 - val_loss: 1.0958\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0881 - val_loss: 1.0603\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0106 - val_loss: 0.9938\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9776 - val_loss: 0.9774\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9699 - val_loss: 0.9870\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9709 - val_loss: 0.9663\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9650 - val_loss: 0.9646\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9641 - val_loss: 0.9713\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9630 - val_loss: 0.9693\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9626 - val_loss: 0.9621\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9609 - val_loss: 0.9621\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9606 - val_loss: 0.9610\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9608 - val_loss: 0.9690\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9600 - val_loss: 0.9628\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9595 - val_loss: 0.9599\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9583 - val_loss: 0.9644\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9590 - val_loss: 0.9596\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9574 - val_loss: 0.9580\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9563 - val_loss: 0.9605\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9560 - val_loss: 0.9596\n",
      "Top-2 accuracy = 0.84\n",
      "6\n",
      "minmaxa|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_744 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0925 - val_loss: 1.0839\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0732 - val_loss: 1.0650\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0511 - val_loss: 1.0446\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0301 - val_loss: 1.0257\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0151 - val_loss: 1.0140\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0064 - val_loss: 1.0067\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0006 - val_loss: 1.0022\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9972 - val_loss: 0.9995\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9940 - val_loss: 0.9965\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9918 - val_loss: 0.9949\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9900 - val_loss: 0.9928\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9886 - val_loss: 0.9917\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9873 - val_loss: 0.9902\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9858 - val_loss: 0.9887\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9844 - val_loss: 0.9873\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9837 - val_loss: 0.9864\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9820 - val_loss: 0.9859\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9807 - val_loss: 0.9838\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9796 - val_loss: 0.9834\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9793 - val_loss: 0.9836\n",
      "Top-2 accuracy = 0.82\n",
      "7\n",
      "minmaxj|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 1.0968 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0954 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0963\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Top-2 accuracy = 0.7\n",
      "8\n",
      "standardizea|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0712 - val_loss: 1.0456\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0343 - val_loss: 1.0271\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0201 - val_loss: 1.0170\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0103 - val_loss: 1.0097\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0022 - val_loss: 1.0036\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9949 - val_loss: 0.9974\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9888 - val_loss: 0.9921\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9837 - val_loss: 0.9869\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9794 - val_loss: 0.9840\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9755 - val_loss: 0.9789\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9725 - val_loss: 0.9763\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9694 - val_loss: 0.9737\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9674 - val_loss: 0.9723\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9657 - val_loss: 0.9702\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9641 - val_loss: 0.9687\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9626 - val_loss: 0.9674\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9616 - val_loss: 0.9663\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9604 - val_loss: 0.9652\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9601 - val_loss: 0.9651\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9586 - val_loss: 0.9641\n",
      "Top-2 accuracy = 0.85\n",
      "9\n",
      "minmaxB|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0646 - val_loss: 1.0268\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0008 - val_loss: 0.9958\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9696 - val_loss: 0.9866\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9643 - val_loss: 0.9629\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9559 - val_loss: 0.9582\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9534 - val_loss: 0.9687\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9528 - val_loss: 0.9612\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9503 - val_loss: 0.9526\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9475 - val_loss: 0.9591\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9491 - val_loss: 0.9679\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9468 - val_loss: 0.9481\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9445 - val_loss: 0.9481\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9432 - val_loss: 0.9587\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9437 - val_loss: 0.9482\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9429 - val_loss: 0.9611\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9430 - val_loss: 0.9491\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9415 - val_loss: 0.9507\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9433 - val_loss: 0.9450\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9404 - val_loss: 0.9462\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9397 - val_loss: 0.9468\n",
      "Top-2 accuracy = 0.85\n",
      "10\n",
      "maxabsw|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0971 - val_loss: 1.0936\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0879 - val_loss: 1.0781\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0578 - val_loss: 1.0425\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0289 - val_loss: 1.0268\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0165 - val_loss: 1.0185\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0120 - val_loss: 1.0081\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0050 - val_loss: 1.0032\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9996 - val_loss: 0.9999\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9962 - val_loss: 1.0038\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9963 - val_loss: 1.0001\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9942 - val_loss: 0.9941\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9916 - val_loss: 0.9932\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9904 - val_loss: 0.9945\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9898 - val_loss: 0.9969\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9905 - val_loss: 0.9959\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9881 - val_loss: 0.9942\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9884 - val_loss: 1.0048\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9888 - val_loss: 0.9897\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9865 - val_loss: 0.9876\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9854 - val_loss: 0.9964\n",
      "Top-2 accuracy = 0.82\n",
      "11\n",
      "standardizeX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 1.0956 - val_loss: 1.0960\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0913 - val_loss: 1.0721\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0243 - val_loss: 1.0037\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9824 - val_loss: 0.9679\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9575 - val_loss: 0.9559\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9491 - val_loss: 0.9710\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9481 - val_loss: 0.9482\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9451 - val_loss: 0.9473\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9443 - val_loss: 0.9485\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9451 - val_loss: 0.9461\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9429 - val_loss: 0.9561\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9437 - val_loss: 0.9486\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9417 - val_loss: 0.9743\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9434 - val_loss: 0.9450\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9422 - val_loss: 0.9490\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9433 - val_loss: 0.9545\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9431 - val_loss: 0.9439\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9411 - val_loss: 0.9443\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9410 - val_loss: 0.9439\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9397 - val_loss: 0.9487\n",
      "Top-2 accuracy = 0.85\n",
      "12\n",
      "maxabsC|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.1058 - val_loss: 1.0508\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0424 - val_loss: 1.0305\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0263 - val_loss: 1.0178\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0145 - val_loss: 1.0081\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0051 - val_loss: 1.0025\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9985 - val_loss: 0.9947\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9915 - val_loss: 0.9894\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9863 - val_loss: 0.9848\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9814 - val_loss: 0.9810\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9774 - val_loss: 0.9771\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9737 - val_loss: 0.9744\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9705 - val_loss: 0.9719\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9675 - val_loss: 0.9705\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9654 - val_loss: 0.9671\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9636 - val_loss: 0.9645\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9607 - val_loss: 0.9640\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9591 - val_loss: 0.9627\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9572 - val_loss: 0.9597\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9562 - val_loss: 0.9589\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9543 - val_loss: 0.9588\n",
      "Top-2 accuracy = 0.85\n",
      "13\n",
      "standardizeh|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0969 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0954 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "14\n",
      "standardizez|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_780 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0714 - val_loss: 1.0496\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0215 - val_loss: 1.0088\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9905 - val_loss: 0.9856\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9727 - val_loss: 0.9712\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9619 - val_loss: 0.9644\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9558 - val_loss: 0.9577\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9518 - val_loss: 0.9555\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9491 - val_loss: 0.9533\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9472 - val_loss: 0.9522\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9452 - val_loss: 0.9502\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9437 - val_loss: 0.9501\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9421 - val_loss: 0.9483\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9409 - val_loss: 0.9478\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9400 - val_loss: 0.9470\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9389 - val_loss: 0.9453\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9380 - val_loss: 0.9447\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9379 - val_loss: 0.9434\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9361 - val_loss: 0.9433\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9354 - val_loss: 0.9421\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9344 - val_loss: 0.9422\n",
      "Top-2 accuracy = 0.85\n",
      "15\n",
      "minmaxc|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0922\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0697 - val_loss: 1.0327\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9966 - val_loss: 0.9924\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9757 - val_loss: 0.9766\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9674 - val_loss: 0.9710\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9605 - val_loss: 0.9638\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9583 - val_loss: 0.9643\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9546 - val_loss: 0.9599\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9530 - val_loss: 0.9600\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9518 - val_loss: 0.9567\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9504 - val_loss: 0.9566\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9493 - val_loss: 0.9541\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9493 - val_loss: 0.9547\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9467 - val_loss: 0.9646\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9523 - val_loss: 0.9550\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9463 - val_loss: 0.9512\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9447 - val_loss: 0.9504\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9441 - val_loss: 0.9537\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9446 - val_loss: 0.9540\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9440 - val_loss: 0.9516\n",
      "Top-2 accuracy = 0.85\n",
      "16\n",
      "maxabsp|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0502 - val_loss: 1.0037\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9860 - val_loss: 0.9882\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9706 - val_loss: 0.9668\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9606 - val_loss: 0.9648\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9568 - val_loss: 0.9600\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9521 - val_loss: 0.9546\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9515 - val_loss: 0.9560\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9489 - val_loss: 0.9621\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9556 - val_loss: 0.9502\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9481 - val_loss: 0.9549\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9460 - val_loss: 0.9500\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9458 - val_loss: 0.9486\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9470 - val_loss: 0.9491\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9445 - val_loss: 0.9777\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9473 - val_loss: 0.9502\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9442 - val_loss: 0.9461\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9439 - val_loss: 0.9454\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9444 - val_loss: 0.9482\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9441 - val_loss: 0.9529\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9430 - val_loss: 0.9462\n",
      "Top-2 accuracy = 0.85\n",
      "17\n",
      "minmaxM|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0967 - val_loss: 1.0962\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0953 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "18\n",
      "standardizeu|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0618 - val_loss: 1.0252\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9976 - val_loss: 0.9747\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9622 - val_loss: 0.9612\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9509 - val_loss: 0.9494\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9454 - val_loss: 0.9479\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9437 - val_loss: 0.9475\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9425 - val_loss: 0.9468\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9416 - val_loss: 0.9465\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9412 - val_loss: 0.9502\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9402 - val_loss: 0.9464\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9400 - val_loss: 0.9460\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9398 - val_loss: 0.9490\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9415 - val_loss: 0.9454\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9396 - val_loss: 0.9477\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9385 - val_loss: 0.9456\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9393 - val_loss: 0.9488\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9389 - val_loss: 0.9450\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9376 - val_loss: 0.9433\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9373 - val_loss: 0.9455\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9374 - val_loss: 0.9492\n",
      "Top-2 accuracy = 0.85\n",
      "19\n",
      "normalizee|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0968 - val_loss: 1.0965\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0954 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0963\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "20\n",
      "normalizen|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0638 - val_loss: 1.0089\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9821 - val_loss: 0.9615\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9506 - val_loss: 0.9544\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9449 - val_loss: 0.9484\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9401 - val_loss: 0.9435\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9388 - val_loss: 0.9423\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9364 - val_loss: 0.9448\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9370 - val_loss: 0.9498\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9355 - val_loss: 0.9394\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9323 - val_loss: 0.9418\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9318 - val_loss: 0.9357\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9289 - val_loss: 0.9379\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9290 - val_loss: 0.9382\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9286 - val_loss: 0.9341\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9281 - val_loss: 0.9396\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9270 - val_loss: 0.9350\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9267 - val_loss: 0.9363\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9254 - val_loss: 0.9362\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9255 - val_loss: 0.9333\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9250 - val_loss: 0.9334\n",
      "Top-2 accuracy = 0.85\n",
      "21\n",
      "maxabsS|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0842 - val_loss: 1.0582\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0232 - val_loss: 1.0010\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9799 - val_loss: 0.9859\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9638 - val_loss: 0.9581\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9558 - val_loss: 0.9694\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9510 - val_loss: 0.9497\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9468 - val_loss: 0.9520\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9438 - val_loss: 0.9518\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9433 - val_loss: 0.9715\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9441 - val_loss: 0.9467\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9416 - val_loss: 0.9480\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9412 - val_loss: 0.9576\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9428 - val_loss: 0.9482\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9391 - val_loss: 0.9448\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9383 - val_loss: 0.9539\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9398 - val_loss: 0.9474\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9378 - val_loss: 0.9442\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9379 - val_loss: 0.9452\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9372 - val_loss: 0.9481\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9401 - val_loss: 0.9498\n",
      "Top-2 accuracy = 0.85\n",
      "22\n",
      "normalizeC|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0598 - val_loss: 1.0086\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9833 - val_loss: 0.9686\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9592 - val_loss: 0.9614\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9526 - val_loss: 0.9604\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9478 - val_loss: 0.9551\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9455 - val_loss: 0.9515\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9430 - val_loss: 0.9564\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9428 - val_loss: 0.9550\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9422 - val_loss: 0.9475\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9385 - val_loss: 0.9456\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9373 - val_loss: 0.9462\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9373 - val_loss: 0.9459\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9366 - val_loss: 0.9435\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9347 - val_loss: 0.9424\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9340 - val_loss: 0.9448\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9326 - val_loss: 0.9436\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9349 - val_loss: 0.9413\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9313 - val_loss: 0.9423\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9308 - val_loss: 0.9415\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9300 - val_loss: 0.9383\n",
      "Top-2 accuracy = 0.85\n",
      "23\n",
      "maxabsm|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0854 - val_loss: 1.0472\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0055 - val_loss: 0.9795\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9690 - val_loss: 0.9591\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9560 - val_loss: 0.9612\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9529 - val_loss: 0.9522\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9487 - val_loss: 0.9515\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9465 - val_loss: 0.9493\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9449 - val_loss: 0.9501\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9447 - val_loss: 0.9463\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9431 - val_loss: 0.9443\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9405 - val_loss: 0.9424\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9415 - val_loss: 0.9440\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9388 - val_loss: 0.9415\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9400 - val_loss: 0.9449\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9381 - val_loss: 0.9416\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9375 - val_loss: 0.9416\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9374 - val_loss: 0.9402\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9363 - val_loss: 0.9418\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9360 - val_loss: 0.9404\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9350 - val_loss: 0.9404\n",
      "Top-2 accuracy = 0.85\n",
      "24\n",
      "normalizeV|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0717 - val_loss: 1.0377\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0041 - val_loss: 0.9811\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9605 - val_loss: 0.9554\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9451 - val_loss: 0.9510\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9399 - val_loss: 0.9472\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9382 - val_loss: 0.9451\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9370 - val_loss: 0.9433\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9352 - val_loss: 0.9428\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9350 - val_loss: 0.9434\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9360 - val_loss: 0.9431\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9337 - val_loss: 0.9408\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9325 - val_loss: 0.9395\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9316 - val_loss: 0.9409\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9320 - val_loss: 0.9429\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9316 - val_loss: 0.9393\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9317 - val_loss: 0.9461\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9322 - val_loss: 0.9380\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9307 - val_loss: 0.9390\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9301 - val_loss: 0.9388\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9304 - val_loss: 0.9384\n",
      "Top-2 accuracy = 0.85\n",
      "25\n",
      "standardizeZ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0966 - val_loss: 1.0962\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0953 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0963\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0963\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0963\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0952 - val_loss: 1.0963\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0963\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0963\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0953 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "26\n",
      "standardizeM|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0640 - val_loss: 1.0313\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9908 - val_loss: 0.9670\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9483 - val_loss: 0.9476\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9373 - val_loss: 0.9447\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9360 - val_loss: 0.9454\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9332 - val_loss: 0.9417\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9317 - val_loss: 0.9392\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9315 - val_loss: 0.9418\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9317 - val_loss: 0.9405\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9301 - val_loss: 0.9381\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9293 - val_loss: 0.9405\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9303 - val_loss: 0.9401\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9298 - val_loss: 0.9393\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9288 - val_loss: 0.9384\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9279 - val_loss: 0.9379\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9286 - val_loss: 0.9455\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9308 - val_loss: 0.9374\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9276 - val_loss: 0.9411\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9294 - val_loss: 0.9367\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9283 - val_loss: 0.9361\n",
      "Top-2 accuracy = 0.86\n",
      "27\n",
      "standardizeI|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0424 - val_loss: 0.9907\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9708 - val_loss: 0.9611\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9535 - val_loss: 0.9535\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9468 - val_loss: 0.9523\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9441 - val_loss: 0.9490\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9412 - val_loss: 0.9471\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9400 - val_loss: 0.9460\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9370 - val_loss: 0.9481\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9352 - val_loss: 0.9449\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9358 - val_loss: 0.9435\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9329 - val_loss: 0.9455\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9319 - val_loss: 0.9433\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9324 - val_loss: 0.9431\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9313 - val_loss: 0.9423\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9293 - val_loss: 0.9408\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9281 - val_loss: 0.9405\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9273 - val_loss: 0.9407\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9273 - val_loss: 0.9413\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9278 - val_loss: 0.9440\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9270 - val_loss: 0.9407\n",
      "Top-2 accuracy = 0.85\n",
      "28\n",
      "robustf|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0953 - val_loss: 1.0962\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "29\n",
      "standardizei|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0728 - val_loss: 1.0357\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0025 - val_loss: 0.9860\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9757 - val_loss: 0.9707\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9641 - val_loss: 0.9637\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9586 - val_loss: 0.9620\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9547 - val_loss: 0.9591\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9527 - val_loss: 0.9569\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9505 - val_loss: 0.9559\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9496 - val_loss: 0.9556\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9489 - val_loss: 0.9639\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9508 - val_loss: 0.9539\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9468 - val_loss: 0.9562\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9472 - val_loss: 0.9537\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9459 - val_loss: 0.9525\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9464 - val_loss: 0.9516\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9444 - val_loss: 0.9510\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9438 - val_loss: 0.9511\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9433 - val_loss: 0.9504\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9439 - val_loss: 0.9521\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9436 - val_loss: 0.9493\n",
      "Top-2 accuracy = 0.85\n",
      "0\n",
      "robusto|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0938 - val_loss: 1.0838\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0605 - val_loss: 1.0374\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0188 - val_loss: 1.0093\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9969 - val_loss: 0.9931\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9858 - val_loss: 0.9837\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9788 - val_loss: 0.9820\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9740 - val_loss: 0.9751\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9698 - val_loss: 0.9727\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9667 - val_loss: 0.9703\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9648 - val_loss: 0.9701\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9638 - val_loss: 0.9692\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9622 - val_loss: 0.9693\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9620 - val_loss: 0.9671\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9604 - val_loss: 0.9656\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9600 - val_loss: 0.9647\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9592 - val_loss: 0.9640\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9589 - val_loss: 0.9659\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9584 - val_loss: 0.9634\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9575 - val_loss: 0.9625\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9579 - val_loss: 0.9631\n",
      "Top-2 accuracy = 0.84\n",
      "1\n",
      "standardizen|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0540 - val_loss: 1.0103\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9876 - val_loss: 0.9749\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9691 - val_loss: 0.9696\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9585 - val_loss: 0.9652\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9551 - val_loss: 0.9555\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9503 - val_loss: 0.9552\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9458 - val_loss: 0.9551\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9453 - val_loss: 0.9467\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9428 - val_loss: 0.9469\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9392 - val_loss: 0.9440\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9371 - val_loss: 0.9437\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9363 - val_loss: 0.9440\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9391 - val_loss: 0.9431\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9347 - val_loss: 0.9416\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9323 - val_loss: 0.9398\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9319 - val_loss: 0.9402\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9300 - val_loss: 0.9433\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9324 - val_loss: 0.9440\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9296 - val_loss: 0.9383\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9290 - val_loss: 0.9350\n",
      "Top-2 accuracy = 0.85\n",
      "2\n",
      "maxabsh|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0599 - val_loss: 1.0030\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9721 - val_loss: 0.9596\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9489 - val_loss: 0.9613\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9450 - val_loss: 0.9546\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9388 - val_loss: 0.9465\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9387 - val_loss: 0.9438\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9353 - val_loss: 0.9523\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9368 - val_loss: 0.9413\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9316 - val_loss: 0.9446\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9322 - val_loss: 0.9398\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9297 - val_loss: 0.9408\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9288 - val_loss: 0.9461\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9282 - val_loss: 0.9513\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9290 - val_loss: 0.9442\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9283 - val_loss: 0.9489\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9268 - val_loss: 0.9403\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9266 - val_loss: 0.9425\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9251 - val_loss: 0.9376\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9249 - val_loss: 0.9528\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9254 - val_loss: 0.9445\n",
      "Top-2 accuracy = 0.85\n",
      "3\n",
      "standardizeg|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0582 - val_loss: 1.0143\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9944 - val_loss: 0.9837\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9752 - val_loss: 0.9718\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9652 - val_loss: 0.9651\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9595 - val_loss: 0.9604\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9539 - val_loss: 0.9579\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9498 - val_loss: 0.9548\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9476 - val_loss: 0.9543\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9444 - val_loss: 0.9504\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9422 - val_loss: 0.9495\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9402 - val_loss: 0.9488\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9394 - val_loss: 0.9462\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9375 - val_loss: 0.9455\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9367 - val_loss: 0.9450\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9352 - val_loss: 0.9441\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9341 - val_loss: 0.9439\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9330 - val_loss: 0.9421\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9317 - val_loss: 0.9428\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9318 - val_loss: 0.9409\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9304 - val_loss: 0.9418\n",
      "Top-2 accuracy = 0.85\n",
      "4\n",
      "normalizeX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0297 - val_loss: 0.9513\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9374 - val_loss: 0.9447\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9376 - val_loss: 0.9413\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9326 - val_loss: 0.9419\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9312 - val_loss: 0.9374\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9301 - val_loss: 0.9415\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9291 - val_loss: 0.9393\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9289 - val_loss: 0.9411\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9301 - val_loss: 0.9361\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9277 - val_loss: 0.9373\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9278 - val_loss: 0.9548\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9296 - val_loss: 0.9405\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9277 - val_loss: 0.9432\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9278 - val_loss: 0.9386\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9265 - val_loss: 0.9384\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9262 - val_loss: 0.9356\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9251 - val_loss: 0.9374\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9253 - val_loss: 0.9337\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9242 - val_loss: 0.9365\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9251 - val_loss: 0.9325\n",
      "Top-2 accuracy = 0.85\n",
      "5\n",
      "standardizex|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0785 - val_loss: 1.0576\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0427 - val_loss: 1.0296\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0185 - val_loss: 1.0102\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0023 - val_loss: 1.0008\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9915 - val_loss: 0.9902\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9848 - val_loss: 0.9859\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9793 - val_loss: 0.9816\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9746 - val_loss: 0.9777\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9708 - val_loss: 0.9726\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9680 - val_loss: 0.9698\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9652 - val_loss: 0.9724\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9645 - val_loss: 0.9673\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9629 - val_loss: 0.9652\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9608 - val_loss: 0.9654\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9604 - val_loss: 0.9630\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9581 - val_loss: 0.9619\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9575 - val_loss: 0.9605\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9559 - val_loss: 0.9631\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9559 - val_loss: 0.9619\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9548 - val_loss: 0.9585\n",
      "Top-2 accuracy = 0.85\n",
      "6\n",
      "minmaxU|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0894 - val_loss: 1.0609\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0050 - val_loss: 0.9833\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9766 - val_loss: 0.9741\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9690 - val_loss: 0.9687\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9650 - val_loss: 0.9798\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9632 - val_loss: 0.9620\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9579 - val_loss: 0.9573\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9565 - val_loss: 0.9578\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9560 - val_loss: 0.9558\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9531 - val_loss: 0.9545\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9523 - val_loss: 0.9554\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9535 - val_loss: 0.9547\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9508 - val_loss: 0.9514\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9491 - val_loss: 0.9583\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9513 - val_loss: 0.9527\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9486 - val_loss: 0.9525\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9494 - val_loss: 0.9557\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9472 - val_loss: 0.9496\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9459 - val_loss: 0.9509\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9454 - val_loss: 0.9483\n",
      "Top-2 accuracy = 0.85\n",
      "7\n",
      "maxabsj|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0888 - val_loss: 1.0699\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0340 - val_loss: 1.0048\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9881 - val_loss: 0.9843\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9652 - val_loss: 0.9868\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9557 - val_loss: 0.9531\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9463 - val_loss: 0.9513\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9433 - val_loss: 0.9484\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9416 - val_loss: 0.9451\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9415 - val_loss: 0.9771\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9473 - val_loss: 0.9478\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9389 - val_loss: 0.9466\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9383 - val_loss: 0.9425\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9384 - val_loss: 0.9435\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9367 - val_loss: 0.9514\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9392 - val_loss: 0.9482\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9372 - val_loss: 0.9485\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9363 - val_loss: 0.9413\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9352 - val_loss: 0.9426\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9369 - val_loss: 0.9416\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9360 - val_loss: 0.9403\n",
      "Top-2 accuracy = 0.85\n",
      "8\n",
      "maxabsb|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0914 - val_loss: 1.0757\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0342 - val_loss: 0.9952\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9734 - val_loss: 0.9675\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9576 - val_loss: 0.9588\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9521 - val_loss: 0.9643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9525 - val_loss: 0.9568\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9495 - val_loss: 0.9536\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9488 - val_loss: 0.9532\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9482 - val_loss: 0.9531\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9490 - val_loss: 0.9530\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9467 - val_loss: 0.9541\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9469 - val_loss: 0.9522\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9459 - val_loss: 0.9531\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9457 - val_loss: 0.9538\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9456 - val_loss: 0.9514\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9447 - val_loss: 0.9524\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9448 - val_loss: 0.9510\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9448 - val_loss: 0.9504\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9447 - val_loss: 0.9506\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9437 - val_loss: 0.9554\n",
      "Top-2 accuracy = 0.85\n",
      "9\n",
      "normalizeC|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0562 - val_loss: 1.0530\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0072 - val_loss: 1.0047\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9827 - val_loss: 0.9736\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9653 - val_loss: 0.9634\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9573 - val_loss: 0.9583\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9534 - val_loss: 0.9698\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9533 - val_loss: 0.9644\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9545 - val_loss: 0.9625\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9545 - val_loss: 0.9574\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9495 - val_loss: 0.9551\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9501 - val_loss: 0.9545\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9495 - val_loss: 0.9518\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9478 - val_loss: 0.9561\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9484 - val_loss: 0.9575\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9473 - val_loss: 0.9564\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9483 - val_loss: 0.9564\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9485 - val_loss: 0.9645\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9470 - val_loss: 0.9554\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9473 - val_loss: 0.9606\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9504 - val_loss: 0.9633\n",
      "Top-2 accuracy = 0.84\n",
      "10\n",
      "maxabsK|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0431 - val_loss: 1.0019\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9697 - val_loss: 0.9568\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9493 - val_loss: 0.9518\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9458 - val_loss: 0.9594\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9457 - val_loss: 0.9555\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9449 - val_loss: 0.9575\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9449 - val_loss: 0.9478\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9423 - val_loss: 0.9466\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9426 - val_loss: 0.9612\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9415 - val_loss: 0.9515\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9415 - val_loss: 0.9474\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9415 - val_loss: 0.9449\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9408 - val_loss: 0.9446\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9400 - val_loss: 0.9463\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9399 - val_loss: 0.9456\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9381 - val_loss: 0.9435\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9386 - val_loss: 0.9601\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9380 - val_loss: 0.9456\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9379 - val_loss: 0.9488\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9366 - val_loss: 0.9482\n",
      "Top-2 accuracy = 0.85\n",
      "11\n",
      "standardized|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 1.0624 - val_loss: 1.0168\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9833 - val_loss: 0.9743\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9515 - val_loss: 0.9496\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9415 - val_loss: 0.9528\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9438 - val_loss: 0.9462\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9393 - val_loss: 0.9523\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9379 - val_loss: 0.9441\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9365 - val_loss: 0.9454\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9371 - val_loss: 0.9506\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9359 - val_loss: 0.9443\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9368 - val_loss: 0.9464\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9347 - val_loss: 0.9461\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9349 - val_loss: 0.9458\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9334 - val_loss: 0.9449\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9357 - val_loss: 0.9406\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9328 - val_loss: 0.9411\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9335 - val_loss: 0.9418\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9341 - val_loss: 0.9425\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9318 - val_loss: 0.9420\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9324 - val_loss: 0.9419\n",
      "Top-2 accuracy = 0.85\n",
      "12\n",
      "minmaxJ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0497 - val_loss: 0.9907\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9660 - val_loss: 0.9798\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9547 - val_loss: 0.9531\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9501 - val_loss: 0.9706\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9477 - val_loss: 0.9484\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9439 - val_loss: 0.9499\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9434 - val_loss: 0.9497\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9420 - val_loss: 0.9447\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9405 - val_loss: 0.9437\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9397 - val_loss: 0.9479\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9386 - val_loss: 0.9440\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9393 - val_loss: 0.9410\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9389 - val_loss: 0.9418\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9393 - val_loss: 0.9429\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9353 - val_loss: 0.9436\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9355 - val_loss: 0.9428\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9354 - val_loss: 0.9423\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9357 - val_loss: 0.9495\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9366 - val_loss: 0.9574\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9360 - val_loss: 0.9377\n",
      "Top-2 accuracy = 0.85\n",
      "13\n",
      "minmaxT|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 1.0449 - val_loss: 1.0006\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9844 - val_loss: 0.9970\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9707 - val_loss: 0.9796\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9636 - val_loss: 0.9645\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9578 - val_loss: 0.9701\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9572 - val_loss: 0.9601\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9535 - val_loss: 0.9647\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9537 - val_loss: 0.9868\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9543 - val_loss: 0.9775\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9546 - val_loss: 0.9625\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9519 - val_loss: 0.9634\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9534 - val_loss: 0.9551\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9487 - val_loss: 0.9719\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9515 - val_loss: 0.9633\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9482 - val_loss: 0.9582\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9471 - val_loss: 0.9514\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9466 - val_loss: 0.9536\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9471 - val_loss: 0.9498\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9455 - val_loss: 0.9584\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9455 - val_loss: 0.9503\n",
      "Top-2 accuracy = 0.85\n",
      "14\n",
      "normalizeN|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0910 - val_loss: 1.0732\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0361 - val_loss: 1.0055\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9910 - val_loss: 0.9922\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9787 - val_loss: 0.9755\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9707 - val_loss: 0.9785\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9705 - val_loss: 0.9713\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9672 - val_loss: 0.9686\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9660 - val_loss: 0.9694\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9639 - val_loss: 0.9670\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9629 - val_loss: 0.9644\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9629 - val_loss: 0.9641\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9619 - val_loss: 0.9624\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9604 - val_loss: 0.9618\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9596 - val_loss: 0.9662\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9591 - val_loss: 0.9601\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9574 - val_loss: 0.9621\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9566 - val_loss: 0.9580\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9558 - val_loss: 0.9579\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9546 - val_loss: 0.9571\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9552 - val_loss: 0.9564\n",
      "Top-2 accuracy = 0.85\n",
      "15\n",
      "standardizee|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0647 - val_loss: 1.0213\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9914 - val_loss: 0.9810\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9645 - val_loss: 0.9665\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9543 - val_loss: 0.9608\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9484 - val_loss: 0.9582\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9481 - val_loss: 0.9541\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9437 - val_loss: 0.9522\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9422 - val_loss: 0.9500\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9422 - val_loss: 0.9489\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9415 - val_loss: 0.9486\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9388 - val_loss: 0.9492\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9378 - val_loss: 0.9479\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9374 - val_loss: 0.9504\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9373 - val_loss: 0.9490\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9369 - val_loss: 0.9454\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9368 - val_loss: 0.9445\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9349 - val_loss: 0.9463\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9350 - val_loss: 0.9455\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9341 - val_loss: 0.9441\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9333 - val_loss: 0.9424\n",
      "Top-2 accuracy = 0.85\n",
      "16\n",
      "maxabsP|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0500 - val_loss: 0.9892\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9661 - val_loss: 0.9578\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9512 - val_loss: 0.9543\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9455 - val_loss: 0.9502\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9428 - val_loss: 0.9486\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9406 - val_loss: 0.9461\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9393 - val_loss: 0.9447\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9375 - val_loss: 0.9475\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9394 - val_loss: 0.9437\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9354 - val_loss: 0.9436\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9345 - val_loss: 0.9422\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9336 - val_loss: 0.9401\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9325 - val_loss: 0.9435\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9334 - val_loss: 0.9443\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9322 - val_loss: 0.9453\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9310 - val_loss: 0.9375\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9317 - val_loss: 0.9401\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9295 - val_loss: 0.9373\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9304 - val_loss: 0.9464\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9298 - val_loss: 0.9373\n",
      "Top-2 accuracy = 0.85\n",
      "17\n",
      "normalizeJ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0824 - val_loss: 1.0396\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0044 - val_loss: 0.9758\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9624 - val_loss: 0.9650\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9521 - val_loss: 0.9657\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9468 - val_loss: 0.9477\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9445 - val_loss: 0.9447\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9409 - val_loss: 0.9455\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9434 - val_loss: 0.9428\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9384 - val_loss: 0.9387\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9367 - val_loss: 0.9379\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9355 - val_loss: 0.9387\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9354 - val_loss: 0.9428\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9358 - val_loss: 0.9380\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9346 - val_loss: 0.9389\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9339 - val_loss: 0.9391\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9346 - val_loss: 0.9391\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9323 - val_loss: 0.9392\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9333 - val_loss: 0.9504\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9378 - val_loss: 0.9355\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9334 - val_loss: 0.9378\n",
      "Top-2 accuracy = 0.85\n",
      "18\n",
      "normalizes|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0940 - val_loss: 1.0873\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0757 - val_loss: 1.0645\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0490 - val_loss: 1.0345\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0205 - val_loss: 1.0117\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0021 - val_loss: 0.9999\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9918 - val_loss: 0.9915\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9851 - val_loss: 0.9862\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9805 - val_loss: 0.9820\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9765 - val_loss: 0.9783\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9737 - val_loss: 0.9760\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9713 - val_loss: 0.9738\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9695 - val_loss: 0.9721\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9680 - val_loss: 0.9704\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9663 - val_loss: 0.9694\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9651 - val_loss: 0.9681\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9638 - val_loss: 0.9673\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9639 - val_loss: 0.9660\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9614 - val_loss: 0.9649\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9605 - val_loss: 0.9643\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9598 - val_loss: 0.9632\n",
      "Top-2 accuracy = 0.84\n",
      "19\n",
      "maxabsq|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 1.0691 - val_loss: 1.0173\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9737 - val_loss: 0.9541\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9492 - val_loss: 0.9598\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9482 - val_loss: 0.9491\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9445 - val_loss: 0.9603\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9453 - val_loss: 0.9471\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9421 - val_loss: 0.9456\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9412 - val_loss: 0.9571\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9421 - val_loss: 0.9459\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9422 - val_loss: 0.9504\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9401 - val_loss: 0.9445\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9393 - val_loss: 0.9464\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9370 - val_loss: 0.9488\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9375 - val_loss: 0.9418\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9371 - val_loss: 0.9427\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9403 - val_loss: 0.9448\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9352 - val_loss: 0.9417\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9370 - val_loss: 0.9479\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9346 - val_loss: 0.9424\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9340 - val_loss: 0.9411\n",
      "Top-2 accuracy = 0.85\n",
      "20\n",
      "standardizeu|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 1.0581 - val_loss: 1.0249\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0064 - val_loss: 0.9916\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9715 - val_loss: 0.9608\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9493 - val_loss: 0.9519\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9405 - val_loss: 0.9463\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9379 - val_loss: 0.9426\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9357 - val_loss: 0.9406\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9361 - val_loss: 0.9406\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9362 - val_loss: 0.9418\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9326 - val_loss: 0.9395\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9339 - val_loss: 0.9408\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9325 - val_loss: 0.9396\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9322 - val_loss: 0.9393\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9311 - val_loss: 0.9592\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9357 - val_loss: 0.9419\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9332 - val_loss: 0.9463\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9312 - val_loss: 0.9398\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9306 - val_loss: 0.9378\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9313 - val_loss: 0.9370\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9311 - val_loss: 0.9436\n",
      "Top-2 accuracy = 0.85\n",
      "21\n",
      "standardizeP|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0647 - val_loss: 1.0435\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0256 - val_loss: 1.0159\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0005 - val_loss: 0.9956\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9813 - val_loss: 0.9812\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9687 - val_loss: 0.9706\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9622 - val_loss: 0.9729\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9583 - val_loss: 0.9655\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9561 - val_loss: 0.9625\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9552 - val_loss: 0.9634\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9544 - val_loss: 0.9607\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9534 - val_loss: 0.9600\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9526 - val_loss: 0.9600\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9530 - val_loss: 0.9628\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9518 - val_loss: 0.9588\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9509 - val_loss: 0.9578\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9503 - val_loss: 0.9579\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9503 - val_loss: 0.9567\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9503 - val_loss: 0.9564\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9491 - val_loss: 0.9565\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9490 - val_loss: 0.9575\n",
      "Top-2 accuracy = 0.85\n",
      "22\n",
      "minmaxx|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0676 - val_loss: 1.0364\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0194 - val_loss: 1.0178\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0015 - val_loss: 0.9986\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9927 - val_loss: 0.9865\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9866 - val_loss: 0.9836\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9816 - val_loss: 0.9848\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9808 - val_loss: 0.9857\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9787 - val_loss: 0.9803\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9783 - val_loss: 0.9828\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9774 - val_loss: 0.9963\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9852 - val_loss: 0.9796\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9766 - val_loss: 0.9791\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9783 - val_loss: 0.9904\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9776 - val_loss: 0.9935\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9766 - val_loss: 0.9829\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9756 - val_loss: 0.9769\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9639 - val_loss: 0.9656\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9568 - val_loss: 0.9627\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9537 - val_loss: 0.9569\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9520 - val_loss: 0.9555\n",
      "Top-2 accuracy = 0.84\n",
      "23\n",
      "maxabsE|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.1036 - val_loss: 1.0968\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0958 - val_loss: 1.0963\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0955 - val_loss: 1.0962\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0954 - val_loss: 1.0962\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0953 - val_loss: 1.0960\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0964\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0947 - val_loss: 1.0952\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0933 - val_loss: 1.0923\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0833 - val_loss: 1.0654\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0384 - val_loss: 1.0106\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9965 - val_loss: 0.9811\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9735 - val_loss: 0.9701\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9631 - val_loss: 0.9638\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9603 - val_loss: 0.9634\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9587 - val_loss: 0.9641\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9573 - val_loss: 0.9611\n",
      "Top-2 accuracy = 0.84\n",
      "24\n",
      "minmaxH|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0497 - val_loss: 1.0088\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9867 - val_loss: 0.9830\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9716 - val_loss: 0.9758\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9654 - val_loss: 0.9762\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9644 - val_loss: 0.9638\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9599 - val_loss: 0.9630\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9582 - val_loss: 0.9623\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9578 - val_loss: 0.9611\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9558 - val_loss: 0.9652\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9547 - val_loss: 0.9606\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9542 - val_loss: 0.9665\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9533 - val_loss: 0.9558\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9518 - val_loss: 0.9554\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9512 - val_loss: 0.9563\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9501 - val_loss: 0.9532\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9498 - val_loss: 0.9564\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9491 - val_loss: 0.9594\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9487 - val_loss: 0.9566\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9475 - val_loss: 0.9534\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9457 - val_loss: 0.9506\n",
      "Top-2 accuracy = 0.84\n",
      "25\n",
      "robustG|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0498 - val_loss: 0.9978\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9731 - val_loss: 0.9661\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9599 - val_loss: 0.9602\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9531 - val_loss: 0.9584\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9484 - val_loss: 0.9484\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9437 - val_loss: 0.9470\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9415 - val_loss: 0.9435\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9373 - val_loss: 0.9478\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9377 - val_loss: 0.9389\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9335 - val_loss: 0.9394\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9347 - val_loss: 0.9381\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9314 - val_loss: 0.9394\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9288 - val_loss: 0.9359\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9265 - val_loss: 0.9335\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9261 - val_loss: 0.9306\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9261 - val_loss: 0.9331\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9242 - val_loss: 0.9336\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9235 - val_loss: 0.9312\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9215 - val_loss: 0.9384\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9215 - val_loss: 0.9275\n",
      "Top-2 accuracy = 0.85\n",
      "26\n",
      "minmaxd|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_983 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0890 - val_loss: 1.0711\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0558 - val_loss: 1.0450\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0289 - val_loss: 1.0213\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0089 - val_loss: 1.0061\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9953 - val_loss: 0.9950\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9868 - val_loss: 0.9872\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9804 - val_loss: 0.9822\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9762 - val_loss: 0.9779\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9719 - val_loss: 0.9769\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9706 - val_loss: 0.9729\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9670 - val_loss: 0.9711\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9641 - val_loss: 0.9674\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9624 - val_loss: 0.9647\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9598 - val_loss: 0.9642\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9589 - val_loss: 0.9622\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9567 - val_loss: 0.9599\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9550 - val_loss: 0.9587\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9540 - val_loss: 0.9564\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9518 - val_loss: 0.9564\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9512 - val_loss: 0.9540\n",
      "Top-2 accuracy = 0.84\n",
      "27\n",
      "normalizeK|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0397 - val_loss: 0.9893\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9620 - val_loss: 0.9639\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9502 - val_loss: 0.9519\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9483 - val_loss: 0.9520\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9507 - val_loss: 0.9512\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9460 - val_loss: 0.9508\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9489 - val_loss: 0.9501\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9452 - val_loss: 0.9470\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9446 - val_loss: 0.9464\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9434 - val_loss: 0.9461\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9421 - val_loss: 0.9476\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9402 - val_loss: 0.9477\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9411 - val_loss: 0.9440\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9409 - val_loss: 0.9601\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9434 - val_loss: 0.9503\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9406 - val_loss: 0.9512\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9393 - val_loss: 0.9450\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9376 - val_loss: 0.9441\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9368 - val_loss: 0.9681\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9425 - val_loss: 0.9438\n",
      "Top-2 accuracy = 0.85\n",
      "28\n",
      "maxabsr|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0968 - val_loss: 1.0962\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0954 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "29\n",
      "maxabsp|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 1.0310 - val_loss: 0.9823\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9611 - val_loss: 0.9679\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9537 - val_loss: 0.9556\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9485 - val_loss: 0.9624\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9488 - val_loss: 0.9506\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9477 - val_loss: 0.9507\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9478 - val_loss: 0.9505\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9467 - val_loss: 0.9537\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9463 - val_loss: 0.9498\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9458 - val_loss: 0.9512\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9463 - val_loss: 0.9617\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9453 - val_loss: 0.9608\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9452 - val_loss: 0.9598\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9447 - val_loss: 0.9514\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9467 - val_loss: 0.9544\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9463 - val_loss: 0.9599\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9455 - val_loss: 0.9820\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9513 - val_loss: 0.9479\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9424 - val_loss: 0.9521\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.9438 - val_loss: 0.9479\n",
      "Top-2 accuracy = 0.85\n",
      "0\n",
      "maxabsT|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0976 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0954 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "1\n",
      "minmaxi|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0965 - val_loss: 1.0961\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0963\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "2\n",
      "normalizej|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0841 - val_loss: 1.0623\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0469 - val_loss: 1.0353\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0204 - val_loss: 1.0116\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9900 - val_loss: 0.9727\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9678 - val_loss: 0.9670\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9617 - val_loss: 0.9650\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9597 - val_loss: 0.9611\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9571 - val_loss: 0.9622\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9565 - val_loss: 0.9597\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9546 - val_loss: 0.9595\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9544 - val_loss: 0.9591\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9529 - val_loss: 0.9666\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9545 - val_loss: 0.9587\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9550 - val_loss: 0.9634\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9519 - val_loss: 0.9579\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9517 - val_loss: 0.9587\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9513 - val_loss: 0.9582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9509 - val_loss: 0.9619\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9516 - val_loss: 0.9577\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9518 - val_loss: 0.9575\n",
      "Top-2 accuracy = 0.85\n",
      "3\n",
      "normalizey|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0724 - val_loss: 1.0367\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0156 - val_loss: 1.0183\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9993 - val_loss: 0.9951\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9832 - val_loss: 0.9782\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9712 - val_loss: 0.9669\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9600 - val_loss: 0.9584\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9585 - val_loss: 0.9563\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9531 - val_loss: 0.9633\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9520 - val_loss: 0.9578\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9500 - val_loss: 0.9592\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9506 - val_loss: 0.9556\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9509 - val_loss: 0.9651\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9526 - val_loss: 0.9517\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9476 - val_loss: 0.9588\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9492 - val_loss: 0.9577\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9477 - val_loss: 0.9494\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9462 - val_loss: 0.9553\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9476 - val_loss: 0.9505\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9489 - val_loss: 0.9565\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9451 - val_loss: 0.9634\n",
      "Top-2 accuracy = 0.84\n",
      "4\n",
      "standardizef|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0969 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0954 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "5\n",
      "robustA|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0316 - val_loss: 0.9554\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9450 - val_loss: 0.9483\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9385 - val_loss: 0.9497\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9362 - val_loss: 0.9426\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9336 - val_loss: 0.9402\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9311 - val_loss: 0.9487\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9306 - val_loss: 0.9404\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9277 - val_loss: 0.9376\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9266 - val_loss: 0.9412\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9269 - val_loss: 0.9389\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9262 - val_loss: 0.9557\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9287 - val_loss: 0.9359\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9257 - val_loss: 0.9401\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9245 - val_loss: 0.9388\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9244 - val_loss: 0.9431\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9240 - val_loss: 0.9351\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9239 - val_loss: 0.9394\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9218 - val_loss: 0.9429\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9238 - val_loss: 0.9375\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9220 - val_loss: 0.9399\n",
      "Top-2 accuracy = 0.85\n",
      "6\n",
      "normalizeG|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0809 - val_loss: 1.0329\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9809 - val_loss: 0.9593\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9522 - val_loss: 0.9492\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9442 - val_loss: 0.9460\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9402 - val_loss: 0.9420\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9357 - val_loss: 0.9451\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9342 - val_loss: 0.9386\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9314 - val_loss: 0.9426\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9323 - val_loss: 0.9392\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9288 - val_loss: 0.9335\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9281 - val_loss: 0.9332\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9266 - val_loss: 0.9353\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9262 - val_loss: 0.9322\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9247 - val_loss: 0.9309\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9245 - val_loss: 0.9330\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9241 - val_loss: 0.9353\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9246 - val_loss: 0.9313\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9230 - val_loss: 0.9294\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9227 - val_loss: 0.9305\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9221 - val_loss: 0.9328\n",
      "Top-2 accuracy = 0.85\n",
      "7\n",
      "maxabsE|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0792 - val_loss: 1.0549\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0297 - val_loss: 1.0173\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0060 - val_loss: 1.0036\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9939 - val_loss: 0.9946\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9852 - val_loss: 0.9882\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9795 - val_loss: 0.9825\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9745 - val_loss: 0.9779\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9709 - val_loss: 0.9761\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9683 - val_loss: 0.9742\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9659 - val_loss: 0.9715\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9641 - val_loss: 0.9700\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9626 - val_loss: 0.9691\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9611 - val_loss: 0.9688\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9609 - val_loss: 0.9668\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9586 - val_loss: 0.9672\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9582 - val_loss: 0.9647\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9573 - val_loss: 0.9646\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9567 - val_loss: 0.9648\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9564 - val_loss: 0.9627\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9557 - val_loss: 0.9625\n",
      "Top-2 accuracy = 0.85\n",
      "8\n",
      "standardizeX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0968 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0953 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "9\n",
      "standardizek|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 1.0819 - val_loss: 1.0634\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0348 - val_loss: 1.0131\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9933 - val_loss: 0.9871\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9788 - val_loss: 0.9836\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9779 - val_loss: 0.9822\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9734 - val_loss: 0.9851\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9730 - val_loss: 0.9875\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9722 - val_loss: 0.9782\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9716 - val_loss: 0.9779\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9687 - val_loss: 0.9755\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9675 - val_loss: 0.9739\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9665 - val_loss: 0.9798\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9686 - val_loss: 0.9703\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9642 - val_loss: 0.9704\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9630 - val_loss: 0.9705\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9615 - val_loss: 0.9673\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9609 - val_loss: 0.9665\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9595 - val_loss: 0.9671\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9576 - val_loss: 0.9726\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9570 - val_loss: 0.9652\n",
      "Top-2 accuracy = 0.84\n",
      "10\n",
      "normalizeS|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0992 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0954 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "11\n",
      "normalizez|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0971 - val_loss: 1.0954\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0825 - val_loss: 1.0538\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0256 - val_loss: 1.0096\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9971 - val_loss: 0.9917\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9809 - val_loss: 0.9830\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9709 - val_loss: 0.9714\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9643 - val_loss: 0.9757\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9614 - val_loss: 0.9624\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9570 - val_loss: 0.9614\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9546 - val_loss: 0.9602\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9532 - val_loss: 0.9570\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9515 - val_loss: 0.9636\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9522 - val_loss: 0.9570\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9503 - val_loss: 0.9552\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9501 - val_loss: 0.9566\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9493 - val_loss: 0.9545\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9488 - val_loss: 0.9535\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9494 - val_loss: 0.9533\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9476 - val_loss: 0.9566\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9474 - val_loss: 0.9523\n",
      "Top-2 accuracy = 0.85\n",
      "12\n",
      "minmaxq|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0960 - val_loss: 1.0961\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0963\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0963\n",
      "Top-2 accuracy = 0.7\n",
      "13\n",
      "robustI|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0779 - val_loss: 1.0544\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0233 - val_loss: 0.9967\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9723 - val_loss: 0.9639\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9531 - val_loss: 0.9545\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9450 - val_loss: 0.9488\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9399 - val_loss: 0.9463\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9429 - val_loss: 0.9446\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9369 - val_loss: 0.9439\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9361 - val_loss: 0.9449\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9351 - val_loss: 0.9459\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9354 - val_loss: 0.9447\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9343 - val_loss: 0.9474\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9323 - val_loss: 0.9405\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9315 - val_loss: 0.9403\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9314 - val_loss: 0.9423\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9308 - val_loss: 0.9400\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9300 - val_loss: 0.9442\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9300 - val_loss: 0.9581\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9331 - val_loss: 0.9473\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9317 - val_loss: 0.9380\n",
      "Top-2 accuracy = 0.85\n",
      "14\n",
      "standardizeK|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0689 - val_loss: 1.0216\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9920 - val_loss: 0.9733\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9670 - val_loss: 0.9672\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9560 - val_loss: 0.9590\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9508 - val_loss: 0.9668\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9469 - val_loss: 0.9544\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9453 - val_loss: 0.9479\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9436 - val_loss: 0.9492\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9422 - val_loss: 0.9529\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9407 - val_loss: 0.9462\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9395 - val_loss: 0.9434\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9426 - val_loss: 0.9425\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9387 - val_loss: 0.9430\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9413 - val_loss: 0.9442\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9386 - val_loss: 0.9450\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9380 - val_loss: 0.9498\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9421 - val_loss: 0.9398\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9360 - val_loss: 0.9401\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9368 - val_loss: 0.9432\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9353 - val_loss: 0.9407\n",
      "Top-2 accuracy = 0.85\n",
      "15\n",
      "minmaxk|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0674 - val_loss: 1.0382\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0147 - val_loss: 1.0044\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9914 - val_loss: 0.9930\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9805 - val_loss: 0.9808\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9726 - val_loss: 0.9767\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9681 - val_loss: 0.9720\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9647 - val_loss: 0.9711\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9628 - val_loss: 0.9676\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9601 - val_loss: 0.9657\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9591 - val_loss: 0.9670\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9605 - val_loss: 0.9641\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9566 - val_loss: 0.9623\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9547 - val_loss: 0.9623\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9561 - val_loss: 0.9596\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9540 - val_loss: 0.9666\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9547 - val_loss: 0.9626\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9538 - val_loss: 0.9597\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9523 - val_loss: 0.9578\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9518 - val_loss: 0.9573\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9515 - val_loss: 0.9587\n",
      "Top-2 accuracy = 0.85\n",
      "16\n",
      "minmaxp|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0668 - val_loss: 1.0304\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0083 - val_loss: 1.0010\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9863 - val_loss: 0.9813\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9716 - val_loss: 0.9661\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9695 - val_loss: 0.9719\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9630 - val_loss: 0.9584\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9561 - val_loss: 0.9601\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9560 - val_loss: 0.9536\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9532 - val_loss: 0.9512\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9527 - val_loss: 0.9509\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9491 - val_loss: 0.9701\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9500 - val_loss: 0.9490\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9461 - val_loss: 0.9486\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9466 - val_loss: 0.9543\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9470 - val_loss: 0.9460\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9439 - val_loss: 0.9541\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9435 - val_loss: 0.9880\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9486 - val_loss: 0.9447\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9432 - val_loss: 0.9528\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9422 - val_loss: 0.9435\n",
      "Top-2 accuracy = 0.85\n",
      "17\n",
      "normalizeJ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0959 - val_loss: 1.0956\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0935 - val_loss: 1.0912\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0766 - val_loss: 1.0606\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0333 - val_loss: 1.0227\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0095 - val_loss: 1.0071\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9998 - val_loss: 1.0007\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9949 - val_loss: 0.9993\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9913 - val_loss: 0.9934\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9881 - val_loss: 0.9917\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9865 - val_loss: 0.9896\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9843 - val_loss: 0.9869\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9831 - val_loss: 0.9853\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9803 - val_loss: 0.9852\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9803 - val_loss: 0.9842\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9782 - val_loss: 0.9819\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9765 - val_loss: 0.9803\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9755 - val_loss: 0.9874\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9758 - val_loss: 0.9798\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9727 - val_loss: 0.9764\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9750 - val_loss: 0.9751\n",
      "Top-2 accuracy = 0.83\n",
      "18\n",
      "normalizeg|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 1.0951 - val_loss: 1.0898\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0712 - val_loss: 1.0428\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0236 - val_loss: 1.0216\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0019 - val_loss: 0.9891\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9894 - val_loss: 0.9830\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9765 - val_loss: 0.9697\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9720 - val_loss: 0.9658\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9675 - val_loss: 0.9964\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9703 - val_loss: 0.9915\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9614 - val_loss: 1.0074\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9639 - val_loss: 0.9582\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9605 - val_loss: 0.9694\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9581 - val_loss: 1.0161\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9632 - val_loss: 1.0278\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9749 - val_loss: 0.9627\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9599 - val_loss: 0.9624\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9576 - val_loss: 0.9540\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9574 - val_loss: 0.9654\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9565 - val_loss: 0.9607\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9544 - val_loss: 0.9574\n",
      "Top-2 accuracy = 0.84\n",
      "19\n",
      "normalizeO|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0937 - val_loss: 1.0899\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0512 - val_loss: 1.0208\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0088 - val_loss: 0.9918\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9855 - val_loss: 0.9868\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9778 - val_loss: 0.9858\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9715 - val_loss: 0.9921\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9660 - val_loss: 0.9706\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9669 - val_loss: 0.9723\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9658 - val_loss: 1.0688\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9731 - val_loss: 0.9648\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9623 - val_loss: 0.9872\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9626 - val_loss: 0.9711\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9597 - val_loss: 0.9614\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9608 - val_loss: 1.0078\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9608 - val_loss: 0.9644\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9570 - val_loss: 0.9596\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9569 - val_loss: 0.9619\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9566 - val_loss: 0.9669\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9563 - val_loss: 0.9571\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9558 - val_loss: 0.9585\n",
      "Top-2 accuracy = 0.84\n",
      "20\n",
      "robustM|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 1.0964 - val_loss: 1.0961\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "21\n",
      "maxabsb|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0722 - val_loss: 1.0499\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0306 - val_loss: 1.0173\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0045 - val_loss: 0.9998\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9906 - val_loss: 0.9902\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.9811 - val_loss: 0.9843\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9774 - val_loss: 0.9793\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9684 - val_loss: 0.9769\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9656 - val_loss: 0.9824\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9606 - val_loss: 0.9678\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9583 - val_loss: 0.9640\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9558 - val_loss: 0.9660\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9527 - val_loss: 0.9701\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9513 - val_loss: 0.9575\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9485 - val_loss: 0.9570\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9464 - val_loss: 0.9702\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9485 - val_loss: 0.9592\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9440 - val_loss: 0.9730\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9463 - val_loss: 0.9550\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9422 - val_loss: 0.9559\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9419 - val_loss: 0.9531\n",
      "Top-2 accuracy = 0.85\n",
      "22\n",
      "robustE|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0936 - val_loss: 1.0856\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0609 - val_loss: 1.0274\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9978 - val_loss: 0.9797\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9661 - val_loss: 0.9625\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9571 - val_loss: 0.9573\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9556 - val_loss: 0.9559\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9537 - val_loss: 0.9765\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9553 - val_loss: 0.9592\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9496 - val_loss: 0.9524\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9481 - val_loss: 0.9536\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9472 - val_loss: 0.9667\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9493 - val_loss: 0.9526\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9487 - val_loss: 0.9503\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9452 - val_loss: 0.9507\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9450 - val_loss: 0.9512\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9448 - val_loss: 0.9511\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9457 - val_loss: 0.9639\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9465 - val_loss: 0.9517\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9429 - val_loss: 0.9488\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9434 - val_loss: 0.9593\n",
      "Top-2 accuracy = 0.83\n",
      "23\n",
      "robustS|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0619 - val_loss: 1.0195\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0017 - val_loss: 0.9853\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9740 - val_loss: 0.9717\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9537 - val_loss: 0.9822\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9478 - val_loss: 0.9445\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9430 - val_loss: 0.9499\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9410 - val_loss: 0.9451\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9395 - val_loss: 0.9545\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9405 - val_loss: 0.9449\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9367 - val_loss: 0.9409\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9348 - val_loss: 0.9453\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9353 - val_loss: 0.9473\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9344 - val_loss: 0.9452\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9354 - val_loss: 0.9393\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9330 - val_loss: 0.9393\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9324 - val_loss: 0.9374\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9318 - val_loss: 0.9509\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9334 - val_loss: 0.9371\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9343 - val_loss: 0.9367\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9310 - val_loss: 0.9381\n",
      "Top-2 accuracy = 0.85\n",
      "24\n",
      "minmaxC|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1122 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0885 - val_loss: 1.0784\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0485 - val_loss: 1.0196\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0045 - val_loss: 1.0019\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9909 - val_loss: 0.9893\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9824 - val_loss: 0.9838\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9776 - val_loss: 0.9865\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9764 - val_loss: 0.9775\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9722 - val_loss: 0.9762\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9703 - val_loss: 0.9763\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9684 - val_loss: 0.9718\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9685 - val_loss: 0.9741\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9672 - val_loss: 0.9712\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9651 - val_loss: 0.9683\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9641 - val_loss: 0.9675\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9630 - val_loss: 0.9669\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9623 - val_loss: 0.9660\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9613 - val_loss: 0.9645\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9607 - val_loss: 0.9645\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9594 - val_loss: 0.9653\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9591 - val_loss: 0.9621\n",
      "Top-2 accuracy = 0.84\n",
      "25\n",
      "normalizeA|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0956 - val_loss: 1.0961\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0781 - val_loss: 1.0426\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0002 - val_loss: 0.9782\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9736 - val_loss: 0.9672\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9653 - val_loss: 0.9663\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9596 - val_loss: 0.9609\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9568 - val_loss: 0.9771\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9537 - val_loss: 0.9540\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9511 - val_loss: 0.9524\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9497 - val_loss: 0.9509\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9468 - val_loss: 0.9509\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9484 - val_loss: 0.9491\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9439 - val_loss: 0.9504\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9449 - val_loss: 0.9460\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9427 - val_loss: 1.0121\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9467 - val_loss: 0.9461\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9394 - val_loss: 0.9447\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9387 - val_loss: 0.9457\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9413 - val_loss: 0.9599\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9431 - val_loss: 0.9492\n",
      "Top-2 accuracy = 0.85\n",
      "26\n",
      "normalizeM|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0969 - val_loss: 1.0961\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "27\n",
      "minmaxQ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0969 - val_loss: 1.0964\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0955 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "28\n",
      "minmaxU|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0505 - val_loss: 1.0038\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9864 - val_loss: 0.9692\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9600 - val_loss: 0.9510\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9524 - val_loss: 0.9497\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9520 - val_loss: 0.9875\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9574 - val_loss: 0.9522\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9518 - val_loss: 0.9490\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9470 - val_loss: 0.9562\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9474 - val_loss: 0.9526\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9452 - val_loss: 0.9489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9453 - val_loss: 0.9623\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9450 - val_loss: 0.9456\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9425 - val_loss: 0.9451\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9423 - val_loss: 0.9694\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9476 - val_loss: 0.9561\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9467 - val_loss: 0.9437\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9432 - val_loss: 0.9446\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9415 - val_loss: 0.9528\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9413 - val_loss: 0.9440\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9415 - val_loss: 0.9431\n",
      "Top-2 accuracy = 0.85\n",
      "29\n",
      "robustQ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0251 - val_loss: 0.9735\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9586 - val_loss: 0.9503\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9438 - val_loss: 0.9434\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9388 - val_loss: 0.9425\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9361 - val_loss: 0.9474\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9336 - val_loss: 0.9374\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9329 - val_loss: 0.9374\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9330 - val_loss: 0.9408\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9299 - val_loss: 0.9346\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9297 - val_loss: 0.9353\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9280 - val_loss: 0.9341\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9280 - val_loss: 0.9402\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9285 - val_loss: 0.9330\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9256 - val_loss: 0.9347\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9250 - val_loss: 0.9305\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9237 - val_loss: 0.9331\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9255 - val_loss: 0.9303\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9241 - val_loss: 0.9291\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9213 - val_loss: 0.9285\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9210 - val_loss: 0.9273\n",
      "Top-2 accuracy = 0.85\n",
      "0\n",
      "maxabsx|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0970 - val_loss: 1.0842\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0157 - val_loss: 0.9631\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9534 - val_loss: 0.9514\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9475 - val_loss: 0.9490\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9435 - val_loss: 0.9456\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9421 - val_loss: 0.9596\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9410 - val_loss: 0.9470\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9384 - val_loss: 0.9607\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9396 - val_loss: 0.9437\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9370 - val_loss: 0.9461\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9373 - val_loss: 0.9416\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9348 - val_loss: 0.9409\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9357 - val_loss: 0.9441\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9358 - val_loss: 0.9410\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9338 - val_loss: 0.9412\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9333 - val_loss: 0.9409\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9336 - val_loss: 0.9404\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9319 - val_loss: 0.9423\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9326 - val_loss: 0.9408\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9318 - val_loss: 0.9466\n",
      "Top-2 accuracy = 0.85\n",
      "1\n",
      "minmaxY|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0848 - val_loss: 1.0669\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0503 - val_loss: 1.0329\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0227 - val_loss: 1.0124\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0037 - val_loss: 0.9980\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9914 - val_loss: 0.9889\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9844 - val_loss: 0.9843\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9774 - val_loss: 0.9816\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9734 - val_loss: 0.9734\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9686 - val_loss: 0.9716\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9661 - val_loss: 0.9820\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9652 - val_loss: 0.9669\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9619 - val_loss: 0.9664\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9614 - val_loss: 0.9640\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9590 - val_loss: 0.9629\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9571 - val_loss: 0.9621\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9568 - val_loss: 0.9709\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9565 - val_loss: 0.9679\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9558 - val_loss: 0.9667\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9547 - val_loss: 0.9605\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9539 - val_loss: 0.9587\n",
      "Top-2 accuracy = 0.85\n",
      "2\n",
      "robusty|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0192 - val_loss: 0.9643\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9432 - val_loss: 0.9412\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9337 - val_loss: 0.9399\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9313 - val_loss: 0.9427\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9267 - val_loss: 0.9440\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9265 - val_loss: 0.9384\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9264 - val_loss: 0.9400\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9254 - val_loss: 0.9384\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9240 - val_loss: 0.9353\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9255 - val_loss: 0.9380\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9220 - val_loss: 0.9409\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9237 - val_loss: 0.9388\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9221 - val_loss: 0.9365\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9212 - val_loss: 0.9354\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9217 - val_loss: 0.9385\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9211 - val_loss: 0.9393\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9228 - val_loss: 0.9401\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9205 - val_loss: 0.9420\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9216 - val_loss: 0.9390\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9223 - val_loss: 0.9375\n",
      "Top-2 accuracy = 0.85\n",
      "3\n",
      "standardizeu|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0870\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0751 - val_loss: 1.0627\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0483 - val_loss: 1.0354\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0238 - val_loss: 1.0143\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0070 - val_loss: 1.0018\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9948 - val_loss: 0.9920\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9858 - val_loss: 0.9849\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9793 - val_loss: 0.9787\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9738 - val_loss: 0.9754\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9694 - val_loss: 0.9720\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9657 - val_loss: 0.9689\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9630 - val_loss: 0.9676\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9606 - val_loss: 0.9647\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9583 - val_loss: 0.9649\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9570 - val_loss: 0.9624\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9550 - val_loss: 0.9603\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9534 - val_loss: 0.9591\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9527 - val_loss: 0.9589\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9515 - val_loss: 0.9569\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9499 - val_loss: 0.9564\n",
      "Top-2 accuracy = 0.85\n",
      "4\n",
      "maxabsu|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 1.0934 - val_loss: 1.0820\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0472 - val_loss: 1.0065\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9728 - val_loss: 0.9582\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9533 - val_loss: 0.9645\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9489 - val_loss: 0.9536\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9452 - val_loss: 0.9511\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9448 - val_loss: 0.9621\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9439 - val_loss: 0.9497\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9454 - val_loss: 0.9518\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9429 - val_loss: 0.9705\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9443 - val_loss: 0.9676\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9426 - val_loss: 0.9511\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9434 - val_loss: 0.9529\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9417 - val_loss: 0.9472\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9423 - val_loss: 0.9489\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9417 - val_loss: 0.9527\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9413 - val_loss: 0.9478\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9413 - val_loss: 0.9468\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9409 - val_loss: 0.9480\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9401 - val_loss: 0.9466\n",
      "Top-2 accuracy = 0.85\n",
      "5\n",
      "robustn|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0801 - val_loss: 1.0500\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0148 - val_loss: 0.9964\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9844 - val_loss: 0.9776\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9689 - val_loss: 0.9680\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9601 - val_loss: 0.9590\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9530 - val_loss: 0.9544\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9485 - val_loss: 0.9488\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9454 - val_loss: 0.9528\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9428 - val_loss: 0.9451\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9408 - val_loss: 0.9446\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9390 - val_loss: 0.9494\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9384 - val_loss: 0.9430\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9363 - val_loss: 0.9413\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9347 - val_loss: 0.9413\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9346 - val_loss: 0.9400\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9341 - val_loss: 0.9403\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9323 - val_loss: 0.9386\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9318 - val_loss: 0.9394\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9330 - val_loss: 0.9395\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9310 - val_loss: 0.9377\n",
      "Top-2 accuracy = 0.85\n",
      "6\n",
      "normalizem|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 1.0966 - val_loss: 1.0962\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0953 - val_loss: 1.0963\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0963\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0964\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "7\n",
      "normalizet|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0640 - val_loss: 1.0393\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0209 - val_loss: 1.0075\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9957 - val_loss: 0.9847\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9784 - val_loss: 0.9954\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9725 - val_loss: 0.9677\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9647 - val_loss: 0.9650\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9615 - val_loss: 0.9619\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9592 - val_loss: 0.9605\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9581 - val_loss: 0.9602\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9572 - val_loss: 0.9642\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9563 - val_loss: 0.9636\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9561 - val_loss: 0.9582\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9570 - val_loss: 0.9601\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9542 - val_loss: 0.9569\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9521 - val_loss: 0.9567\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9518 - val_loss: 0.9549\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9514 - val_loss: 0.9543\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9501 - val_loss: 0.9555\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9504 - val_loss: 0.9614\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9521 - val_loss: 0.9596\n",
      "Top-2 accuracy = 0.84\n",
      "8\n",
      "normalizej|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 1.0498 - val_loss: 0.9996\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9731 - val_loss: 0.9626\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9606 - val_loss: 0.9551\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9526 - val_loss: 0.9535\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9494 - val_loss: 0.9555\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9465 - val_loss: 0.9497\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9471 - val_loss: 0.9473\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9424 - val_loss: 0.9456\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9418 - val_loss: 0.9431\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9406 - val_loss: 0.9468\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9403 - val_loss: 0.9501\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9411 - val_loss: 0.9449\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9397 - val_loss: 0.9451\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9392 - val_loss: 0.9431\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9383 - val_loss: 0.9425\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9389 - val_loss: 0.9470\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9367 - val_loss: 0.9408\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9383 - val_loss: 0.9413\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9365 - val_loss: 0.9440\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9378 - val_loss: 0.9409\n",
      "Top-2 accuracy = 0.85\n",
      "9\n",
      "maxabsI|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0502 - val_loss: 1.0006\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9627 - val_loss: 0.9671\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9428 - val_loss: 0.9817\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9407 - val_loss: 0.9452\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9339 - val_loss: 0.9430\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9326 - val_loss: 0.9383\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9321 - val_loss: 0.9382\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9317 - val_loss: 0.9417\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9308 - val_loss: 0.9403\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9330 - val_loss: 0.9373\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9287 - val_loss: 0.9400\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9285 - val_loss: 0.9359\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9286 - val_loss: 0.9501\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9271 - val_loss: 0.9356\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9266 - val_loss: 0.9358\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9277 - val_loss: 0.9394\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9257 - val_loss: 0.9367\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9271 - val_loss: 0.9426\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9267 - val_loss: 0.9352\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9257 - val_loss: 0.9468\n",
      "Top-2 accuracy = 0.85\n",
      "10\n",
      "maxabsb|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0968 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0953 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0923 - val_loss: 1.0777\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0624 - val_loss: 1.0471\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0519 - val_loss: 1.0612\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0573 - val_loss: 1.0624\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0590 - val_loss: 1.0630\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0582 - val_loss: 1.0543\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0561 - val_loss: 1.0640\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0523 - val_loss: 1.0493\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0341 - val_loss: 1.0203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0213 - val_loss: 1.0232\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0196 - val_loss: 1.0303\n",
      "Top-2 accuracy = 0.77\n",
      "11\n",
      "minmaxk|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0656 - val_loss: 1.0037\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9747 - val_loss: 0.9653\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9566 - val_loss: 0.9542\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9527 - val_loss: 0.9527\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9490 - val_loss: 0.9538\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9477 - val_loss: 0.9528\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9487 - val_loss: 0.9552\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9455 - val_loss: 0.9560\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9443 - val_loss: 0.9534\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9433 - val_loss: 0.9460\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9424 - val_loss: 0.9455\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9419 - val_loss: 0.9444\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9427 - val_loss: 0.9477\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9421 - val_loss: 0.9602\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9431 - val_loss: 0.9436\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9408 - val_loss: 0.9472\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9407 - val_loss: 0.9439\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9388 - val_loss: 0.9418\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9414 - val_loss: 0.9467\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9419 - val_loss: 0.9592\n",
      "Top-2 accuracy = 0.84\n",
      "12\n",
      "maxabsL|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0380 - val_loss: 0.9946\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9655 - val_loss: 0.9711\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9502 - val_loss: 0.9608\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9465 - val_loss: 0.9475\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9439 - val_loss: 0.9464\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9440 - val_loss: 0.9468\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9424 - val_loss: 0.9555\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9453 - val_loss: 0.9456\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9445 - val_loss: 0.9464\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9410 - val_loss: 0.9438\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9422 - val_loss: 0.9469\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9408 - val_loss: 0.9473\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9395 - val_loss: 0.9664\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9465 - val_loss: 0.9429\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9404 - val_loss: 0.9562\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9397 - val_loss: 0.9448\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9390 - val_loss: 0.9419\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9393 - val_loss: 0.9419\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9375 - val_loss: 0.9440\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9386 - val_loss: 0.9675\n",
      "Top-2 accuracy = 0.83\n",
      "13\n",
      "normalizeZ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0976 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "14\n",
      "maxabsX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0431 - val_loss: 1.0034\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9776 - val_loss: 0.9666\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9589 - val_loss: 0.9572\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9538 - val_loss: 0.9610\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9560 - val_loss: 0.9552\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9499 - val_loss: 0.9543\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9493 - val_loss: 0.9520\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9478 - val_loss: 0.9512\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9464 - val_loss: 0.9632\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9494 - val_loss: 0.9483\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9445 - val_loss: 0.9482\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9441 - val_loss: 0.9483\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9424 - val_loss: 0.9527\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9437 - val_loss: 0.9634\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9458 - val_loss: 0.9457\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9428 - val_loss: 0.9625\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9407 - val_loss: 0.9472\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9411 - val_loss: 0.9453\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9397 - val_loss: 0.9567\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9424 - val_loss: 0.9448\n",
      "Top-2 accuracy = 0.85\n",
      "15\n",
      "normalizeF|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0709 - val_loss: 1.0250\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9955 - val_loss: 0.9841\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9691 - val_loss: 0.9679\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9591 - val_loss: 0.9605\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9552 - val_loss: 0.9883\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9523 - val_loss: 0.9605\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9477 - val_loss: 0.9561\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9456 - val_loss: 0.9623\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9445 - val_loss: 0.9544\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9471 - val_loss: 0.9619\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9446 - val_loss: 0.9562\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9422 - val_loss: 0.9546\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9406 - val_loss: 0.9471\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9395 - val_loss: 0.9673\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9403 - val_loss: 0.9468\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9402 - val_loss: 0.9660\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9408 - val_loss: 0.9505\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9381 - val_loss: 0.9457\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9378 - val_loss: 0.9443\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9386 - val_loss: 0.9471\n",
      "Top-2 accuracy = 0.85\n",
      "16\n",
      "standardizeO|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0423 - val_loss: 0.9971\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9693 - val_loss: 0.9620\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9473 - val_loss: 0.9546\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9436 - val_loss: 0.9540\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9384 - val_loss: 0.9445\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9355 - val_loss: 0.9422\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9323 - val_loss: 0.9441\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9308 - val_loss: 0.9422\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9312 - val_loss: 0.9520\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9305 - val_loss: 0.9398\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9293 - val_loss: 0.9403\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9289 - val_loss: 0.9385\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9272 - val_loss: 0.9399\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9276 - val_loss: 0.9376\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9264 - val_loss: 0.9555\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9288 - val_loss: 0.9413\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9261 - val_loss: 0.9379\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9271 - val_loss: 0.9369\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9267 - val_loss: 0.9366\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9249 - val_loss: 0.9379\n",
      "Top-2 accuracy = 0.85\n",
      "17\n",
      "minmaxr|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0488 - val_loss: 0.9922\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9745 - val_loss: 0.9968\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9577 - val_loss: 0.9675\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9565 - val_loss: 0.9611\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9497 - val_loss: 0.9497\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9441 - val_loss: 0.9541\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9420 - val_loss: 0.9525\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9461 - val_loss: 0.9456\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9423 - val_loss: 0.9775\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9466 - val_loss: 0.9450\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9399 - val_loss: 0.9419\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9376 - val_loss: 0.9654\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9434 - val_loss: 0.9399\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9369 - val_loss: 0.9415\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9361 - val_loss: 0.9520\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9359 - val_loss: 0.9517\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9362 - val_loss: 0.9451\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9369 - val_loss: 0.9391\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9355 - val_loss: 0.9398\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9351 - val_loss: 0.9485\n",
      "Top-2 accuracy = 0.85\n",
      "18\n",
      "minmaxv|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 1.0965 - val_loss: 1.0961\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0948 - val_loss: 1.0947\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0779 - val_loss: 1.0761\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0708 - val_loss: 1.0742\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0558 - val_loss: 1.0447\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0574 - val_loss: 1.0422\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0381 - val_loss: 1.0394\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0571 - val_loss: 1.0792\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0747 - val_loss: 1.0662\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0526 - val_loss: 1.0593\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0624 - val_loss: 1.0684\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0628 - val_loss: 1.0664\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0616 - val_loss: 1.0664\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0617 - val_loss: 1.0668\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0664 - val_loss: 1.0920\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0909 - val_loss: 1.0923\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0773 - val_loss: 1.0743\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0690 - val_loss: 1.0742\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0704 - val_loss: 1.0780\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0795 - val_loss: 1.0840\n",
      "Top-2 accuracy = 0.74\n",
      "19\n",
      "standardizeD|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 1.0640 - val_loss: 1.0345\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9975 - val_loss: 0.9795\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9613 - val_loss: 0.9584\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9491 - val_loss: 0.9477\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9413 - val_loss: 0.9443\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9419 - val_loss: 0.9454\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9390 - val_loss: 0.9426\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9356 - val_loss: 0.9399\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9366 - val_loss: 0.9403\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9360 - val_loss: 0.9605\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9383 - val_loss: 0.9399\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9343 - val_loss: 0.9410\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9360 - val_loss: 0.9464\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9348 - val_loss: 0.9402\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9334 - val_loss: 0.9520\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9365 - val_loss: 0.9419\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9323 - val_loss: 0.9401\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9310 - val_loss: 0.9370\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9312 - val_loss: 0.9411\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9309 - val_loss: 0.9366\n",
      "Top-2 accuracy = 0.85\n",
      "20\n",
      "robustd|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0823 - val_loss: 1.0583\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0271 - val_loss: 1.0099\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9945 - val_loss: 0.9862\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9781 - val_loss: 0.9752\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9705 - val_loss: 0.9713\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9655 - val_loss: 0.9682\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9632 - val_loss: 0.9625\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9594 - val_loss: 0.9598\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9564 - val_loss: 0.9587\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9548 - val_loss: 0.9556\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9529 - val_loss: 0.9616\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9533 - val_loss: 0.9543\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9516 - val_loss: 0.9537\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9520 - val_loss: 0.9556\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9509 - val_loss: 0.9555\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9489 - val_loss: 0.9531\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9493 - val_loss: 0.9511\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9486 - val_loss: 0.9511\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9493 - val_loss: 0.9504\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9480 - val_loss: 0.9522\n",
      "Top-2 accuracy = 0.84\n",
      "21\n",
      "robustf|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0970 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0954 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "22\n",
      "maxabsQ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 1.0695 - val_loss: 1.0260\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9843 - val_loss: 0.9674\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9517 - val_loss: 0.9561\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9443 - val_loss: 0.9539\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9421 - val_loss: 0.9492\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9400 - val_loss: 0.9528\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9386 - val_loss: 0.9601\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9422 - val_loss: 0.9481\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9374 - val_loss: 0.9843\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9418 - val_loss: 0.9455\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.9379 - val_loss: 0.9450\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9350 - val_loss: 0.9558\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9348 - val_loss: 0.9439\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9338 - val_loss: 0.9442\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9357 - val_loss: 0.9754\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9347 - val_loss: 0.9439\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9322 - val_loss: 0.9395\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9312 - val_loss: 0.9415\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9311 - val_loss: 0.9402\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9306 - val_loss: 0.9431\n",
      "Top-2 accuracy = 0.85\n",
      "23\n",
      "maxabsJ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0503 - val_loss: 1.0014\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9778 - val_loss: 0.9649\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9586 - val_loss: 0.9608\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9537 - val_loss: 0.9553\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9481 - val_loss: 0.9509\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9455 - val_loss: 0.9523\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9431 - val_loss: 0.9472\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9409 - val_loss: 0.9472\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9393 - val_loss: 0.9450\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9373 - val_loss: 0.9474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9372 - val_loss: 0.9475\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9373 - val_loss: 0.9473\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9368 - val_loss: 0.9435\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9333 - val_loss: 0.9419\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9329 - val_loss: 0.9414\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9336 - val_loss: 0.9401\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9334 - val_loss: 0.9411\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9311 - val_loss: 0.9438\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9312 - val_loss: 0.9438\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9313 - val_loss: 0.9417\n",
      "Top-2 accuracy = 0.85\n",
      "24\n",
      "maxabsf|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0969 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0955 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "25\n",
      "maxabsv|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 1.0956 - val_loss: 1.0962\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0963\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0963\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Top-2 accuracy = 0.7\n",
      "26\n",
      "minmaxH|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 1.0304 - val_loss: 0.9841\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.9687 - val_loss: 1.0133\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.9643 - val_loss: 0.9754\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.9583 - val_loss: 0.9620\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9539 - val_loss: 0.9751\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 0.9525 - val_loss: 0.9722\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.9512 - val_loss: 0.9928\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9533 - val_loss: 0.9621\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9511 - val_loss: 0.9723\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9496 - val_loss: 0.9701\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9517 - val_loss: 0.9543\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9514 - val_loss: 0.9570\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9457 - val_loss: 0.9615\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9466 - val_loss: 0.9566\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.9461 - val_loss: 0.9728\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9502 - val_loss: 0.9497\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9457 - val_loss: 0.9549\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9443 - val_loss: 0.9522\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9501 - val_loss: 0.9566\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9463 - val_loss: 0.9480\n",
      "Top-2 accuracy = 0.85\n",
      "27\n",
      "minmaxX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0583 - val_loss: 1.0202\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9945 - val_loss: 0.9904\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9736 - val_loss: 0.9705\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9664 - val_loss: 0.9655\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9639 - val_loss: 0.9654\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9646 - val_loss: 0.9649\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9596 - val_loss: 0.9600\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9578 - val_loss: 0.9608\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.9569 - val_loss: 0.9590\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9557 - val_loss: 0.9572\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9578 - val_loss: 0.9579\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9539 - val_loss: 0.9575\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9544 - val_loss: 0.9719\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9584 - val_loss: 0.9563\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9522 - val_loss: 0.9551\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9503 - val_loss: 0.9568\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9513 - val_loss: 0.9561\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9491 - val_loss: 0.9564\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9486 - val_loss: 0.9547\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9489 - val_loss: 0.9547\n",
      "Top-2 accuracy = 0.84\n",
      "28\n",
      "minmaxD|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0971 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0954 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Top-2 accuracy = 0.7\n",
      "29\n",
      "standardizez|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0277 - val_loss: 0.9930\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9743 - val_loss: 0.9634\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9501 - val_loss: 0.9507\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9450 - val_loss: 0.9518\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9398 - val_loss: 0.9477\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9371 - val_loss: 0.9484\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9352 - val_loss: 0.9428\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9337 - val_loss: 0.9423\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9332 - val_loss: 0.9404\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9336 - val_loss: 0.9408\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9319 - val_loss: 0.9402\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9303 - val_loss: 0.9391\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9290 - val_loss: 0.9428\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9286 - val_loss: 0.9386\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9285 - val_loss: 0.9372\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9264 - val_loss: 0.9364\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9274 - val_loss: 0.9382\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9257 - val_loss: 0.9365\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9264 - val_loss: 0.9398\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9286 - val_loss: 0.9367\n",
      "Top-2 accuracy = 0.85\n",
      "0\n",
      "standardizeR|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0632 - val_loss: 1.0081\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9796 - val_loss: 0.9645\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9517 - val_loss: 0.9502\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9437 - val_loss: 0.9477\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9420 - val_loss: 0.9478\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9387 - val_loss: 0.9490\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9367 - val_loss: 0.9419\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9353 - val_loss: 0.9408\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9339 - val_loss: 0.9464\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9338 - val_loss: 0.9414\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9322 - val_loss: 0.9418\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9325 - val_loss: 0.9385\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9313 - val_loss: 0.9367\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9316 - val_loss: 0.9397\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9316 - val_loss: 0.9384\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9319 - val_loss: 0.9402\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9304 - val_loss: 0.9362\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9289 - val_loss: 0.9375\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9292 - val_loss: 0.9352\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9273 - val_loss: 0.9360\n",
      "Top-2 accuracy = 0.85\n",
      "1\n",
      "robusta|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0796 - val_loss: 1.0307\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9878 - val_loss: 0.9654\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9582 - val_loss: 0.9575\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9500 - val_loss: 0.9538\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9451 - val_loss: 0.9507\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9404 - val_loss: 0.9462\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9366 - val_loss: 0.9424\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9340 - val_loss: 0.9411\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9327 - val_loss: 0.9411\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9319 - val_loss: 0.9395\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9297 - val_loss: 0.9390\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9287 - val_loss: 0.9379\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9282 - val_loss: 0.9369\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9272 - val_loss: 0.9367\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9258 - val_loss: 0.9351\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9253 - val_loss: 0.9344\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9255 - val_loss: 0.9348\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9253 - val_loss: 0.9335\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9242 - val_loss: 0.9350\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9231 - val_loss: 0.9338\n",
      "Top-2 accuracy = 0.85\n",
      "2\n",
      "robustX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 11ms/step - loss: 1.0670 - val_loss: 1.0266\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9983 - val_loss: 0.9859\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9759 - val_loss: 0.9765\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9639 - val_loss: 0.9699\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9562 - val_loss: 0.9572\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9495 - val_loss: 0.9552\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9483 - val_loss: 0.9811\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9464 - val_loss: 0.9484\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9413 - val_loss: 0.9499\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9399 - val_loss: 0.9466\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9467 - val_loss: 0.9534\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9389 - val_loss: 0.9439\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9369 - val_loss: 0.9427\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9359 - val_loss: 0.9566\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9357 - val_loss: 0.9568\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9388 - val_loss: 0.9437\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9348 - val_loss: 0.9427\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9350 - val_loss: 0.9416\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9353 - val_loss: 0.9428\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9356 - val_loss: 0.9511\n",
      "Top-2 accuracy = 0.85\n",
      "3\n",
      "normalizeQ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 1.0810 - val_loss: 1.0384\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0145 - val_loss: 0.9968\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9808 - val_loss: 0.9791\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.9780 - val_loss: 1.0228\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.9667 - val_loss: 0.9625\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.9546 - val_loss: 0.9619\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.9522 - val_loss: 0.9540\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.9490 - val_loss: 0.9772\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.9497 - val_loss: 0.9508\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.9475 - val_loss: 0.9632\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.9455 - val_loss: 0.9478\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.9431 - val_loss: 0.9489\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9416 - val_loss: 0.9494\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.9408 - val_loss: 0.9487\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.9412 - val_loss: 0.9516\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 0.9421 - val_loss: 0.9660\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - ETA: 0s - loss: 0.943 - 1s 6ms/step - loss: 0.9432 - val_loss: 0.9504\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.9414 - val_loss: 0.9433\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9368 - val_loss: 0.9534\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9384 - val_loss: 0.9470\n",
      "Top-2 accuracy = 0.85\n",
      "4\n",
      "maxabsT|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 1.0515 - val_loss: 1.0168\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0084 - val_loss: 1.0118\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0080 - val_loss: 1.0140\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9931 - val_loss: 0.9863\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9615 - val_loss: 0.9764\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9509 - val_loss: 0.9727\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9450 - val_loss: 0.9443\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9392 - val_loss: 0.9719\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9409 - val_loss: 0.9481\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9376 - val_loss: 0.9446\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9365 - val_loss: 0.9404\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9333 - val_loss: 0.9467\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9328 - val_loss: 0.9420\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9336 - val_loss: 0.9413\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9346 - val_loss: 0.9633\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9339 - val_loss: 0.9391\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9299 - val_loss: 0.9433\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9296 - val_loss: 0.9398\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9290 - val_loss: 0.9449\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9316 - val_loss: 0.9400\n",
      "Top-2 accuracy = 0.85\n",
      "5\n",
      "robustx|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0884 - val_loss: 1.0700\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0410 - val_loss: 1.0177\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.0032 - val_loss: 1.0017\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9918 - val_loss: 0.9929\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9829 - val_loss: 0.9849\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9747 - val_loss: 0.9771\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9682 - val_loss: 0.9720\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9629 - val_loss: 0.9695\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9586 - val_loss: 0.9634\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9552 - val_loss: 0.9622\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9525 - val_loss: 0.9580\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9506 - val_loss: 0.9575\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9487 - val_loss: 0.9555\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9483 - val_loss: 0.9541\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9460 - val_loss: 0.9533\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9449 - val_loss: 0.9539\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9445 - val_loss: 0.9509\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9433 - val_loss: 0.9519\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9426 - val_loss: 0.9501\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 0.9415 - val_loss: 0.9499\n",
      "Top-2 accuracy = 0.84\n",
      "6\n",
      "standardizeb|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0744 - val_loss: 1.0506\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0374 - val_loss: 1.0263\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0148 - val_loss: 1.0062\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9959 - val_loss: 0.9889\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9783 - val_loss: 0.9770\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9668 - val_loss: 0.9670\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9596 - val_loss: 0.9655\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9563 - val_loss: 0.9617\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9551 - val_loss: 0.9580\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9531 - val_loss: 0.9566\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9510 - val_loss: 0.9572\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9505 - val_loss: 0.9546\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9491 - val_loss: 0.9550\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9488 - val_loss: 0.9542\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9488 - val_loss: 0.9557\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9475 - val_loss: 0.9533\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9485 - val_loss: 0.9533\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9480 - val_loss: 0.9530\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9463 - val_loss: 0.9535\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9463 - val_loss: 0.9544\n",
      "Top-2 accuracy = 0.85\n",
      "7\n",
      "normalizeJ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0969 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0954 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "8\n",
      "normalizey|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 1.0969 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0954 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "9\n",
      "maxabss|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0969 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0954 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "10\n",
      "standardizeB|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0567 - val_loss: 1.0114\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9810 - val_loss: 0.9694\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9597 - val_loss: 0.9580\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9523 - val_loss: 0.9558\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9491 - val_loss: 0.9527\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9463 - val_loss: 0.9521\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9448 - val_loss: 0.9501\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9430 - val_loss: 0.9491\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9413 - val_loss: 0.9467\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9409 - val_loss: 0.9454\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9393 - val_loss: 0.9439\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9383 - val_loss: 0.9452\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9379 - val_loss: 0.9461\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9366 - val_loss: 0.9415\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9355 - val_loss: 0.9412\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9353 - val_loss: 0.9402\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9347 - val_loss: 0.9401\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9346 - val_loss: 0.9400\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9335 - val_loss: 0.9402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9327 - val_loss: 0.9419\n",
      "Top-2 accuracy = 0.85\n",
      "11\n",
      "standardizev|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0214 - val_loss: 0.9759\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9458 - val_loss: 0.9437\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9330 - val_loss: 0.9418\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9289 - val_loss: 0.9372\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9273 - val_loss: 0.9516\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9301 - val_loss: 0.9370\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9271 - val_loss: 0.9372\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9275 - val_loss: 0.9383\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9257 - val_loss: 0.9380\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9252 - val_loss: 0.9368\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9240 - val_loss: 0.9352\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9246 - val_loss: 0.9354\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9235 - val_loss: 0.9383\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9240 - val_loss: 0.9444\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9254 - val_loss: 0.9509\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9287 - val_loss: 0.9359\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9227 - val_loss: 0.9411\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9232 - val_loss: 0.9351\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9230 - val_loss: 0.9353\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9232 - val_loss: 0.9403\n",
      "Top-2 accuracy = 0.85\n",
      "12\n",
      "normalizeY|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 1.0602 - val_loss: 1.0238\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9890 - val_loss: 0.9686\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9512 - val_loss: 0.9547\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9446 - val_loss: 0.9673\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9420 - val_loss: 0.9645\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9388 - val_loss: 0.9591\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9407 - val_loss: 0.9422\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9373 - val_loss: 0.9538\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9387 - val_loss: 0.9669\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9398 - val_loss: 0.9511\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9363 - val_loss: 0.9553\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9374 - val_loss: 0.9651\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9378 - val_loss: 0.9416\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9375 - val_loss: 0.9403\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9350 - val_loss: 0.9527\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9346 - val_loss: 0.9561\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9396 - val_loss: 0.9491\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9355 - val_loss: 0.9379\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9335 - val_loss: 0.9538\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9339 - val_loss: 0.9606\n",
      "Top-2 accuracy = 0.84\n",
      "13\n",
      "maxabsD|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0879 - val_loss: 1.0604\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0137 - val_loss: 0.9895\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9680 - val_loss: 0.9714\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9521 - val_loss: 0.9509\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9435 - val_loss: 0.9458\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9427 - val_loss: 0.9463\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9398 - val_loss: 0.9500\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9384 - val_loss: 0.9416\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9390 - val_loss: 0.9568\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9379 - val_loss: 0.9428\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9359 - val_loss: 0.9405\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9342 - val_loss: 0.9405\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9350 - val_loss: 0.9402\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9354 - val_loss: 0.9534\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9335 - val_loss: 0.9398\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9333 - val_loss: 0.9431\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9337 - val_loss: 0.9388\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9335 - val_loss: 0.9370\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9310 - val_loss: 0.9449\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9316 - val_loss: 0.9455\n",
      "Top-2 accuracy = 0.85\n",
      "14\n",
      "normalizes|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 1.0744 - val_loss: 1.0367\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0158 - val_loss: 1.0077\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9905 - val_loss: 0.9903\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9817 - val_loss: 0.9842\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9792 - val_loss: 0.9837\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9785 - val_loss: 1.0461\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9797 - val_loss: 0.9951\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9767 - val_loss: 0.9839\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9772 - val_loss: 0.9878\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9743 - val_loss: 0.9780\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9720 - val_loss: 0.9761\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9729 - val_loss: 0.9859\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9721 - val_loss: 0.9881\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9710 - val_loss: 0.9764\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9655 - val_loss: 0.9673\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9655 - val_loss: 0.9662\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9664 - val_loss: 0.9659\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9612 - val_loss: 0.9654\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9602 - val_loss: 0.9917\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9604 - val_loss: 0.9662\n",
      "Top-2 accuracy = 0.84\n",
      "15\n",
      "minmaxa|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0959 - val_loss: 1.0964\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0953 - val_loss: 1.0964\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0949 - val_loss: 1.0931\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0587 - val_loss: 1.0210\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9972 - val_loss: 0.9967\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9813 - val_loss: 0.9778\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9765 - val_loss: 0.9848\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9725 - val_loss: 0.9697\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9668 - val_loss: 0.9680\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9626 - val_loss: 0.9704\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9635 - val_loss: 0.9637\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9581 - val_loss: 0.9669\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9604 - val_loss: 0.9785\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9584 - val_loss: 0.9802\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9559 - val_loss: 0.9575\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9547 - val_loss: 0.9561\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9523 - val_loss: 0.9550\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9512 - val_loss: 0.9724\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9521 - val_loss: 0.9729\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9532 - val_loss: 0.9557\n",
      "Top-2 accuracy = 0.84\n",
      "16\n",
      "minmaxn|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 1.1192 - val_loss: 1.0981\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0962 - val_loss: 1.0962\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "17\n",
      "normalizeW|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 1.0643 - val_loss: 1.0185\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9911 - val_loss: 0.9967\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9753 - val_loss: 0.9678\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9657 - val_loss: 0.9776\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9630 - val_loss: 0.9655\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9588 - val_loss: 0.9623\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9566 - val_loss: 0.9600\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9553 - val_loss: 0.9813\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9579 - val_loss: 0.9576\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9535 - val_loss: 0.9551\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9542 - val_loss: 0.9768\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9527 - val_loss: 0.9644\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9531 - val_loss: 0.9611\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9509 - val_loss: 0.9679\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9512 - val_loss: 0.9572\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9500 - val_loss: 0.9496\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9485 - val_loss: 0.9504\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9471 - val_loss: 0.9481\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9466 - val_loss: 0.9476\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9458 - val_loss: 0.9491\n",
      "Top-2 accuracy = 0.85\n",
      "18\n",
      "minmaxl|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0695 - val_loss: 1.0031\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9791 - val_loss: 0.9876\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9664 - val_loss: 0.9757\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9602 - val_loss: 0.9897\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9676 - val_loss: 0.9567\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9512 - val_loss: 0.9526\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9513 - val_loss: 0.9616\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9475 - val_loss: 0.9492\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9453 - val_loss: 0.9506\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9455 - val_loss: 0.9460\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9464 - val_loss: 0.9532\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9467 - val_loss: 0.9462\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9417 - val_loss: 0.9485\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9427 - val_loss: 0.9582\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9426 - val_loss: 0.9485\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9401 - val_loss: 0.9822\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9441 - val_loss: 0.9435\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9391 - val_loss: 0.9467\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9388 - val_loss: 0.9411\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9368 - val_loss: 0.9529\n",
      "Top-2 accuracy = 0.84\n",
      "19\n",
      "standardizen|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0975 - val_loss: 1.0964\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0955 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "20\n",
      "robustm|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0612 - val_loss: 1.0018\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9676 - val_loss: 0.9526\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9409 - val_loss: 0.9495\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9357 - val_loss: 0.9439\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9345 - val_loss: 0.9415\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9331 - val_loss: 0.9393\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9313 - val_loss: 0.9395\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9304 - val_loss: 0.9412\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9310 - val_loss: 0.9385\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9298 - val_loss: 0.9387\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9283 - val_loss: 0.9386\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9270 - val_loss: 0.9402\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9280 - val_loss: 0.9374\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9273 - val_loss: 0.9386\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9273 - val_loss: 0.9371\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9272 - val_loss: 0.9368\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9269 - val_loss: 0.9381\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9262 - val_loss: 0.9355\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9254 - val_loss: 0.9390\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9268 - val_loss: 0.9372\n",
      "Top-2 accuracy = 0.85\n",
      "21\n",
      "maxabsn|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0818 - val_loss: 1.0415\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0046 - val_loss: 0.9800\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9669 - val_loss: 0.9682\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9567 - val_loss: 0.9591\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9574 - val_loss: 0.9566\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9519 - val_loss: 0.9541\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9499 - val_loss: 0.9515\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9465 - val_loss: 0.9585\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9456 - val_loss: 0.9513\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9439 - val_loss: 0.9485\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9432 - val_loss: 0.9492\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9408 - val_loss: 0.9460\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9400 - val_loss: 0.9510\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9399 - val_loss: 0.9453\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9388 - val_loss: 0.9477\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9399 - val_loss: 0.9492\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9382 - val_loss: 0.9437\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9373 - val_loss: 0.9438\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9354 - val_loss: 0.9483\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9347 - val_loss: 0.9453\n",
      "Top-2 accuracy = 0.85\n",
      "22\n",
      "normalizeX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0398 - val_loss: 0.9905\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9737 - val_loss: 0.9823\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9637 - val_loss: 0.9652\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9570 - val_loss: 0.9700\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9548 - val_loss: 0.9586\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9529 - val_loss: 0.9655\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9518 - val_loss: 0.9563\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9500 - val_loss: 0.9638\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9542 - val_loss: 0.9550\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9485 - val_loss: 0.9557\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9475 - val_loss: 0.9539\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9462 - val_loss: 0.9503\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9449 - val_loss: 0.9643\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9451 - val_loss: 0.9490\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9439 - val_loss: 0.9558\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9436 - val_loss: 0.9624\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9439 - val_loss: 0.9545\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9424 - val_loss: 0.9531\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9421 - val_loss: 0.9478\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9413 - val_loss: 0.9522\n",
      "Top-2 accuracy = 0.85\n",
      "23\n",
      "standardizeE|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 1.0884 - val_loss: 1.0769\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0715 - val_loss: 1.0704\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0683 - val_loss: 1.0688\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0668 - val_loss: 1.0685\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0671 - val_loss: 1.0684\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0690 - val_loss: 1.0670\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0662 - val_loss: 1.0666\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0642 - val_loss: 1.0619\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0657 - val_loss: 1.0642\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0624 - val_loss: 1.0619\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0635 - val_loss: 1.0623\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0608 - val_loss: 1.0587\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0472 - val_loss: 1.0426\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0408 - val_loss: 1.0438\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0480 - val_loss: 1.0487\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0466 - val_loss: 1.0375\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0361 - val_loss: 1.0290\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0274 - val_loss: 1.0304\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0315 - val_loss: 1.0353\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0332 - val_loss: 1.0411\n",
      "Top-2 accuracy = 0.77\n",
      "24\n",
      "robustJ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0614 - val_loss: 1.0244\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9968 - val_loss: 0.9798\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9647 - val_loss: 0.9641\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9523 - val_loss: 0.9545\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9467 - val_loss: 0.9554\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9442 - val_loss: 0.9495\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9423 - val_loss: 0.9494\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9416 - val_loss: 0.9495\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9397 - val_loss: 0.9473\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9396 - val_loss: 0.9485\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9386 - val_loss: 0.9472\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9387 - val_loss: 0.9481\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9380 - val_loss: 0.9464\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9371 - val_loss: 0.9464\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9373 - val_loss: 0.9449\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9365 - val_loss: 0.9436\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9366 - val_loss: 0.9445\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9360 - val_loss: 0.9433\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9359 - val_loss: 0.9433\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9355 - val_loss: 0.9427\n",
      "Top-2 accuracy = 0.85\n",
      "25\n",
      "standardizeP|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 1.1011 - val_loss: 1.0961\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0953 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0963\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0964\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0965\n",
      "Top-2 accuracy = 0.7\n",
      "26\n",
      "maxabsS|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0968 - val_loss: 1.0951\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0870 - val_loss: 1.0770\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0490 - val_loss: 1.0215\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9902 - val_loss: 0.9754\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9621 - val_loss: 0.9633\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9522 - val_loss: 0.9617\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9492 - val_loss: 0.9563\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9476 - val_loss: 0.9546\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9467 - val_loss: 0.9537\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9445 - val_loss: 0.9553\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9443 - val_loss: 0.9534\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9431 - val_loss: 0.9483\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9426 - val_loss: 0.9483\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9426 - val_loss: 0.9503\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9423 - val_loss: 0.9494\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9439 - val_loss: 0.9476\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9409 - val_loss: 0.9494\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9419 - val_loss: 0.9466\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9417 - val_loss: 0.9492\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9410 - val_loss: 0.9467\n",
      "Top-2 accuracy = 0.85\n",
      "27\n",
      "robusts|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 1.0579 - val_loss: 1.0128\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9965 - val_loss: 0.9792\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9650 - val_loss: 0.9779\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9563 - val_loss: 0.9544\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9465 - val_loss: 0.9632\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9476 - val_loss: 0.9467\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9414 - val_loss: 0.9533\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9416 - val_loss: 0.9529\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9446 - val_loss: 0.9455\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9386 - val_loss: 0.9487\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9374 - val_loss: 0.9429\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9357 - val_loss: 0.9513\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9356 - val_loss: 0.9461\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9360 - val_loss: 0.9458\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9359 - val_loss: 0.9489\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9359 - val_loss: 0.9440\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9343 - val_loss: 0.9472\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9337 - val_loss: 0.9413\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9340 - val_loss: 0.9408\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9336 - val_loss: 0.9414\n",
      "Top-2 accuracy = 0.85\n",
      "28\n",
      "robustc|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0929 - val_loss: 1.0839\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0676 - val_loss: 1.0565\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0406 - val_loss: 1.0268\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0099 - val_loss: 1.0081\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9937 - val_loss: 0.9977\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9854 - val_loss: 0.9965\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9813 - val_loss: 0.9898\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9779 - val_loss: 0.9884\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9773 - val_loss: 0.9873\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9760 - val_loss: 0.9859\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9762 - val_loss: 0.9861\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9728 - val_loss: 0.9837\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9724 - val_loss: 0.9807\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9721 - val_loss: 0.9788\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9702 - val_loss: 0.9792\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9693 - val_loss: 0.9774\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9690 - val_loss: 0.9789\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9687 - val_loss: 0.9753\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9672 - val_loss: 0.9737\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9655 - val_loss: 0.9742\n",
      "Top-2 accuracy = 0.84\n",
      "29\n",
      "minmaxX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0953 - val_loss: 1.0878\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0347 - val_loss: 1.0053\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9906 - val_loss: 0.9885\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9808 - val_loss: 0.9862\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9774 - val_loss: 0.9814\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9742 - val_loss: 0.9791\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9727 - val_loss: 0.9791\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9722 - val_loss: 0.9789\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9709 - val_loss: 0.9755\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9693 - val_loss: 0.9756\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9681 - val_loss: 0.9795\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9685 - val_loss: 0.9725\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9666 - val_loss: 0.9722\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9654 - val_loss: 0.9758\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9646 - val_loss: 0.9703\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9633 - val_loss: 0.9689\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9619 - val_loss: 0.9690\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9609 - val_loss: 0.9669\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9601 - val_loss: 0.9670\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9595 - val_loss: 0.9643\n",
      "Top-2 accuracy = 0.84\n",
      "0\n",
      "maxabst|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0729 - val_loss: 1.0184\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9758 - val_loss: 0.9629\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9547 - val_loss: 0.9890\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9525 - val_loss: 0.9567\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9502 - val_loss: 0.9537\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9463 - val_loss: 0.9510\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9463 - val_loss: 0.9505\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9461 - val_loss: 0.9488\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9421 - val_loss: 0.9487\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9438 - val_loss: 0.9526\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9422 - val_loss: 0.9582\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9466 - val_loss: 0.9543\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9442 - val_loss: 0.9471\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9413 - val_loss: 0.9464\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9406 - val_loss: 0.9569\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9406 - val_loss: 0.9499\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9401 - val_loss: 0.9481\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9404 - val_loss: 0.9453\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9383 - val_loss: 0.9593\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9418 - val_loss: 0.9628\n",
      "Top-2 accuracy = 0.84\n",
      "1\n",
      "robustg|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 1.0838 - val_loss: 1.0535\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0364 - val_loss: 1.0273\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0207 - val_loss: 1.0242\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0214 - val_loss: 1.0243\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0206 - val_loss: 1.0236\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0217 - val_loss: 1.0327\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0305 - val_loss: 1.0308\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0341 - val_loss: 1.0449\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0401 - val_loss: 1.0460\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0405 - val_loss: 1.0461\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0397 - val_loss: 1.0445\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0392 - val_loss: 1.0450\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0393 - val_loss: 1.0457\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0442 - val_loss: 1.1043\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0981 - val_loss: 1.0962\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0954 - val_loss: 1.0960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0953 - val_loss: 1.0960\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0964\n",
      "Top-2 accuracy = 0.7\n",
      "2\n",
      "normalizem|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0722 - val_loss: 1.0453\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0138 - val_loss: 0.9894\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9763 - val_loss: 0.9734\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9664 - val_loss: 0.9696\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9628 - val_loss: 0.9667\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9608 - val_loss: 0.9662\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9592 - val_loss: 0.9642\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9574 - val_loss: 0.9637\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9563 - val_loss: 0.9614\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9546 - val_loss: 0.9601\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9536 - val_loss: 0.9586\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9532 - val_loss: 0.9580\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9513 - val_loss: 0.9565\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9502 - val_loss: 0.9562\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9484 - val_loss: 0.9544\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9469 - val_loss: 0.9548\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9463 - val_loss: 0.9542\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9449 - val_loss: 0.9534\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9447 - val_loss: 0.9533\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9435 - val_loss: 0.9549\n",
      "Top-2 accuracy = 0.84\n",
      "3\n",
      "robustF|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0130 - val_loss: 0.9727\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9547 - val_loss: 0.9517\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9446 - val_loss: 0.9637\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9449 - val_loss: 0.9553\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9401 - val_loss: 0.9503\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9389 - val_loss: 0.9456\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9385 - val_loss: 0.9463\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9368 - val_loss: 0.9431\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9370 - val_loss: 0.9525\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9355 - val_loss: 0.9495\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9358 - val_loss: 0.9434\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9356 - val_loss: 0.9432\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9333 - val_loss: 0.9437\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9340 - val_loss: 0.9423\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9347 - val_loss: 0.9441\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9329 - val_loss: 0.9419\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9325 - val_loss: 0.9428\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9331 - val_loss: 0.9431\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9325 - val_loss: 0.9413\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9322 - val_loss: 0.9412\n",
      "Top-2 accuracy = 0.85\n",
      "4\n",
      "robustO|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0662 - val_loss: 1.0284\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9955 - val_loss: 0.9794\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9633 - val_loss: 0.9601\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9506 - val_loss: 0.9580\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9487 - val_loss: 0.9526\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9460 - val_loss: 0.9531\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9452 - val_loss: 0.9496\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9436 - val_loss: 0.9506\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9408 - val_loss: 0.9476\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9410 - val_loss: 0.9499\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9388 - val_loss: 0.9828\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9441 - val_loss: 0.9547\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9389 - val_loss: 0.9462\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9389 - val_loss: 0.9465\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9382 - val_loss: 0.9449\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9375 - val_loss: 0.9497\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9369 - val_loss: 0.9476\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9374 - val_loss: 0.9454\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9356 - val_loss: 0.9476\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9355 - val_loss: 0.9436\n",
      "Top-2 accuracy = 0.85\n",
      "5\n",
      "standardizeX|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0864 - val_loss: 1.0507\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0223 - val_loss: 1.0107\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9922 - val_loss: 0.9738\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9603 - val_loss: 0.9579\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9502 - val_loss: 0.9516\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9454 - val_loss: 0.9886\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9494 - val_loss: 0.9547\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9404 - val_loss: 0.9458\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9383 - val_loss: 0.9447\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9390 - val_loss: 0.9460\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 3s 33ms/step - loss: 0.9369 - val_loss: 0.9428\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9357 - val_loss: 0.9444\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9356 - val_loss: 0.9417\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9333 - val_loss: 0.9511\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9340 - val_loss: 0.9443\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9330 - val_loss: 0.9448\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9315 - val_loss: 0.9467\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9320 - val_loss: 0.9411\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9306 - val_loss: 0.9408\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9305 - val_loss: 0.9401\n",
      "Top-2 accuracy = 0.85\n",
      "6\n",
      "maxabsm|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0971 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0953 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0959\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0674 - val_loss: 1.0295\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0085 - val_loss: 0.9939\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9795 - val_loss: 0.9734\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9628 - val_loss: 0.9635\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9564 - val_loss: 0.9623\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9517 - val_loss: 0.9558\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9490 - val_loss: 0.9569\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9485 - val_loss: 0.9542\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9474 - val_loss: 0.9539\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9464 - val_loss: 0.9525\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9445 - val_loss: 0.9510\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9436 - val_loss: 0.9501\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9431 - val_loss: 0.9501\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9428 - val_loss: 0.9496\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9429 - val_loss: 0.9531\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9430 - val_loss: 0.9504\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9424 - val_loss: 0.9491\n",
      "Top-2 accuracy = 0.85\n",
      "7\n",
      "robusty|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 1.0619 - val_loss: 1.0344\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0005 - val_loss: 1.0009\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9855 - val_loss: 1.0198\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9999 - val_loss: 1.0237\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9935 - val_loss: 1.0135\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9919 - val_loss: 0.9952\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9860 - val_loss: 0.9940\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9805 - val_loss: 0.9857\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9594 - val_loss: 0.9874\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9677 - val_loss: 0.9726\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9547 - val_loss: 0.9729\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9557 - val_loss: 0.9620\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9539 - val_loss: 0.9569\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9498 - val_loss: 0.9696\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9649 - val_loss: 0.9696\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9550 - val_loss: 0.9647\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9482 - val_loss: 0.9526\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9553 - val_loss: 0.9632\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9546 - val_loss: 0.9585\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9549 - val_loss: 0.9606\n",
      "Top-2 accuracy = 0.84\n",
      "8\n",
      "standardizee|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0965 - val_loss: 1.0961\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Top-2 accuracy = 0.7\n",
      "9\n",
      "maxabss|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 1.0721 - val_loss: 1.0395\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0120 - val_loss: 0.9923\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9770 - val_loss: 0.9755\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9645 - val_loss: 0.9682\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9604 - val_loss: 0.9623\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9560 - val_loss: 0.9602\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9540 - val_loss: 0.9770\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9543 - val_loss: 0.9582\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9525 - val_loss: 0.9598\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9522 - val_loss: 0.9608\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9518 - val_loss: 0.9557\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9496 - val_loss: 0.9560\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9490 - val_loss: 0.9614\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9502 - val_loss: 0.9682\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9505 - val_loss: 0.9552\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9493 - val_loss: 0.9589\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9496 - val_loss: 0.9587\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9496 - val_loss: 0.9643\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9537 - val_loss: 0.9540\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9498 - val_loss: 0.9557\n",
      "Top-2 accuracy = 0.84\n",
      "10\n",
      "robuste|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 1.0883 - val_loss: 1.0746\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0560 - val_loss: 1.0437\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0166 - val_loss: 1.0046\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9900 - val_loss: 0.9861\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9710 - val_loss: 0.9934\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9681 - val_loss: 0.9687\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9568 - val_loss: 0.9596\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9543 - val_loss: 0.9651\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9538 - val_loss: 0.9575\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9521 - val_loss: 0.9579\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9502 - val_loss: 0.9584\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9525 - val_loss: 0.9588\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9507 - val_loss: 0.9572\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9487 - val_loss: 0.9588\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9493 - val_loss: 0.9591\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9492 - val_loss: 0.9542\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9483 - val_loss: 0.9547\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9487 - val_loss: 0.9544\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9477 - val_loss: 0.9533\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9487 - val_loss: 0.9529\n",
      "Top-2 accuracy = 0.85\n",
      "11\n",
      "standardizec|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0928 - val_loss: 1.0667\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9966 - val_loss: 0.9625\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9514 - val_loss: 0.9503\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9420 - val_loss: 0.9445\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9392 - val_loss: 0.9423\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9383 - val_loss: 0.9440\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9351 - val_loss: 0.9416\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9336 - val_loss: 0.9490\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9342 - val_loss: 0.9389\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9325 - val_loss: 0.9400\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9334 - val_loss: 0.9418\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9295 - val_loss: 0.9400\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9274 - val_loss: 0.9403\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9272 - val_loss: 0.9344\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9258 - val_loss: 0.9357\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9247 - val_loss: 0.9321\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9243 - val_loss: 0.9308\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9243 - val_loss: 0.9344\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9226 - val_loss: 0.9308\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9220 - val_loss: 0.9410\n",
      "Top-2 accuracy = 0.85\n",
      "12\n",
      "standardizeq|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0694 - val_loss: 1.0407\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0145 - val_loss: 0.9863\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9718 - val_loss: 0.9669\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9588 - val_loss: 0.9593\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9533 - val_loss: 0.9732\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9550 - val_loss: 0.9551\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9478 - val_loss: 0.9668\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9472 - val_loss: 0.9524\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9451 - val_loss: 0.9529\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9431 - val_loss: 0.9523\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9435 - val_loss: 0.9516\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9409 - val_loss: 0.9470\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9400 - val_loss: 0.9483\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9382 - val_loss: 0.9458\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9381 - val_loss: 0.9450\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9388 - val_loss: 0.9449\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9379 - val_loss: 0.9485\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9376 - val_loss: 0.9441\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9371 - val_loss: 0.9449\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9352 - val_loss: 0.9455\n",
      "Top-2 accuracy = 0.85\n",
      "13\n",
      "minmaxb|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 1.0963 - val_loss: 1.0960\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Top-2 accuracy = 0.7\n",
      "14\n",
      "standardizep|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0699 - val_loss: 1.0360\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0079 - val_loss: 0.9871\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9675 - val_loss: 0.9617\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9516 - val_loss: 0.9563\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9471 - val_loss: 0.9530\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9455 - val_loss: 0.9506\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9442 - val_loss: 0.9559\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9449 - val_loss: 0.9531\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9430 - val_loss: 0.9492\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9426 - val_loss: 0.9482\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9418 - val_loss: 0.9482\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9408 - val_loss: 0.9482\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9410 - val_loss: 0.9476\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9409 - val_loss: 0.9510\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9411 - val_loss: 0.9491\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9406 - val_loss: 0.9465\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9393 - val_loss: 0.9466\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9392 - val_loss: 0.9479\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9393 - val_loss: 0.9488\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9396 - val_loss: 0.9483\n",
      "Top-2 accuracy = 0.85\n",
      "15\n",
      "robustT|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 1.0968 - val_loss: 1.0963\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0954 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "16\n",
      "normalizeZ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 7ms/step - loss: 1.0399 - val_loss: 0.9783\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9555 - val_loss: 0.9517\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9455 - val_loss: 0.9615\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9416 - val_loss: 0.9494\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9407 - val_loss: 0.9436\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9368 - val_loss: 0.9458\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9364 - val_loss: 0.9463\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9344 - val_loss: 0.9407\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9332 - val_loss: 0.9456\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9335 - val_loss: 0.9421\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9319 - val_loss: 0.9406\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9340 - val_loss: 0.9403\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9327 - val_loss: 0.9383\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9303 - val_loss: 0.9393\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9303 - val_loss: 0.9381\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9299 - val_loss: 0.9852\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9399 - val_loss: 0.9363\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9267 - val_loss: 0.9515\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9311 - val_loss: 0.9375\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9292 - val_loss: 0.9425\n",
      "Top-2 accuracy = 0.85\n",
      "17\n",
      "standardizef|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 1.0963 - val_loss: 1.0962\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0963\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0963\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Top-2 accuracy = 0.7\n",
      "18\n",
      "maxabsf|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 1.0898 - val_loss: 1.0769\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0609 - val_loss: 1.0533\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0431 - val_loss: 1.0345\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0116 - val_loss: 1.0036\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9945 - val_loss: 0.9934\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9865 - val_loss: 0.9860\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9829 - val_loss: 0.9866\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9786 - val_loss: 0.9779\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9725 - val_loss: 0.9822\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9709 - val_loss: 0.9802\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9688 - val_loss: 0.9695\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9678 - val_loss: 0.9739\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9671 - val_loss: 0.9675\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9650 - val_loss: 0.9666\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9641 - val_loss: 0.9671\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9639 - val_loss: 0.9649\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9622 - val_loss: 0.9667\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9652 - val_loss: 0.9648\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9637 - val_loss: 0.9657\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9624 - val_loss: 0.9776\n",
      "Top-2 accuracy = 0.82\n",
      "19\n",
      "robustC|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 1.0214 - val_loss: 0.9540\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9393 - val_loss: 0.9443\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9333 - val_loss: 0.9490\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9326 - val_loss: 0.9410\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9280 - val_loss: 0.9398\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 0.9279 - val_loss: 0.9548\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9280 - val_loss: 0.9551\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9277 - val_loss: 0.9393\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9241 - val_loss: 0.9470\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9269 - val_loss: 0.9394\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9240 - val_loss: 0.9487\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 0.9239 - val_loss: 0.9410\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9210 - val_loss: 0.9382\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9218 - val_loss: 0.9431\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9231 - val_loss: 0.9445\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9238 - val_loss: 0.9470\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9231 - val_loss: 0.9389\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9205 - val_loss: 0.9527\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9220 - val_loss: 0.9401\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9206 - val_loss: 0.9411\n",
      "Top-2 accuracy = 0.85\n",
      "20\n",
      "maxabsK|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0963 - val_loss: 1.0885\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0736 - val_loss: 1.0567\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0322 - val_loss: 1.0157\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9976 - val_loss: 0.9835\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9725 - val_loss: 0.9729\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9631 - val_loss: 0.9683\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9591 - val_loss: 0.9672\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9588 - val_loss: 0.9606\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9547 - val_loss: 0.9588\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9529 - val_loss: 0.9598\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9512 - val_loss: 0.9574\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9500 - val_loss: 0.9560\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9492 - val_loss: 0.9576\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9477 - val_loss: 0.9532\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9461 - val_loss: 0.9533\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9454 - val_loss: 0.9546\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9455 - val_loss: 0.9544\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9448 - val_loss: 0.9506\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9429 - val_loss: 0.9492\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9418 - val_loss: 0.9510\n",
      "Top-2 accuracy = 0.84\n",
      "21\n",
      "maxabsh|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0391 - val_loss: 0.9822\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9616 - val_loss: 0.9588\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9526 - val_loss: 0.9655\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9527 - val_loss: 0.9526\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9476 - val_loss: 0.9520\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9462 - val_loss: 0.9508\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9456 - val_loss: 0.9508\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9448 - val_loss: 0.9484\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9441 - val_loss: 0.9574\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9462 - val_loss: 0.9481\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9435 - val_loss: 0.9508\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9422 - val_loss: 0.9462\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9417 - val_loss: 0.9518\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9442 - val_loss: 0.9456\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9420 - val_loss: 0.9508\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9404 - val_loss: 0.9487\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9399 - val_loss: 0.9455\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9401 - val_loss: 0.9445\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9396 - val_loss: 0.9507\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9399 - val_loss: 0.9607\n",
      "Top-2 accuracy = 0.84\n",
      "22\n",
      "standardizea|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Top-2 accuracy = 0.7\n",
      "23\n",
      "robustt|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 6ms/step - loss: 1.0989 - val_loss: 1.0962\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0952 - val_loss: 1.0964\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0955 - val_loss: 1.0962\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0953 - val_loss: 1.0962\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0953 - val_loss: 1.0968\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0953 - val_loss: 1.0960\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0968\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0953 - val_loss: 1.0959\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0951 - val_loss: 1.0964\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0633 - val_loss: 1.0440\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0247 - val_loss: 1.0066\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0162 - val_loss: 1.0170\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0040 - val_loss: 0.9884\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9790 - val_loss: 0.9856\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9751 - val_loss: 0.9730\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9706 - val_loss: 0.9663\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9676 - val_loss: 0.9662\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9603 - val_loss: 0.9661\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9598 - val_loss: 0.9577\n",
      "Top-2 accuracy = 0.84\n",
      "24\n",
      "minmaxM|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 1.0397 - val_loss: 0.9969\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9722 - val_loss: 0.9642\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9582 - val_loss: 0.9580\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9528 - val_loss: 0.9578\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9516 - val_loss: 0.9655\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9484 - val_loss: 0.9537\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9494 - val_loss: 0.9635\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9481 - val_loss: 0.9543\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9476 - val_loss: 0.9644\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9463 - val_loss: 0.9511\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9453 - val_loss: 0.9557\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9460 - val_loss: 0.9557\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9456 - val_loss: 0.9493\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9445 - val_loss: 0.9481\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9439 - val_loss: 0.9585\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9436 - val_loss: 0.9566\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9436 - val_loss: 0.9479\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9432 - val_loss: 0.9614\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9429 - val_loss: 0.9469\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9415 - val_loss: 0.9448\n",
      "Top-2 accuracy = 0.85\n",
      "25\n",
      "standardizei|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0795 - val_loss: 1.0478\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.0290 - val_loss: 1.0169\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.0059 - val_loss: 0.9985\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9908 - val_loss: 0.9889\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9799 - val_loss: 0.9786\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9723 - val_loss: 0.9788\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9680 - val_loss: 0.9712\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9656 - val_loss: 0.9672\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9638 - val_loss: 0.9637\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9640 - val_loss: 0.9613\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9594 - val_loss: 0.9604\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9587 - val_loss: 0.9651\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9589 - val_loss: 0.9601\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9583 - val_loss: 0.9600\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9570 - val_loss: 0.9602\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9565 - val_loss: 0.9585\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9557 - val_loss: 0.9566\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9549 - val_loss: 0.9584\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9556 - val_loss: 0.9583\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 0.9550 - val_loss: 0.9559\n",
      "Top-2 accuracy = 0.84\n",
      "26\n",
      "robustL|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 6ms/step - loss: 1.0116 - val_loss: 0.9597\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9531 - val_loss: 0.9514\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9464 - val_loss: 0.9496\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9414 - val_loss: 0.9521\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9428 - val_loss: 0.9555\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9433 - val_loss: 0.9613\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9396 - val_loss: 0.9518\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9392 - val_loss: 0.9497\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9389 - val_loss: 0.9442\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9402 - val_loss: 0.9583\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9394 - val_loss: 0.9477\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9365 - val_loss: 0.9418\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9354 - val_loss: 0.9503\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9375 - val_loss: 0.9431\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9343 - val_loss: 0.9431\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9354 - val_loss: 0.9448\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9355 - val_loss: 0.9460\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9370 - val_loss: 0.9508\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9359 - val_loss: 0.9427\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9345 - val_loss: 0.9467\n",
      "Top-2 accuracy = 0.85\n",
      "27\n",
      "standardizeU|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 1.0446 - val_loss: 1.0048\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9752 - val_loss: 0.9618\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9515 - val_loss: 0.9510\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9456 - val_loss: 0.9629\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9499 - val_loss: 0.9522\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9396 - val_loss: 0.9481\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9379 - val_loss: 0.9452\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9376 - val_loss: 0.9465\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9393 - val_loss: 0.9506\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9358 - val_loss: 0.9404\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9351 - val_loss: 0.9455\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9352 - val_loss: 0.9424\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9353 - val_loss: 0.9417\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9336 - val_loss: 0.9411\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9321 - val_loss: 0.9444\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9304 - val_loss: 0.9405\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9309 - val_loss: 0.9450\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9322 - val_loss: 0.9418\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 0.9319 - val_loss: 0.9411\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9319 - val_loss: 0.9403\n",
      "Top-2 accuracy = 0.85\n",
      "28\n",
      "minmaxF|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 10ms/step - loss: 1.0971 - val_loss: 1.0964\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0953 - val_loss: 1.0962\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.0952 - val_loss: 1.0962\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0952 - val_loss: 1.0960\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0951 - val_loss: 1.0962\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0952 - val_loss: 1.0961\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0951 - val_loss: 1.0960\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.0951 - val_loss: 1.0961\n",
      "Top-2 accuracy = 0.7\n",
      "29\n",
      "robustV|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 8ms/step - loss: 1.0632 - val_loss: 1.0234\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9977 - val_loss: 0.9869\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9738 - val_loss: 0.9980\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9716 - val_loss: 0.9750\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9661 - val_loss: 0.9763\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9644 - val_loss: 0.9696\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9617 - val_loss: 0.9668\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9593 - val_loss: 0.9652\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9595 - val_loss: 0.9749\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9604 - val_loss: 0.9656\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9584 - val_loss: 0.9657\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9578 - val_loss: 0.9623\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9561 - val_loss: 0.9633\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9571 - val_loss: 0.9712\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9569 - val_loss: 0.9646\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9569 - val_loss: 0.9634\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9562 - val_loss: 0.9626\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9547 - val_loss: 0.9670\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 0.9582 - val_loss: 0.9716\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 0.9572 - val_loss: 0.9612\n",
      "Top-2 accuracy = 0.84\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"n_runs\": 10,\n",
    "    \"transforms\": [\"normalize\", \"standardize\", \"robust\", \"maxabs\", \"minmax\"] * 30,\n",
    "    \"metrics\": [\"accuracy\", \"pd\", \"prec\"],\n",
    "    \"random\": True,\n",
    "    \"learners\": [MulticlassDL(n_classes=3, random={'n_layers': (2, 6), 'n_units': (3, 20)}, n_epochs=20)],\n",
    "    \"post_train_hooks\": [top2_hook],\n",
    "    \"log_path\": \"./\",\n",
    "    \"data\": [data],\n",
    "    \"name\": \"firefox-3class\"\n",
    "}\n",
    "for _ in range(50):\n",
    "    config[\"learners\"].append(\n",
    "        MulticlassDL(n_classes=3, random={'n_layers': (2, 6), 'n_units': (3, 20)}, n_epochs=20))\n",
    "\n",
    "dodge = DODGE(config)\n",
    "dodge.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(*train_test_split(x, y))\n",
    "data.y_train = np.where(data.y_train < 1, 0, np.where(data.y_train < 3, 1, np.where(data.y_train < 6, 2, np.where(data.y_train < 21, 3, 4))))\n",
    "data.y_test = np.where(data.y_test < 1, 0, np.where(data.y_test < 3, 1, np.where(data.y_test < 6, 2, np.where(data.y_test < 21, 3, 4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.y_train = to_categorical(data.y_train, num_classes=5)\n",
    "data.y_test = to_categorical(data.y_test, num_classes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x146d6b430>, 'loss': 'categorical_crossentropy', 'n_classes': 3, 'n_epochs': 20, 'n_layers': 4, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x146d6bb50>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 6, 'n_units': 15, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175500820>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 2, 'n_units': 18, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175500be0>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 16, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175500d60>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 5, 'n_units': 16, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175500f70>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 5, 'n_units': 18, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175500880>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 3, 'n_units': 12, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x17550a340>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 5, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x17550a490>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 2, 'n_units': 18, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x17550a730>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 2, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x17550aa30>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 3, 'n_units': 20, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x17550ab80>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 6, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x17550acd0>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 5, 'n_units': 16, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x17550aee0>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x17550ae20>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 5, 'n_units': 9, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1755141f0>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 3, 'n_units': 15, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175514400>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 16, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175514610>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 2, 'n_units': 4, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175514820>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 5, 'n_units': 9, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1755149d0>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 14, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175514b50>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 3, 'n_units': 14, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175514d60>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 11, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175514f70>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 2, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x17551d1c0>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 3, 'n_units': 6, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x17551d3d0>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 2, 'n_units': 11, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x17551d5e0>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 12, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x17551d7f0>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 2, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1754624c0>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 9, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175462280>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 2, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175462790>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 6, 'n_units': 9, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175462f10>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 8, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175462a90>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 3, 'n_units': 19, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175462850>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 6, 'n_units': 16, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175462c40>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 19, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x173f44670>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 19, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175451ca0>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 20, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175451b80>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175451670>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175451310>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 5, 'n_units': 15, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175451df0>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 6, 'n_units': 11, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175451fd0>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 12, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175451160>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 5, 'n_units': 4, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x17539f8e0>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 6, 'n_units': 9, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x17539f160>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 2, 'n_units': 19, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x17539f3a0>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x17539f580>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 3, 'n_units': 6, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1754401f0>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 11, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175440070>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 11, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1754403d0>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 4, 'n_units': 19, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175440b50>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 2, 'n_units': 17, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x175440280>, 'loss': 'categorical_crossentropy', 'n_classes': 5, 'n_epochs': 20, 'n_layers': 3, 'n_units': 9, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardizeR|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1642 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5612 - val_loss: 1.4906\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4605 - val_loss: 1.4434\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4249 - val_loss: 1.4131\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4008 - val_loss: 1.3943\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3864 - val_loss: 1.3833\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3769 - val_loss: 1.3771\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3706 - val_loss: 1.3724\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3656 - val_loss: 1.3678\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3629 - val_loss: 1.3649\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3597 - val_loss: 1.3626\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3573 - val_loss: 1.3610\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3552 - val_loss: 1.3591\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3537 - val_loss: 1.3576\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3521 - val_loss: 1.3562\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3508 - val_loss: 1.3553\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3492 - val_loss: 1.3562\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3482 - val_loss: 1.3541\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3472 - val_loss: 1.3532\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3461 - val_loss: 1.3514\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3452 - val_loss: 1.3521\n",
      "Top-2 accuracy = 0.661\n",
      "1\n",
      "minmaxG|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1645 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5693 - val_loss: 1.5418\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5151 - val_loss: 1.4985\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4808 - val_loss: 1.4707\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4578 - val_loss: 1.4501\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4414 - val_loss: 1.4350\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4291 - val_loss: 1.4232\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4201 - val_loss: 1.4149\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4123 - val_loss: 1.4079\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4066 - val_loss: 1.4033\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4014 - val_loss: 1.3982\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3959 - val_loss: 1.3936\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3914 - val_loss: 1.3901\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3867 - val_loss: 1.3861\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3828 - val_loss: 1.3834\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3798 - val_loss: 1.3808\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3771 - val_loss: 1.3790\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3746 - val_loss: 1.3783\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3736 - val_loss: 1.3766\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3721 - val_loss: 1.3760\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3710 - val_loss: 1.3755\n",
      "Top-2 accuracy = 0.645\n",
      "2\n",
      "robustj|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1648 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 5ms/step - loss: 1.6058 - val_loss: 1.5523\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5330 - val_loss: 1.5233\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5128 - val_loss: 1.5046\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4939 - val_loss: 1.4869\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4710 - val_loss: 1.4649\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4457 - val_loss: 1.4445\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4237 - val_loss: 1.4272\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4078 - val_loss: 1.4143\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3958 - val_loss: 1.4020\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3869 - val_loss: 1.3950\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3810 - val_loss: 1.3868\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3769 - val_loss: 1.3843\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3736 - val_loss: 1.3824\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3701 - val_loss: 1.3773\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3676 - val_loss: 1.3760\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3659 - val_loss: 1.3745\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3648 - val_loss: 1.3735\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3631 - val_loss: 1.3720\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3623 - val_loss: 1.3718\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3605 - val_loss: 1.3724\n",
      "Top-2 accuracy = 0.655\n",
      "3\n",
      "minmaxy|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1653 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5390 - val_loss: 1.4750\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4374 - val_loss: 1.4133\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3972 - val_loss: 1.3916\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3815 - val_loss: 1.3823\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3751 - val_loss: 1.3787\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3706 - val_loss: 1.3786\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3687 - val_loss: 1.3731\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3659 - val_loss: 1.3777\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3647 - val_loss: 1.3703\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3633 - val_loss: 1.3706\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3624 - val_loss: 1.3691\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3605 - val_loss: 1.3704\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3605 - val_loss: 1.3699\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3587 - val_loss: 1.3665\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3578 - val_loss: 1.3668\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3564 - val_loss: 1.3639\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3556 - val_loss: 1.3628\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3550 - val_loss: 1.3617\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3535 - val_loss: 1.3633\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3525 - val_loss: 1.3604\n",
      "Top-2 accuracy = 0.652\n",
      "4\n",
      "minmaxL|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1658 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5724 - val_loss: 1.5222\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4859 - val_loss: 1.4398\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4200 - val_loss: 1.4084\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4022 - val_loss: 1.3952\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3913 - val_loss: 1.3868\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3843 - val_loss: 1.3806\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3783 - val_loss: 1.3796\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3738 - val_loss: 1.3745\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3708 - val_loss: 1.3730\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3685 - val_loss: 1.3719\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3670 - val_loss: 1.3709\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3653 - val_loss: 1.3691\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3640 - val_loss: 1.3679\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3636 - val_loss: 1.3669\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3613 - val_loss: 1.3663\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3603 - val_loss: 1.3690\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3597 - val_loss: 1.3657\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3588 - val_loss: 1.3663\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3582 - val_loss: 1.3666\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3573 - val_loss: 1.3630\n",
      "Top-2 accuracy = 0.656\n",
      "5\n",
      "minmaxB|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1665 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5591 - val_loss: 1.5311\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5022 - val_loss: 1.4764\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4497 - val_loss: 1.4296\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4142 - val_loss: 1.4041\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3984 - val_loss: 1.3958\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3905 - val_loss: 1.3899\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3861 - val_loss: 1.3868\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3824 - val_loss: 1.3839\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3789 - val_loss: 1.3812\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3765 - val_loss: 1.3806\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3743 - val_loss: 1.3772\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3727 - val_loss: 1.3752\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3712 - val_loss: 1.3755\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3702 - val_loss: 1.3734\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3680 - val_loss: 1.3729\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3670 - val_loss: 1.3713\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3660 - val_loss: 1.3708\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3652 - val_loss: 1.3727\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3642 - val_loss: 1.3694\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3637 - val_loss: 1.3692\n",
      "Top-2 accuracy = 0.645\n",
      "6\n",
      "normalizeZ|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1670 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6026 - val_loss: 1.5726\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5533 - val_loss: 1.5465\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5294 - val_loss: 1.5233\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5091 - val_loss: 1.5063\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4930 - val_loss: 1.4890\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4755 - val_loss: 1.4708\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4569 - val_loss: 1.4500\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4381 - val_loss: 1.4299\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4220 - val_loss: 1.4148\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4100 - val_loss: 1.4037\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4010 - val_loss: 1.3966\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3939 - val_loss: 1.3911\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3880 - val_loss: 1.3886\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3850 - val_loss: 1.3874\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3828 - val_loss: 1.3858\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3806 - val_loss: 1.3854\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3798 - val_loss: 1.3843\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3787 - val_loss: 1.3832\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3778 - val_loss: 1.3820\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3770 - val_loss: 1.3818\n",
      "Top-2 accuracy = 0.637\n",
      "7\n",
      "normalizey|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1674 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5915 - val_loss: 1.5702\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5525 - val_loss: 1.5415\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5250 - val_loss: 1.5028\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4733 - val_loss: 1.4503\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4387 - val_loss: 1.4273\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4202 - val_loss: 1.4120\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4084 - val_loss: 1.4043\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4020 - val_loss: 1.4002\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3979 - val_loss: 1.3975\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3944 - val_loss: 1.3966\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3923 - val_loss: 1.3942\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3905 - val_loss: 1.3912\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3888 - val_loss: 1.3914\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3880 - val_loss: 1.3895\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3859 - val_loss: 1.3879\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3847 - val_loss: 1.3865\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3830 - val_loss: 1.3861\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3811 - val_loss: 1.3850\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3797 - val_loss: 1.3853\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3776 - val_loss: 1.3825\n",
      "Top-2 accuracy = 0.64\n",
      "8\n",
      "maxabsJ|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1679 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5945 - val_loss: 1.5798\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5669 - val_loss: 1.5543\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5410 - val_loss: 1.5304\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5216 - val_loss: 1.5165\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5084 - val_loss: 1.5035\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4924 - val_loss: 1.4842\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4731 - val_loss: 1.4682\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4589 - val_loss: 1.4567\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4478 - val_loss: 1.4474\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4384 - val_loss: 1.4396\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4311 - val_loss: 1.4326\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4246 - val_loss: 1.4270\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4194 - val_loss: 1.4225\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4145 - val_loss: 1.4208\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4108 - val_loss: 1.4139\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4070 - val_loss: 1.4111\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4042 - val_loss: 1.4081\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4022 - val_loss: 1.4053\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3991 - val_loss: 1.4043\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3975 - val_loss: 1.4023\n",
      "Top-2 accuracy = 0.629\n",
      "9\n",
      "robuste|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1685 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5719 - val_loss: 1.5179\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4657 - val_loss: 1.4325\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4128 - val_loss: 1.4026\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3938 - val_loss: 1.3925\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3851 - val_loss: 1.3849\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3797 - val_loss: 1.3815\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3760 - val_loss: 1.3772\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3721 - val_loss: 1.3740\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3692 - val_loss: 1.3708\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3667 - val_loss: 1.3689\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3642 - val_loss: 1.3669\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3623 - val_loss: 1.3652\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3612 - val_loss: 1.3648\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3602 - val_loss: 1.3639\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3579 - val_loss: 1.3626\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3576 - val_loss: 1.3610\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3557 - val_loss: 1.3609\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3557 - val_loss: 1.3594\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3548 - val_loss: 1.3605\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3545 - val_loss: 1.3580\n",
      "Top-2 accuracy = 0.661\n",
      "10\n",
      "maxabsT|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1691 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5689 - val_loss: 1.5382\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5175 - val_loss: 1.4972\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4787 - val_loss: 1.4622\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4479 - val_loss: 1.4369\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4266 - val_loss: 1.4201\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4102 - val_loss: 1.4074\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3987 - val_loss: 1.3974\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3899 - val_loss: 1.3906\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3841 - val_loss: 1.3862\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3798 - val_loss: 1.3824\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3769 - val_loss: 1.3798\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3749 - val_loss: 1.3781\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3735 - val_loss: 1.3766\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3724 - val_loss: 1.3756\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3713 - val_loss: 1.3747\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3705 - val_loss: 1.3750\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3698 - val_loss: 1.3738\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3688 - val_loss: 1.3731\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3683 - val_loss: 1.3723\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3679 - val_loss: 1.3717\n",
      "Top-2 accuracy = 0.642\n",
      "11\n",
      "maxabsz|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1694 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5587 - val_loss: 1.5152\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4756 - val_loss: 1.4459\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4197 - val_loss: 1.4040\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3917 - val_loss: 1.3876\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3802 - val_loss: 1.3807\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3750 - val_loss: 1.3776\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3713 - val_loss: 1.3746\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3689 - val_loss: 1.3750\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3676 - val_loss: 1.3714\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3658 - val_loss: 1.3711\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3648 - val_loss: 1.3708\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3636 - val_loss: 1.3681\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3632 - val_loss: 1.3680\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3624 - val_loss: 1.3668\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3606 - val_loss: 1.3661\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3597 - val_loss: 1.3674\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3594 - val_loss: 1.3647\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3583 - val_loss: 1.3639\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3578 - val_loss: 1.3639\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3581 - val_loss: 1.3627\n",
      "Top-2 accuracy = 0.653\n",
      "12\n",
      "minmaxh|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1699 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5546 - val_loss: 1.5193\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4853 - val_loss: 1.4530\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4291 - val_loss: 1.4092\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3997 - val_loss: 1.3919\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3876 - val_loss: 1.3841\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3809 - val_loss: 1.3841\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3772 - val_loss: 1.3794\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3741 - val_loss: 1.3780\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3721 - val_loss: 1.3739\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3707 - val_loss: 1.3743\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3696 - val_loss: 1.3718\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3687 - val_loss: 1.3712\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3666 - val_loss: 1.3701\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3663 - val_loss: 1.3712\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3655 - val_loss: 1.3688\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3645 - val_loss: 1.3727\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3640 - val_loss: 1.3665\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3625 - val_loss: 1.3663\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3620 - val_loss: 1.3667\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3609 - val_loss: 1.3651\n",
      "Top-2 accuracy = 0.647\n",
      "13\n",
      "minmaxb|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5394 - val_loss: 1.5072\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4567 - val_loss: 1.4221\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4041 - val_loss: 1.4030\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3903 - val_loss: 1.3911\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3849 - val_loss: 1.3857\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3817 - val_loss: 1.3846\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3798 - val_loss: 1.3823\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3772 - val_loss: 1.3772\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3741 - val_loss: 1.3743\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3707 - val_loss: 1.3721\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3692 - val_loss: 1.3700\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3670 - val_loss: 1.3717\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3652 - val_loss: 1.3682\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3631 - val_loss: 1.3656\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3621 - val_loss: 1.3644\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3605 - val_loss: 1.3654\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3589 - val_loss: 1.3616\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3573 - val_loss: 1.3613\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3564 - val_loss: 1.3631\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3553 - val_loss: 1.3606\n",
      "Top-2 accuracy = 0.653\n",
      "14\n",
      "standardizen|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1707 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5319 - val_loss: 1.4675\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4234 - val_loss: 1.3893\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3805 - val_loss: 1.3749\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3697 - val_loss: 1.3673\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3622 - val_loss: 1.3635\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3582 - val_loss: 1.3602\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3538 - val_loss: 1.3578\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3515 - val_loss: 1.3574\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3495 - val_loss: 1.3545\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3477 - val_loss: 1.3543\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3466 - val_loss: 1.3524\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3441 - val_loss: 1.3508\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3429 - val_loss: 1.3508\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3423 - val_loss: 1.3511\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3408 - val_loss: 1.3477\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3397 - val_loss: 1.3489\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3398 - val_loss: 1.3491\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3387 - val_loss: 1.3472\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3376 - val_loss: 1.3477\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3363 - val_loss: 1.3460\n",
      "Top-2 accuracy = 0.669\n",
      "15\n",
      "minmaxF|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1712 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5426 - val_loss: 1.4922\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4495 - val_loss: 1.4228\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4097 - val_loss: 1.3986\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3914 - val_loss: 1.3864\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3797 - val_loss: 1.3827\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3752 - val_loss: 1.3759\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3704 - val_loss: 1.3727\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3690 - val_loss: 1.3723\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 28s 342ms/step - loss: 1.3661 - val_loss: 1.3717\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3650 - val_loss: 1.3688\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3637 - val_loss: 1.3664\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3624 - val_loss: 1.3663\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3606 - val_loss: 1.3645\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3599 - val_loss: 1.3653\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3598 - val_loss: 1.3624\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3579 - val_loss: 1.3617\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3567 - val_loss: 1.3617\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3557 - val_loss: 1.3629\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3564 - val_loss: 1.3603\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3549 - val_loss: 1.3592\n",
      "Top-2 accuracy = 0.652\n",
      "16\n",
      "robustQ|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5729 - val_loss: 1.5222\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4758 - val_loss: 1.4509\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4198 - val_loss: 1.4143\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3984 - val_loss: 1.3993\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3858 - val_loss: 1.3887\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3787 - val_loss: 1.3824\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3752 - val_loss: 1.3786\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3733 - val_loss: 1.3771\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3710 - val_loss: 1.3744\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3702 - val_loss: 1.3760\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3697 - val_loss: 1.3754\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3708 - val_loss: 1.3720\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3685 - val_loss: 1.3723\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3683 - val_loss: 1.3724\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3677 - val_loss: 1.3747\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3683 - val_loss: 1.3698\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3671 - val_loss: 1.3701\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3667 - val_loss: 1.3702\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3674 - val_loss: 1.3702\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3667 - val_loss: 1.3695\n",
      "Top-2 accuracy = 0.657\n",
      "17\n",
      "standardizev|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1722 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5569 - val_loss: 1.5018\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4663 - val_loss: 1.4353\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4127 - val_loss: 1.3996\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3859 - val_loss: 1.3838\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3755 - val_loss: 1.3749\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3700 - val_loss: 1.3730\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3670 - val_loss: 1.3708\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3651 - val_loss: 1.3691\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3621 - val_loss: 1.3662\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3608 - val_loss: 1.3651\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3594 - val_loss: 1.3642\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3585 - val_loss: 1.3630\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3576 - val_loss: 1.3622\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3558 - val_loss: 1.3607\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3552 - val_loss: 1.3602\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3538 - val_loss: 1.3594\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3531 - val_loss: 1.3582\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3520 - val_loss: 1.3580\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3514 - val_loss: 1.3556\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3506 - val_loss: 1.3568\n",
      "Top-2 accuracy = 0.658\n",
      "18\n",
      "maxabsw|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1727 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5584 - val_loss: 1.5190\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4829 - val_loss: 1.4518\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4396 - val_loss: 1.4271\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4194 - val_loss: 1.4129\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4079 - val_loss: 1.4047\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4002 - val_loss: 1.3976\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3930 - val_loss: 1.3914\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3875 - val_loss: 1.3869\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3820 - val_loss: 1.3819\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3786 - val_loss: 1.3787\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3752 - val_loss: 1.3763\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3726 - val_loss: 1.3748\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3710 - val_loss: 1.3737\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3695 - val_loss: 1.3718\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3681 - val_loss: 1.3715\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3670 - val_loss: 1.3705\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3658 - val_loss: 1.3685\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3650 - val_loss: 1.3679\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3640 - val_loss: 1.3672\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3631 - val_loss: 1.3685\n",
      "Top-2 accuracy = 0.649\n",
      "19\n",
      "robustE|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1731 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5313 - val_loss: 1.5057\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4638 - val_loss: 1.4243\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4014 - val_loss: 1.3938\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3791 - val_loss: 1.3783\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3668 - val_loss: 1.3691\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3601 - val_loss: 1.3648\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3542 - val_loss: 1.3629\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3512 - val_loss: 1.3570\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3480 - val_loss: 1.3545\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3456 - val_loss: 1.3527\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3437 - val_loss: 1.3509\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3414 - val_loss: 1.3515\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3407 - val_loss: 1.3489\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3394 - val_loss: 1.3505\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3382 - val_loss: 1.3488\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3377 - val_loss: 1.3487\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3361 - val_loss: 1.3466\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3359 - val_loss: 1.3452\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3347 - val_loss: 1.3452\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3349 - val_loss: 1.3471\n",
      "Top-2 accuracy = 0.665\n",
      "20\n",
      "maxabsa|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1737 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5932 - val_loss: 1.5793\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5639 - val_loss: 1.5561\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5433 - val_loss: 1.5393\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5294 - val_loss: 1.5293\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5209 - val_loss: 1.5224\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5140 - val_loss: 1.5159\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5066 - val_loss: 1.5088\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4981 - val_loss: 1.5007\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4886 - val_loss: 1.4913\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4785 - val_loss: 1.4814\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4680 - val_loss: 1.4714\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4576 - val_loss: 1.4604\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4462 - val_loss: 1.4481\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4335 - val_loss: 1.4349\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4211 - val_loss: 1.4221\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4104 - val_loss: 1.4123\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4018 - val_loss: 1.4038\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3950 - val_loss: 1.3978\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3902 - val_loss: 1.3936\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3863 - val_loss: 1.3901\n",
      "Top-2 accuracy = 0.637\n",
      "21\n",
      "robustn|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1740 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.8574 - val_loss: 1.5329\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4863 - val_loss: 1.4684\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4473 - val_loss: 1.4461\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4270 - val_loss: 1.4291\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4113 - val_loss: 1.4170\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4015 - val_loss: 1.4099\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3950 - val_loss: 1.4014\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3890 - val_loss: 1.3970\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3848 - val_loss: 1.3989\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3804 - val_loss: 1.3919\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3766 - val_loss: 1.3855\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3730 - val_loss: 1.3807\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3701 - val_loss: 1.3807\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3685 - val_loss: 1.3758\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3635 - val_loss: 1.3753\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3615 - val_loss: 1.3751\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3596 - val_loss: 1.3711\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3572 - val_loss: 1.3693\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3554 - val_loss: 1.3685\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3544 - val_loss: 1.3721\n",
      "Top-2 accuracy = 0.658\n",
      "22\n",
      "robustU|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1745 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 3.4953 - val_loss: 2.1705\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.8747 - val_loss: 1.7006\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6164 - val_loss: 1.5771\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5402 - val_loss: 1.5318\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5095 - val_loss: 1.5095\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4910 - val_loss: 1.4926\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4777 - val_loss: 1.4816\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4671 - val_loss: 1.4718\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4574 - val_loss: 1.4623\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4505 - val_loss: 1.4571\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4434 - val_loss: 1.4522\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4379 - val_loss: 1.4428\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4301 - val_loss: 1.4374\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4245 - val_loss: 1.4325\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4194 - val_loss: 1.4265\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4156 - val_loss: 1.4227\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4105 - val_loss: 1.4183\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4057 - val_loss: 1.4149\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4016 - val_loss: 1.4099\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3982 - val_loss: 1.4062\n",
      "Top-2 accuracy = 0.641\n",
      "23\n",
      "normalizeL|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5797 - val_loss: 1.5322\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.4879 - val_loss: 1.4579\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4293 - val_loss: 1.4348\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4109 - val_loss: 1.4135\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3991 - val_loss: 1.4072\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3952 - val_loss: 1.3997\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3898 - val_loss: 1.3974\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3866 - val_loss: 1.3973\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3862 - val_loss: 1.3898\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3833 - val_loss: 1.3877\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3812 - val_loss: 1.3908\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3802 - val_loss: 1.3876\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3790 - val_loss: 1.3843\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3780 - val_loss: 1.3830\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3765 - val_loss: 1.3813\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3757 - val_loss: 1.3809\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3751 - val_loss: 1.3785\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3752 - val_loss: 1.3808\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3747 - val_loss: 1.3864\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3736 - val_loss: 1.3797\n",
      "Top-2 accuracy = 0.652\n",
      "24\n",
      "normalizeq|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5604 - val_loss: 1.5335\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5154 - val_loss: 1.5017\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4697 - val_loss: 1.4546\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4234 - val_loss: 1.4142\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3966 - val_loss: 1.4006\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3874 - val_loss: 1.3939\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3809 - val_loss: 1.3849\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3774 - val_loss: 1.3818\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3731 - val_loss: 1.3816\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3711 - val_loss: 1.3768\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3676 - val_loss: 1.3741\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3651 - val_loss: 1.3716\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3635 - val_loss: 1.3681\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3621 - val_loss: 1.3706\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3608 - val_loss: 1.3738\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3599 - val_loss: 1.3662\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3578 - val_loss: 1.3683\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3590 - val_loss: 1.3655\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3571 - val_loss: 1.3626\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3577 - val_loss: 1.3633\n",
      "Top-2 accuracy = 0.662\n",
      "25\n",
      "standardizer|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1756 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5716 - val_loss: 1.5505\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5353 - val_loss: 1.5259\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5017 - val_loss: 1.4800\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4343 - val_loss: 1.4049\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3902 - val_loss: 1.3866\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3771 - val_loss: 1.3815\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3724 - val_loss: 1.3791\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3708 - val_loss: 1.3763\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3685 - val_loss: 1.3753\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3669 - val_loss: 1.3751\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3657 - val_loss: 1.3744\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3642 - val_loss: 1.3729\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3627 - val_loss: 1.3703\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3621 - val_loss: 1.3698\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3610 - val_loss: 1.3711\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3605 - val_loss: 1.3667\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3589 - val_loss: 1.3665\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3577 - val_loss: 1.3659\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3567 - val_loss: 1.3652\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3560 - val_loss: 1.3643\n",
      "Top-2 accuracy = 0.654\n",
      "26\n",
      "normalizek|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1763 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5510 - val_loss: 1.5183\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4988 - val_loss: 1.4811\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4507 - val_loss: 1.4306\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4066 - val_loss: 1.4010\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3859 - val_loss: 1.3894\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3773 - val_loss: 1.3829\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3727 - val_loss: 1.3788\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3693 - val_loss: 1.3766\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3661 - val_loss: 1.3730\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3634 - val_loss: 1.3706\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3611 - val_loss: 1.3689\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3591 - val_loss: 1.3661\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3573 - val_loss: 1.3660\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3563 - val_loss: 1.3642\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3547 - val_loss: 1.3634\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3537 - val_loss: 1.3603\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3520 - val_loss: 1.3602\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3512 - val_loss: 1.3595\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3500 - val_loss: 1.3583\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3496 - val_loss: 1.3586\n",
      "Top-2 accuracy = 0.663\n",
      "27\n",
      "robustM|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5554 - val_loss: 1.5146\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4864 - val_loss: 1.4624\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4434 - val_loss: 1.4349\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4207 - val_loss: 1.4204\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4059 - val_loss: 1.4067\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3957 - val_loss: 1.3992\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3903 - val_loss: 1.3957\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3857 - val_loss: 1.3904\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3836 - val_loss: 1.3890\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3813 - val_loss: 1.3862\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3811 - val_loss: 1.3853\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3790 - val_loss: 1.3850\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3781 - val_loss: 1.3830\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3775 - val_loss: 1.3848\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3785 - val_loss: 1.3824\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3754 - val_loss: 1.3808\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3746 - val_loss: 1.3810\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3740 - val_loss: 1.3799\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3728 - val_loss: 1.3792\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3715 - val_loss: 1.3818\n",
      "Top-2 accuracy = 0.648\n",
      "28\n",
      "robustH|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 1s 9ms/step - loss: 1.5494 - val_loss: 1.5158\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4816 - val_loss: 1.4479\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4272 - val_loss: 1.4110\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4034 - val_loss: 1.3995\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3941 - val_loss: 1.3927\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3863 - val_loss: 1.3856\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3775 - val_loss: 1.3786\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3707 - val_loss: 1.3750\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3667 - val_loss: 1.3731\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3629 - val_loss: 1.3714\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3614 - val_loss: 1.3694\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3606 - val_loss: 1.3721\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3610 - val_loss: 1.3678\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3586 - val_loss: 1.3671\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3570 - val_loss: 1.3673\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3568 - val_loss: 1.3647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3553 - val_loss: 1.3638\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3554 - val_loss: 1.3638\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3554 - val_loss: 1.3649\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3553 - val_loss: 1.3650\n",
      "Top-2 accuracy = 0.658\n",
      "29\n",
      "minmaxo|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5627 - val_loss: 1.5438\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5304 - val_loss: 1.5315\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5199 - val_loss: 1.5242\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5096 - val_loss: 1.5053\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4752 - val_loss: 1.4481\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4150 - val_loss: 1.4055\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3982 - val_loss: 1.3989\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3931 - val_loss: 1.4074\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3944 - val_loss: 1.3992\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3909 - val_loss: 1.3942\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3881 - val_loss: 1.3946\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3885 - val_loss: 1.3944\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3856 - val_loss: 1.3892\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3847 - val_loss: 1.3894\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3816 - val_loss: 1.3837\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3798 - val_loss: 1.3824\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3773 - val_loss: 1.3788\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3758 - val_loss: 1.3776\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3730 - val_loss: 1.3756\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3722 - val_loss: 1.3726\n",
      "Top-2 accuracy = 0.649\n",
      "0\n",
      "robuste|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1783 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5424 - val_loss: 1.4927\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4442 - val_loss: 1.4128\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3940 - val_loss: 1.3897\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3757 - val_loss: 1.3805\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3690 - val_loss: 1.3744\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3634 - val_loss: 1.3717\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3600 - val_loss: 1.3672\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3566 - val_loss: 1.3656\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3557 - val_loss: 1.3636\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3546 - val_loss: 1.3605\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3511 - val_loss: 1.3607\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3499 - val_loss: 1.3582\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3494 - val_loss: 1.3629\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3486 - val_loss: 1.3611\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3477 - val_loss: 1.3613\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3466 - val_loss: 1.3597\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3461 - val_loss: 1.3587\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3470 - val_loss: 1.3574\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3449 - val_loss: 1.3602\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3439 - val_loss: 1.3547\n",
      "Top-2 accuracy = 0.661\n",
      "1\n",
      "robusti|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5718 - val_loss: 1.5334\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5026 - val_loss: 1.4686\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4328 - val_loss: 1.4179\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4065 - val_loss: 1.4036\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3946 - val_loss: 1.3920\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3864 - val_loss: 1.3863\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3814 - val_loss: 1.3841\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3777 - val_loss: 1.3810\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3744 - val_loss: 1.3784\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3722 - val_loss: 1.3776\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3696 - val_loss: 1.3753\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3675 - val_loss: 1.3736\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3660 - val_loss: 1.3730\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3651 - val_loss: 1.3719\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3638 - val_loss: 1.3715\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3627 - val_loss: 1.3725\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3622 - val_loss: 1.3723\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3614 - val_loss: 1.3710\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3609 - val_loss: 1.3714\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3603 - val_loss: 1.3699\n",
      "Top-2 accuracy = 0.655\n",
      "2\n",
      "minmaxK|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1794 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.6049 - val_loss: 1.5443\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5319 - val_loss: 1.5238\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5122 - val_loss: 1.5028\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4886 - val_loss: 1.4792\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4665 - val_loss: 1.4595\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4483 - val_loss: 1.4446\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4336 - val_loss: 1.4325\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4226 - val_loss: 1.4249\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4156 - val_loss: 1.4172\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4075 - val_loss: 1.4111\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4019 - val_loss: 1.4052\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3967 - val_loss: 1.4009\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3925 - val_loss: 1.3973\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3896 - val_loss: 1.3945\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3863 - val_loss: 1.3929\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3839 - val_loss: 1.3881\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3813 - val_loss: 1.3858\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3790 - val_loss: 1.3854\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3781 - val_loss: 1.3844\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3766 - val_loss: 1.3819\n",
      "Top-2 accuracy = 0.65\n",
      "3\n",
      "maxabss|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1799 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5589 - val_loss: 1.5314\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5085 - val_loss: 1.4850\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4553 - val_loss: 1.4367\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4189 - val_loss: 1.4161\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4035 - val_loss: 1.4048\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3958 - val_loss: 1.3999\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3914 - val_loss: 1.3958\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3856 - val_loss: 1.3927\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3819 - val_loss: 1.3876\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3787 - val_loss: 1.3864\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3763 - val_loss: 1.3883\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3752 - val_loss: 1.3791\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3715 - val_loss: 1.3806\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3701 - val_loss: 1.3775\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3689 - val_loss: 1.3786\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3679 - val_loss: 1.3738\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3663 - val_loss: 1.3716\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3643 - val_loss: 1.3705\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3641 - val_loss: 1.3780\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3639 - val_loss: 1.3690\n",
      "Top-2 accuracy = 0.655\n",
      "4\n",
      "minmaxr|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5086 - val_loss: 1.4319\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4049 - val_loss: 1.4023\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3858 - val_loss: 1.3895\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3778 - val_loss: 1.3893\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3728 - val_loss: 1.3774\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3685 - val_loss: 1.3818\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3654 - val_loss: 1.3708\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3643 - val_loss: 1.3774\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3604 - val_loss: 1.3692\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3595 - val_loss: 1.3671\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3579 - val_loss: 1.3670\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3570 - val_loss: 1.3680\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3559 - val_loss: 1.3640\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3552 - val_loss: 1.3619\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3552 - val_loss: 1.3618\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3527 - val_loss: 1.3619\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3529 - val_loss: 1.3664\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3526 - val_loss: 1.3612\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3511 - val_loss: 1.3611\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3509 - val_loss: 1.3628\n",
      "Top-2 accuracy = 0.657\n",
      "5\n",
      "maxabsT|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1809 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5768 - val_loss: 1.5211\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4804 - val_loss: 1.4354\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4067 - val_loss: 1.3972\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3853 - val_loss: 1.3871\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3797 - val_loss: 1.3874\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3751 - val_loss: 1.3787\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3713 - val_loss: 1.3769\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3702 - val_loss: 1.3751\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3667 - val_loss: 1.3729\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3649 - val_loss: 1.3742\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3627 - val_loss: 1.3705\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3629 - val_loss: 1.3715\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3607 - val_loss: 1.3684\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3586 - val_loss: 1.3676\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3572 - val_loss: 1.3730\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3576 - val_loss: 1.3651\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3554 - val_loss: 1.3637\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3551 - val_loss: 1.3700\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3571 - val_loss: 1.3645\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3536 - val_loss: 1.3601\n",
      "Top-2 accuracy = 0.659\n",
      "6\n",
      "robustB|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1815 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7317 - val_loss: 1.5618\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5084 - val_loss: 1.4673\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.4445 - val_loss: 1.4351\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4133 - val_loss: 1.4099\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3946 - val_loss: 1.3958\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3828 - val_loss: 1.3867\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3752 - val_loss: 1.3811\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3708 - val_loss: 1.3746\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3665 - val_loss: 1.3726\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3634 - val_loss: 1.3688\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3608 - val_loss: 1.3660\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3588 - val_loss: 1.3646\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3579 - val_loss: 1.3635\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3548 - val_loss: 1.3620\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3542 - val_loss: 1.3592\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3527 - val_loss: 1.3588\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3516 - val_loss: 1.3617\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3512 - val_loss: 1.3589\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3500 - val_loss: 1.3571\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3482 - val_loss: 1.3583\n",
      "Top-2 accuracy = 0.659\n",
      "7\n",
      "robusta|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1821 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5386 - val_loss: 1.4807\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4531 - val_loss: 1.4320\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.4130 - val_loss: 1.4028\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.3940 - val_loss: 1.3917\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3848 - val_loss: 1.3853\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3792 - val_loss: 1.3809\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3757 - val_loss: 1.3798\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3725 - val_loss: 1.3772\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3698 - val_loss: 1.3743\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3677 - val_loss: 1.3741\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3655 - val_loss: 1.3720\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3641 - val_loss: 1.3712\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3625 - val_loss: 1.3714\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3613 - val_loss: 1.3709\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3600 - val_loss: 1.3700\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3594 - val_loss: 1.3711\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3588 - val_loss: 1.3697\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3570 - val_loss: 1.3667\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.3559 - val_loss: 1.3668\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.3552 - val_loss: 1.3673\n",
      "Top-2 accuracy = 0.656\n",
      "8\n",
      "maxabsh|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_1826 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:532 train_step  **\n        loss = self.compiled_loss(\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1527 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4561 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:1117 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 5) and (None, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-11ca9da4c000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdodge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDODGE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mdodge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/raise_utils/hyperparams/dodge.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m                     model.set_data(data.x_train, data.y_train,\n\u001b[1;32m     74\u001b[0m                                    data.x_test, data.y_test)\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0;31m# Run post-training hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/raise_utils/learners/multiclassdl.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         self.model.fit(np.array(self.x_train), np.array(self.y_train), batch_size=512, epochs=self.n_epochs,\n\u001b[0m\u001b[1;32m     74\u001b[0m                        validation_split=0.2, verbose=self.verbose, callbacks=[\n\u001b[1;32m     75\u001b[0m             \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 505\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    506\u001b[0m             *args, **kwds))\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2657\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:571 train_function  *\n        outputs = self.distribute_strategy.run(\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:532 train_step  **\n        loss = self.compiled_loss(\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py:205 __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:143 __call__\n        losses = self.call(y_true, y_pred)\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:246 call\n        return self.fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/keras/losses.py:1527 categorical_crossentropy\n        return K.categorical_crossentropy(y_true, y_pred, from_logits=from_logits)\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/keras/backend.py:4561 categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n    /usr/local/lib/python3.8/site-packages/tensorflow/python/framework/tensor_shape.py:1117 assert_is_compatible_with\n        raise ValueError(\"Shapes %s and %s are incompatible\" % (self, other))\n\n    ValueError: Shapes (None, 5) and (None, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"n_runs\": 1,\n",
    "    \"transforms\": [\"normalize\", \"standardize\", \"robust\", \"maxabs\", \"minmax\"] * 30,\n",
    "    \"metrics\": [\"accuracy\", \"pd\", \"prec\"],\n",
    "    \"random\": True,\n",
    "    \"learners\": [MulticlassDL(n_classes=3, random={'n_layers': (2, 6), 'n_units': (3, 20)}, n_epochs=20)],\n",
    "    \"post_train_hooks\": [top2_hook],\n",
    "    \"log_path\": \"./\",\n",
    "    \"data\": [data],\n",
    "    \"name\": \"firefox-5class\"\n",
    "}\n",
    "for _ in range(50):\n",
    "    config[\"learners\"].append(\n",
    "        MulticlassDL(n_classes=5, random={'n_layers': (2, 6), 'n_units': (3, 20)}, n_epochs=20))\n",
    "\n",
    "dodge = DODGE(config)\n",
    "dodge.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(*train_test_split(x, y))\n",
    "data.y_train = np.where(data.y_train < 1, 0, np.where(data.y_train < 2, 1, np.where(data.y_train < 6, 2, np.where(data.y_train < 11, 3, np.where(data.y_train < 21, 4, np.where(data.y_train < 101, 5, 6))))))\n",
    "data.y_test = np.where(data.y_test < 1, 0, np.where(data.y_test < 2, 1, np.where(data.y_test < 6, 2, np.where(data.y_test < 11, 3, np.where(data.y_test < 21, 4, np.where(data.y_test < 101, 5, 6))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.y_train = to_categorical(data.y_train, num_classes=7)\n",
    "data.y_test = to_categorical(data.y_test, num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147785c10>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 2, 'n_units': 12, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147785b80>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 2, 'n_units': 20, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147785e80>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 2, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c3b070>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 2, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c3b310>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 4, 'n_units': 12, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c3b4f0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 4, 'n_units': 9, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147785970>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 6, 'n_units': 5, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c3ba00>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 2, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c3bca0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 4, 'n_units': 11, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c3bf40>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 3, 'n_units': 9, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c3d220>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 2, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c3d4f0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 5, 'n_units': 12, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c3d790>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 3, 'n_units': 15, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c3da00>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 4, 'n_units': 16, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c3dca0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 2, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c3dfa0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 3, 'n_units': 20, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c441c0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 6, 'n_units': 17, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c444f0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 4, 'n_units': 12, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c44730>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 2, 'n_units': 14, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c449d0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 5, 'n_units': 4, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c44c70>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 3, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c44f10>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 6, 'n_units': 8, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c491f0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 2, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c49490>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 4, 'n_units': 11, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c49730>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 3, 'n_units': 8, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c499d0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 5, 'n_units': 5, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c49c70>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 6, 'n_units': 11, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c49f10>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 2, 'n_units': 14, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c511f0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 5, 'n_units': 14, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c51490>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 2, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c51730>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 2, 'n_units': 16, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c519d0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 2, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c51c70>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 2, 'n_units': 14, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c51f70>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 5, 'n_units': 16, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c56190>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 4, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c56430>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 3, 'n_units': 9, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c566d0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 3, 'n_units': 5, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c569d0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 4, 'n_units': 14, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c56bb0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 4, 'n_units': 8, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c56e50>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 6, 'n_units': 4, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c5c130>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 2, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c5c3d0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 6, 'n_units': 4, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c5c670>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 3, 'n_units': 9, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c5c910>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 5, 'n_units': 4, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c5cbb0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 2, 'n_units': 5, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c5ce50>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 6, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c63130>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 2, 'n_units': 8, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c633d0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 4, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c63670>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 3, 'n_units': 5, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c63910>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 3, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c63bb0>, 'loss': 'categorical_crossentropy', 'n_classes': 7, 'n_epochs': 20, 'n_layers': 6, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maxabso|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.8759 - val_loss: 1.8370\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7960 - val_loss: 1.7797\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7474 - val_loss: 1.7390\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7098 - val_loss: 1.7029\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6755 - val_loss: 1.6686\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6454 - val_loss: 1.6399\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6206 - val_loss: 1.6167\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6003 - val_loss: 1.5976\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5844 - val_loss: 1.5834\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5724 - val_loss: 1.5723\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5637 - val_loss: 1.5652\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5569 - val_loss: 1.5590\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5512 - val_loss: 1.5540\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5468 - val_loss: 1.5500\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5437 - val_loss: 1.5466\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5408 - val_loss: 1.5445\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5386 - val_loss: 1.5431\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5370 - val_loss: 1.5399\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5352 - val_loss: 1.5385\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5339 - val_loss: 1.5373\n",
      "Top-2 accuracy = 0.578\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.8/site-packages/raise_utils/learners/multiclassdl.py:84: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "1\n",
      "normalizeD|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9051 - val_loss: 1.8535\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.8005 - val_loss: 1.7644\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7344 - val_loss: 1.7256\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7061 - val_loss: 1.7044\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6868 - val_loss: 1.6863\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6685 - val_loss: 1.6675\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6493 - val_loss: 1.6485\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6307 - val_loss: 1.6308\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6148 - val_loss: 1.6170\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6028 - val_loss: 1.6069\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5947 - val_loss: 1.5995\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5888 - val_loss: 1.5936\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5836 - val_loss: 1.5886\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5787 - val_loss: 1.5841\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5740 - val_loss: 1.5786\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5696 - val_loss: 1.5745\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5654 - val_loss: 1.5703\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5619 - val_loss: 1.5666\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5588 - val_loss: 1.5638\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5562 - val_loss: 1.5610\n",
      "Top-2 accuracy = 0.571\n",
      "2\n",
      "maxabsz|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_13 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7743 - val_loss: 1.7040\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6675 - val_loss: 1.6471\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6195 - val_loss: 1.6118\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5925 - val_loss: 1.5926\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5729 - val_loss: 1.5732\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5579 - val_loss: 1.5607\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5466 - val_loss: 1.5490\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5373 - val_loss: 1.5404\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5296 - val_loss: 1.5337\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5245 - val_loss: 1.5294\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5203 - val_loss: 1.5250\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5180 - val_loss: 1.5222\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5158 - val_loss: 1.5222\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5147 - val_loss: 1.5185\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5130 - val_loss: 1.5168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5118 - val_loss: 1.5158\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5107 - val_loss: 1.5162\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5102 - val_loss: 1.5137\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5092 - val_loss: 1.5134\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5088 - val_loss: 1.5133\n",
      "Top-2 accuracy = 0.593\n",
      "3\n",
      "minmaxc|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.8392 - val_loss: 1.7431\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6646 - val_loss: 1.6127\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5725 - val_loss: 1.5551\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5396 - val_loss: 1.5366\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5289 - val_loss: 1.5309\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5251 - val_loss: 1.5269\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5227 - val_loss: 1.5251\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5209 - val_loss: 1.5242\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5199 - val_loss: 1.5224\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5192 - val_loss: 1.5209\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5170 - val_loss: 1.5209\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5162 - val_loss: 1.5187\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5143 - val_loss: 1.5174\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5133 - val_loss: 1.5197\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5128 - val_loss: 1.5160\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5108 - val_loss: 1.5132\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5091 - val_loss: 1.5128\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5086 - val_loss: 1.5116\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5066 - val_loss: 1.5124\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5063 - val_loss: 1.5100\n",
      "Top-2 accuracy = 0.599\n",
      "4\n",
      "standardized|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_19 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8570 - val_loss: 1.7511\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6893 - val_loss: 1.6484\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6168 - val_loss: 1.6037\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5818 - val_loss: 1.5772\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5583 - val_loss: 1.5566\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5410 - val_loss: 1.5429\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5295 - val_loss: 1.5323\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5210 - val_loss: 1.5250\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5142 - val_loss: 1.5197\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5097 - val_loss: 1.5147\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5057 - val_loss: 1.5114\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5029 - val_loss: 1.5083\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5000 - val_loss: 1.5059\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4982 - val_loss: 1.5050\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4963 - val_loss: 1.5035\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4949 - val_loss: 1.5015\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4940 - val_loss: 1.5002\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4923 - val_loss: 1.4991\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4916 - val_loss: 1.4984\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.4908 - val_loss: 1.4974\n",
      "Top-2 accuracy = 0.607\n",
      "5\n",
      "normalizeC|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_22 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.8871 - val_loss: 1.8250\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7702 - val_loss: 1.7410\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7069 - val_loss: 1.6880\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6565 - val_loss: 1.6406\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6167 - val_loss: 1.6095\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5906 - val_loss: 1.5863\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5717 - val_loss: 1.5725\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5594 - val_loss: 1.5634\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5522 - val_loss: 1.5580\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5475 - val_loss: 1.5557\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5439 - val_loss: 1.5510\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5404 - val_loss: 1.5479\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5381 - val_loss: 1.5458\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5350 - val_loss: 1.5432\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5325 - val_loss: 1.5403\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5300 - val_loss: 1.5374\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5282 - val_loss: 1.5357\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5263 - val_loss: 1.5336\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5250 - val_loss: 1.5318\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5234 - val_loss: 1.5307\n",
      "Top-2 accuracy = 0.594\n",
      "6\n",
      "standardizel|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.9290 - val_loss: 1.9125\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8974 - val_loss: 1.8847\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8710 - val_loss: 1.8612\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8486 - val_loss: 1.8414\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8292 - val_loss: 1.8243\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8127 - val_loss: 1.8098\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7987 - val_loss: 1.7975\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7868 - val_loss: 1.7872\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7767 - val_loss: 1.7784\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7681 - val_loss: 1.7710\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7608 - val_loss: 1.7645\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7546 - val_loss: 1.7589\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7490 - val_loss: 1.7540\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7442 - val_loss: 1.7497\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7399 - val_loss: 1.7458\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7361 - val_loss: 1.7423\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7326 - val_loss: 1.7392\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7296 - val_loss: 1.7363\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7267 - val_loss: 1.7337\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7242 - val_loss: 1.7313\n",
      "Top-2 accuracy = 0.507\n",
      "7\n",
      "minmaxh|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_31 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8581 - val_loss: 1.7372\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6884 - val_loss: 1.6766\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6554 - val_loss: 1.6505\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6305 - val_loss: 1.6259\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6075 - val_loss: 1.6037\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5875 - val_loss: 1.5877\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5746 - val_loss: 1.5779\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5654 - val_loss: 1.5697\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5593 - val_loss: 1.5654\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5544 - val_loss: 1.5620\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5510 - val_loss: 1.5575\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5476 - val_loss: 1.5544\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5454 - val_loss: 1.5531\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5424 - val_loss: 1.5490\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5408 - val_loss: 1.5474\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5382 - val_loss: 1.5448\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5360 - val_loss: 1.5433\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5348 - val_loss: 1.5417\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5330 - val_loss: 1.5401\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5315 - val_loss: 1.5387\n",
      "Top-2 accuracy = 0.587\n",
      "8\n",
      "maxabsC|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_34 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.7906 - val_loss: 1.7178\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6896 - val_loss: 1.6866\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6660 - val_loss: 1.6603\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6251 - val_loss: 1.6026\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5705 - val_loss: 1.5678\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5538 - val_loss: 1.5583\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5451 - val_loss: 1.5514\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5396 - val_loss: 1.5433\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5339 - val_loss: 1.5443\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5313 - val_loss: 1.5361\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5296 - val_loss: 1.5384\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5273 - val_loss: 1.5336\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5248 - val_loss: 1.5286\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5228 - val_loss: 1.5293\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5231 - val_loss: 1.5265\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5205 - val_loss: 1.5257\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5204 - val_loss: 1.5329\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5219 - val_loss: 1.5249\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5190 - val_loss: 1.5289\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5183 - val_loss: 1.5236\n",
      "Top-2 accuracy = 0.59\n",
      "9\n",
      "standardizer|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_41 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8522 - val_loss: 1.7545\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6895 - val_loss: 1.6467\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6145 - val_loss: 1.6025\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5841 - val_loss: 1.5845\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5689 - val_loss: 1.5731\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5591 - val_loss: 1.5657\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5523 - val_loss: 1.5595\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5466 - val_loss: 1.5540\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5418 - val_loss: 1.5494\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5368 - val_loss: 1.5443\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5322 - val_loss: 1.5399\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5282 - val_loss: 1.5358\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5244 - val_loss: 1.5325\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5213 - val_loss: 1.5286\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5180 - val_loss: 1.5256\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5158 - val_loss: 1.5232\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5141 - val_loss: 1.5209\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5122 - val_loss: 1.5190\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5109 - val_loss: 1.5188\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5093 - val_loss: 1.5169\n",
      "Top-2 accuracy = 0.6\n",
      "10\n",
      "robustv|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.8988 - val_loss: 1.8193\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7202 - val_loss: 1.6430\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6092 - val_loss: 1.5936\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5801 - val_loss: 1.5755\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5682 - val_loss: 1.5668\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5603 - val_loss: 1.5603\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5535 - val_loss: 1.5516\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5463 - val_loss: 1.5450\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5407 - val_loss: 1.5379\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5360 - val_loss: 1.5350\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5317 - val_loss: 1.5320\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5292 - val_loss: 1.5304\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5278 - val_loss: 1.5310\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5263 - val_loss: 1.5301\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5258 - val_loss: 1.5281\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5245 - val_loss: 1.5278\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5233 - val_loss: 1.5271\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5228 - val_loss: 1.5271\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5217 - val_loss: 1.5263\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5213 - val_loss: 1.5255\n",
      "Top-2 accuracy = 0.589\n",
      "11\n",
      "normalizeg|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_47 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9157 - val_loss: 1.8743\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.8060 - val_loss: 1.7300\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6701 - val_loss: 1.6375\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6040 - val_loss: 1.5973\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5816 - val_loss: 1.5863\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5726 - val_loss: 1.5788\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5670 - val_loss: 1.5733\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5617 - val_loss: 1.5678\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5563 - val_loss: 1.5625\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5520 - val_loss: 1.5585\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5486 - val_loss: 1.5571\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5456 - val_loss: 1.5519\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5427 - val_loss: 1.5501\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5402 - val_loss: 1.5471\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5380 - val_loss: 1.5464\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5357 - val_loss: 1.5429\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5337 - val_loss: 1.5404\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5320 - val_loss: 1.5382\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5297 - val_loss: 1.5372\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5276 - val_loss: 1.5346\n",
      "Top-2 accuracy = 0.587\n",
      "12\n",
      "robusta|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_52 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9272 - val_loss: 1.8970\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.8388 - val_loss: 1.7941\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7571 - val_loss: 1.7394\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7161 - val_loss: 1.7096\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6777 - val_loss: 1.6671\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6367 - val_loss: 1.6283\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6040 - val_loss: 1.6092\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5908 - val_loss: 1.5957\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5823 - val_loss: 1.5902\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5773 - val_loss: 1.5859\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5741 - val_loss: 1.5814\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5706 - val_loss: 1.5778\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5672 - val_loss: 1.5745\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5638 - val_loss: 1.5727\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5617 - val_loss: 1.5683\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5591 - val_loss: 1.5652\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5568 - val_loss: 1.5624\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5553 - val_loss: 1.5601\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5532 - val_loss: 1.5583\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5517 - val_loss: 1.5567\n",
      "Top-2 accuracy = 0.574\n",
      "13\n",
      "robustu|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_59 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8660 - val_loss: 1.7561\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6735 - val_loss: 1.6145\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5730 - val_loss: 1.5606\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5435 - val_loss: 1.5453\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5341 - val_loss: 1.5367\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5271 - val_loss: 1.5311\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5224 - val_loss: 1.5251\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5181 - val_loss: 1.5238\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5150 - val_loss: 1.5187\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5131 - val_loss: 1.5164\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5103 - val_loss: 1.5150\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5084 - val_loss: 1.5123\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5064 - val_loss: 1.5119\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5060 - val_loss: 1.5103\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5059 - val_loss: 1.5092\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5033 - val_loss: 1.5088\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5022 - val_loss: 1.5076\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5016 - val_loss: 1.5062\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5006 - val_loss: 1.5081\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5004 - val_loss: 1.5050\n",
      "Top-2 accuracy = 0.6\n",
      "14\n",
      "standardizez|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_64 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.9812 - val_loss: 1.8981\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.8457 - val_loss: 1.7902\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7361 - val_loss: 1.7104\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6862 - val_loss: 1.6813\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6630 - val_loss: 1.6609\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6449 - val_loss: 1.6435\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6286 - val_loss: 1.6281\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6134 - val_loss: 1.6143\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5998 - val_loss: 1.6014\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5862 - val_loss: 1.5886\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5757 - val_loss: 1.5797\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5676 - val_loss: 1.5724\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5613 - val_loss: 1.5661\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5559 - val_loss: 1.5605\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5511 - val_loss: 1.5551\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5460 - val_loss: 1.5491\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5420 - val_loss: 1.5465\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5399 - val_loss: 1.5435\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5377 - val_loss: 1.5422\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5364 - val_loss: 1.5397\n",
      "Top-2 accuracy = 0.582\n",
      "15\n",
      "minmaxn|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_68 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8487 - val_loss: 1.7767\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7033 - val_loss: 1.6572\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6186 - val_loss: 1.6045\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5859 - val_loss: 1.5860\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5734 - val_loss: 1.5759\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5654 - val_loss: 1.5719\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5600 - val_loss: 1.5642\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5553 - val_loss: 1.5594\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5516 - val_loss: 1.5551\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5488 - val_loss: 1.5517\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5455 - val_loss: 1.5500\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5434 - val_loss: 1.5485\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5410 - val_loss: 1.5444\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5407 - val_loss: 1.5429\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5384 - val_loss: 1.5436\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5389 - val_loss: 1.5416\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5362 - val_loss: 1.5395\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5352 - val_loss: 1.5380\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5340 - val_loss: 1.5373\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5329 - val_loss: 1.5353\n",
      "Top-2 accuracy = 0.586\n",
      "16\n",
      "normalizeE|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8268 - val_loss: 1.7282\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6994 - val_loss: 1.6875\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6489 - val_loss: 1.6254\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5967 - val_loss: 1.5903\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5713 - val_loss: 1.5729\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5590 - val_loss: 1.5698\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5529 - val_loss: 1.5586\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5484 - val_loss: 1.5534\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5459 - val_loss: 1.5520\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5433 - val_loss: 1.5474\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5401 - val_loss: 1.5451\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5374 - val_loss: 1.5407\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5348 - val_loss: 1.5398\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5343 - val_loss: 1.5360\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5312 - val_loss: 1.5345\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5302 - val_loss: 1.5316\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5289 - val_loss: 1.5315\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5274 - val_loss: 1.5298\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5265 - val_loss: 1.5275\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5254 - val_loss: 1.5275\n",
      "Top-2 accuracy = 0.581\n",
      "17\n",
      "robustL|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_75 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1037 - val_loss: 1.9283\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.8808 - val_loss: 1.8537\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.8276 - val_loss: 1.8150\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7934 - val_loss: 1.7862\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7659 - val_loss: 1.7622\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7430 - val_loss: 1.7411\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7241 - val_loss: 1.7241\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7078 - val_loss: 1.7087\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6916 - val_loss: 1.6937\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6696 - val_loss: 1.6604\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6424 - val_loss: 1.6454\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6315 - val_loss: 1.6375\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6249 - val_loss: 1.6326\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6204 - val_loss: 1.6286\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6168 - val_loss: 1.6253\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6135 - val_loss: 1.6215\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6103 - val_loss: 1.6186\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6074 - val_loss: 1.6169\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6047 - val_loss: 1.6140\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6006 - val_loss: 1.6090\n",
      "Top-2 accuracy = 0.556\n",
      "18\n",
      "robustB|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.8935 - val_loss: 1.8054\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7254 - val_loss: 1.6781\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6238 - val_loss: 1.5989\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5701 - val_loss: 1.5687\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5472 - val_loss: 1.5472\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5331 - val_loss: 1.5367\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5252 - val_loss: 1.5302\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5202 - val_loss: 1.5253\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5151 - val_loss: 1.5196\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5124 - val_loss: 1.5160\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5099 - val_loss: 1.5139\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5074 - val_loss: 1.5124\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5055 - val_loss: 1.5118\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5038 - val_loss: 1.5095\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5033 - val_loss: 1.5088\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5014 - val_loss: 1.5071\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5003 - val_loss: 1.5075\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4989 - val_loss: 1.5076\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4973 - val_loss: 1.5038\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.4974 - val_loss: 1.5034\n",
      "Top-2 accuracy = 0.601\n",
      "19\n",
      "minmaxR|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_82 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7625 - val_loss: 1.7247\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6957 - val_loss: 1.6838\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6603 - val_loss: 1.6502\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6272 - val_loss: 1.6189\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6001 - val_loss: 1.5964\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5801 - val_loss: 1.5814\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5689 - val_loss: 1.5722\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5621 - val_loss: 1.5696\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5591 - val_loss: 1.5646\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5553 - val_loss: 1.5629\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5537 - val_loss: 1.5596\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5513 - val_loss: 1.5577\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5495 - val_loss: 1.5554\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5452 - val_loss: 1.5508\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5423 - val_loss: 1.5485\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5399 - val_loss: 1.5455\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5377 - val_loss: 1.5437\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5356 - val_loss: 1.5417\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5341 - val_loss: 1.5407\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5329 - val_loss: 1.5378\n",
      "Top-2 accuracy = 0.585\n",
      "20\n",
      "minmaxi|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_85 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9119 - val_loss: 1.8571\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7704 - val_loss: 1.7171\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6953 - val_loss: 1.6957\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6842 - val_loss: 1.6898\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6800 - val_loss: 1.6863\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6771 - val_loss: 1.6836\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6747 - val_loss: 1.6817\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6727 - val_loss: 1.6793\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6703 - val_loss: 1.6767\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6676 - val_loss: 1.6743\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6644 - val_loss: 1.6703\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6602 - val_loss: 1.6658\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6556 - val_loss: 1.6613\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6498 - val_loss: 1.6545\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6435 - val_loss: 1.6483\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6373 - val_loss: 1.6419\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6308 - val_loss: 1.6356\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6253 - val_loss: 1.6299\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6192 - val_loss: 1.6240\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6137 - val_loss: 1.6188\n",
      "Top-2 accuracy = 0.552\n",
      "21\n",
      "standardizeC|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.9142 - val_loss: 1.8736\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8099 - val_loss: 1.7724\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7456 - val_loss: 1.7428\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7202 - val_loss: 1.7182\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6993 - val_loss: 1.7010\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6866 - val_loss: 1.6908\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6789 - val_loss: 1.6831\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6710 - val_loss: 1.6725\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6550 - val_loss: 1.6519\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6302 - val_loss: 1.6274\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6096 - val_loss: 1.6116\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5960 - val_loss: 1.6011\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5857 - val_loss: 1.5936\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5781 - val_loss: 1.5850\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5721 - val_loss: 1.5806\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5664 - val_loss: 1.5748\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5626 - val_loss: 1.5708\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5595 - val_loss: 1.5667\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5562 - val_loss: 1.5637\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5527 - val_loss: 1.5600\n",
      "Top-2 accuracy = 0.574\n",
      "22\n",
      "maxabsu|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_99 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.8496 - val_loss: 1.7665\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7142 - val_loss: 1.6884\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6624 - val_loss: 1.6497\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6262 - val_loss: 1.6170\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5967 - val_loss: 1.5881\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5757 - val_loss: 1.5730\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5629 - val_loss: 1.5631\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5546 - val_loss: 1.5559\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5488 - val_loss: 1.5509\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5478 - val_loss: 1.5487\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5409 - val_loss: 1.5459\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5396 - val_loss: 1.5433\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5366 - val_loss: 1.5417\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5350 - val_loss: 1.5418\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5342 - val_loss: 1.5374\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5315 - val_loss: 1.5373\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5294 - val_loss: 1.5346\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5281 - val_loss: 1.5323\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5273 - val_loss: 1.5302\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5254 - val_loss: 1.5303\n",
      "Top-2 accuracy = 0.587\n",
      "23\n",
      "normalizeS|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.9079 - val_loss: 1.8645\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8157 - val_loss: 1.7835\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7602 - val_loss: 1.7554\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7374 - val_loss: 1.7379\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7219 - val_loss: 1.7251\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7107 - val_loss: 1.7152\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7020 - val_loss: 1.7075\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6960 - val_loss: 1.7026\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6921 - val_loss: 1.6994\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6897 - val_loss: 1.6975\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6882 - val_loss: 1.6967\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6873 - val_loss: 1.6956\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6868 - val_loss: 1.6952\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6864 - val_loss: 1.6949\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6861 - val_loss: 1.6946\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6858 - val_loss: 1.6947\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6857 - val_loss: 1.6949\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6858 - val_loss: 1.6944\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6857 - val_loss: 1.6942\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6855 - val_loss: 1.6939\n",
      "Top-2 accuracy = 0.507\n",
      "24\n",
      "minmaxj|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_112 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9017 - val_loss: 1.8295\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7581 - val_loss: 1.7201\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6766 - val_loss: 1.6541\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6240 - val_loss: 1.6140\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5956 - val_loss: 1.5974\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5819 - val_loss: 1.5904\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5742 - val_loss: 1.5820\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5700 - val_loss: 1.5791\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5666 - val_loss: 1.5765\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5638 - val_loss: 1.5737\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5617 - val_loss: 1.5714\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5600 - val_loss: 1.5700\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5585 - val_loss: 1.5689\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5567 - val_loss: 1.5665\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5558 - val_loss: 1.5672\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5547 - val_loss: 1.5645\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5539 - val_loss: 1.5635\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5531 - val_loss: 1.5625\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5522 - val_loss: 1.5626\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5517 - val_loss: 1.5609\n",
      "Top-2 accuracy = 0.579\n",
      "25\n",
      "robuste|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_116 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 4.1746 - val_loss: 2.2919\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0084 - val_loss: 1.8376\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7604 - val_loss: 1.7183\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6778 - val_loss: 1.6623\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6363 - val_loss: 1.6322\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6119 - val_loss: 1.6149\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5966 - val_loss: 1.6029\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5847 - val_loss: 1.5913\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5761 - val_loss: 1.5830\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5695 - val_loss: 1.5785\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5628 - val_loss: 1.5741\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5586 - val_loss: 1.5696\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5531 - val_loss: 1.5641\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5498 - val_loss: 1.5603\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5465 - val_loss: 1.5607\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5440 - val_loss: 1.5540\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5410 - val_loss: 1.5506\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5393 - val_loss: 1.5491\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5369 - val_loss: 1.5476\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5344 - val_loss: 1.5445\n",
      "Top-2 accuracy = 0.576\n",
      "26\n",
      "maxabsk|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_120 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8374 - val_loss: 1.7158\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6380 - val_loss: 1.6003\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5729 - val_loss: 1.5710\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.5535 - val_loss: 1.5570\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5432 - val_loss: 1.5488\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5357 - val_loss: 1.5405\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5305 - val_loss: 1.5360\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5261 - val_loss: 1.5314\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5229 - val_loss: 1.5302\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5201 - val_loss: 1.5277\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5175 - val_loss: 1.5228\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5154 - val_loss: 1.5212\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5131 - val_loss: 1.5191\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5109 - val_loss: 1.5163\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5094 - val_loss: 1.5157\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5076 - val_loss: 1.5129\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5057 - val_loss: 1.5122\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5043 - val_loss: 1.5093\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5027 - val_loss: 1.5076\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5021 - val_loss: 1.5066\n",
      "Top-2 accuracy = 0.604\n",
      "27\n",
      "maxabsn|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_123 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 1.8866 - val_loss: 1.7881\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7269 - val_loss: 1.6986\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6774 - val_loss: 1.6699\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6517 - val_loss: 1.6451\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6266 - val_loss: 1.6205\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.6023 - val_loss: 1.5962\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5802 - val_loss: 1.5767\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5640 - val_loss: 1.5637\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5534 - val_loss: 1.5545\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5460 - val_loss: 1.5479\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5396 - val_loss: 1.5425\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5350 - val_loss: 1.5375\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5311 - val_loss: 1.5338\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5278 - val_loss: 1.5307\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5253 - val_loss: 1.5280\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5231 - val_loss: 1.5265\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5209 - val_loss: 1.5245\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5192 - val_loss: 1.5234\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5178 - val_loss: 1.5213\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5164 - val_loss: 1.5199\n",
      "Top-2 accuracy = 0.594\n",
      "28\n",
      "robustg|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9164 - val_loss: 1.8763\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.8285 - val_loss: 1.7733\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.7110 - val_loss: 1.6621\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.6269 - val_loss: 1.6174\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5967 - val_loss: 1.5994\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5815 - val_loss: 1.5871\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5723 - val_loss: 1.5805\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5668 - val_loss: 1.5751\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5630 - val_loss: 1.5711\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5602 - val_loss: 1.5686\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5577 - val_loss: 1.5670\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5561 - val_loss: 1.5659\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5547 - val_loss: 1.5643\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5536 - val_loss: 1.5630\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.5528 - val_loss: 1.5627\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5518 - val_loss: 1.5619\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5512 - val_loss: 1.5612\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5504 - val_loss: 1.5601\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5498 - val_loss: 1.5597\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.5493 - val_loss: 1.5586\n",
      "Top-2 accuracy = 0.579\n",
      "29\n",
      "robustE|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 1.9286 - val_loss: 1.9123\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8974 - val_loss: 1.8847\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8709 - val_loss: 1.8611\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8483 - val_loss: 1.8411\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8291 - val_loss: 1.8241\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.8128 - val_loss: 1.8099\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7989 - val_loss: 1.7977\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7869 - val_loss: 1.7873\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7771 - val_loss: 1.7787\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7685 - val_loss: 1.7711\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7611 - val_loss: 1.7647\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7547 - val_loss: 1.7591\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7492 - val_loss: 1.7542\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7444 - val_loss: 1.7498\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7401 - val_loss: 1.7459\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7362 - val_loss: 1.7425\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7328 - val_loss: 1.7393\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7297 - val_loss: 1.7365\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7269 - val_loss: 1.7339\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.7243 - val_loss: 1.7314\n",
      "Top-2 accuracy = 0.507\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"n_runs\": 1,\n",
    "    \"transforms\": [\"normalize\", \"standardize\", \"robust\", \"maxabs\", \"minmax\"] * 30,\n",
    "    \"metrics\": [\"accuracy\", \"pd\", \"prec\"],\n",
    "    \"random\": True,\n",
    "    \"learners\": [MulticlassDL(n_classes=7, random={'n_layers': (2, 6), 'n_units': (3, 20)}, n_epochs=20)],\n",
    "    \"post_train_hooks\": [top2_hook],\n",
    "    \"log_path\": \"./\",\n",
    "    \"data\": [data],\n",
    "    \"name\": \"firefox-7class\"\n",
    "}\n",
    "for _ in range(50):\n",
    "    config[\"learners\"].append(\n",
    "        MulticlassDL(n_classes=7, random={'n_layers': (2, 6), 'n_units': (3, 20)}, n_epochs=20))\n",
    "\n",
    "dodge = DODGE(config)\n",
    "dodge.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9-class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(*train_test_split(x, y))\n",
    "data.y_train = np.where(data.y_train < 1, 0, np.where(data.y_train < 2, 1, np.where(data.y_train < 3, 2, np.where(data.y_train < 4, 3, np.where(data.y_train < 6, 4, np.where(data.y_train < 8, 5, np.where(data.y_train < 11, 6, np.where(data.y_train < 21, 7, 8))))))))\n",
    "data.y_test = np.where(data.y_test < 1, 0, np.where(data.y_test < 2, 1, np.where(data.y_test < 3, 2, np.where(data.y_test < 4, 3, np.where(data.y_test < 6, 4, np.where(data.y_test < 8, 5, np.where(data.y_test < 11, 6, np.where(data.y_test < 21, 7, 8))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.y_train = to_categorical(data.y_train, num_classes=9)\n",
    "data.y_test = to_categorical(data.y_test, num_classes=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c7b130>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 4, 'n_units': 5, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c7bac0>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 3, 'n_units': 20, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c76cd0>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 2, 'n_units': 17, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c76850>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 3, 'n_units': 16, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c76dc0>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 3, 'n_units': 9, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c76280>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 6, 'n_units': 9, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c76be0>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 4, 'n_units': 9, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c85580>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 2, 'n_units': 20, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c85b80>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 6, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c85070>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 2, 'n_units': 5, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1477bcb20>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 2, 'n_units': 6, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c85a30>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 2, 'n_units': 9, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c85eb0>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 5, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c85dc0>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 5, 'n_units': 15, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c85370>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 4, 'n_units': 11, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c85d00>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 4, 'n_units': 8, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c73760>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 5, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c73e20>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 6, 'n_units': 17, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c73100>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 4, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c73d30>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 2, 'n_units': 4, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c73a00>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 6, 'n_units': 7, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c734c0>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 6, 'n_units': 18, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c735b0>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 4, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c8c430>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 2, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c8c040>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 6, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c8c4c0>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 6, 'n_units': 18, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c8a520>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 4, 'n_units': 20, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c8ad30>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 5, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c8a400>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 6, 'n_units': 17, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c8ab80>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 5, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c8adc0>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 6, 'n_units': 20, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c8a940>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 2, 'n_units': 15, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c8aac0>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 3, 'n_units': 16, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1479c2df0>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 6, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1479c2d90>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 5, 'n_units': 14, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x14783d700>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 2, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x1479a1be0>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 4, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c801f0>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 5, 'n_units': 5, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c80850>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 2, 'n_units': 17, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c80a30>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 6, 'n_units': 19, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c80550>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 4, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c803d0>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 2, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c80f10>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 3, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c802b0>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 5, 'n_units': 18, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c80610>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 3, 'n_units': 17, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147762eb0>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 4, 'n_units': 15, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147991c40>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 4, 'n_units': 18, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c82d30>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 6, 'n_units': 16, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c82a00>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 6, 'n_units': 3, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c82640>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 3, 'n_units': 20, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'learner': <raise_utils.learners.multiclassdl.MulticlassDL object at 0x147c82430>, 'loss': 'categorical_crossentropy', 'n_classes': 9, 'n_epochs': 20, 'n_layers': 5, 'n_units': 17, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'random_map': {'n_layers': (2, 6), 'n_units': (3, 20)}, 'verbose': 1, 'wfo': False, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalizeO|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_137 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1637 - val_loss: 2.1416\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1305 - val_loss: 2.1208\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1067 - val_loss: 2.0936\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0803 - val_loss: 2.0669\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0504 - val_loss: 2.0477\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0371 - val_loss: 2.0422\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0310 - val_loss: 2.0357\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0297 - val_loss: 2.0334\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0251 - val_loss: 2.0504\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0264 - val_loss: 2.0366\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0220 - val_loss: 2.0274\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0209 - val_loss: 2.0257\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0188 - val_loss: 2.0282\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0198 - val_loss: 2.0227\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0174 - val_loss: 2.0281\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0163 - val_loss: 2.0220\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0158 - val_loss: 2.0249\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0169 - val_loss: 2.0334\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0175 - val_loss: 2.0207\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0141 - val_loss: 2.0190\n",
      "Top-2 accuracy = 0.425\n",
      "1\n",
      "standardizeT|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_144 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1823 - val_loss: 2.1577\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1422 - val_loss: 2.1270\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1133 - val_loss: 2.1036\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0894 - val_loss: 2.0824\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0657 - val_loss: 2.0570\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0464 - val_loss: 2.0417\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0336 - val_loss: 2.0318\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0235 - val_loss: 2.0238\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0173 - val_loss: 2.0190\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0130 - val_loss: 2.0139\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0069 - val_loss: 2.0108\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0031 - val_loss: 2.0078\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9985 - val_loss: 2.0036\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9962 - val_loss: 2.0015\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9929 - val_loss: 1.9988\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9901 - val_loss: 1.9963\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9891 - val_loss: 1.9959\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9863 - val_loss: 1.9952\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9850 - val_loss: 1.9922\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9829 - val_loss: 1.9919\n",
      "Top-2 accuracy = 0.44\n",
      "2\n",
      "maxabsc|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_151 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1814 - val_loss: 2.1561\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1243 - val_loss: 2.0972\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0717 - val_loss: 2.0593\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0422 - val_loss: 2.0381\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0280 - val_loss: 2.0249\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0170 - val_loss: 2.0205\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0076 - val_loss: 2.0213\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0037 - val_loss: 2.0063\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9970 - val_loss: 2.0038\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9934 - val_loss: 1.9999\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9903 - val_loss: 1.9957\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9883 - val_loss: 2.0006\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9889 - val_loss: 1.9918\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9844 - val_loss: 1.9985\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9854 - val_loss: 1.9982\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9822 - val_loss: 1.9880\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9798 - val_loss: 1.9863\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9780 - val_loss: 1.9891\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9802 - val_loss: 1.9866\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9789 - val_loss: 1.9823\n",
      "Top-2 accuracy = 0.438\n",
      "3\n",
      "minmaxP|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_157 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1829 - val_loss: 2.1602\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1424 - val_loss: 2.1280\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1078 - val_loss: 2.0899\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0725 - val_loss: 2.0611\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0498 - val_loss: 2.0481\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0382 - val_loss: 2.0416\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0326 - val_loss: 2.0364\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0281 - val_loss: 2.0358\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0242 - val_loss: 2.0305\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0214 - val_loss: 2.0268\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0182 - val_loss: 2.0239\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0149 - val_loss: 2.0220\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0123 - val_loss: 2.0191\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0093 - val_loss: 2.0173\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0077 - val_loss: 2.0136\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0040 - val_loss: 2.0126\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0015 - val_loss: 2.0096\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9987 - val_loss: 2.0079\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9958 - val_loss: 2.0065\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9935 - val_loss: 2.0017\n",
      "Top-2 accuracy = 0.437\n",
      "4\n",
      "robustS|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_161 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1977 - val_loss: 2.1752\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1585 - val_loss: 2.1347\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1135 - val_loss: 2.0944\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0784 - val_loss: 2.0656\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0438 - val_loss: 2.0322\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0176 - val_loss: 2.0121\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0012 - val_loss: 2.0019\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9927 - val_loss: 1.9943\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9862 - val_loss: 1.9895\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9821 - val_loss: 1.9869\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9803 - val_loss: 1.9862\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9782 - val_loss: 1.9853\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9759 - val_loss: 1.9825\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9745 - val_loss: 1.9800\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9723 - val_loss: 1.9810\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9710 - val_loss: 1.9781\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9698 - val_loss: 1.9772\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9686 - val_loss: 1.9757\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9668 - val_loss: 1.9752\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9657 - val_loss: 1.9780\n",
      "Top-2 accuracy = 0.445\n",
      "5\n",
      "maxabsZ|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_166 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 1s 12ms/step - loss: 2.1876 - val_loss: 2.1790\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1678 - val_loss: 2.1601\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1500 - val_loss: 2.1454\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1363 - val_loss: 2.1325\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1230 - val_loss: 2.1197\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1102 - val_loss: 2.1068\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0974 - val_loss: 2.0946\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0848 - val_loss: 2.0833\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0733 - val_loss: 2.0733\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0637 - val_loss: 2.0663\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0579 - val_loss: 2.0621\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0540 - val_loss: 2.0591\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0513 - val_loss: 2.0565\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0488 - val_loss: 2.0546\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0467 - val_loss: 2.0528\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0450 - val_loss: 2.0518\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0436 - val_loss: 2.0503\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0421 - val_loss: 2.0495\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0408 - val_loss: 2.0478\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0395 - val_loss: 2.0469\n",
      "Top-2 accuracy = 0.42\n",
      "6\n",
      "normalizeo|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_169 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1912 - val_loss: 2.1857\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1821 - val_loss: 2.1789\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1769 - val_loss: 2.1750\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1740 - val_loss: 2.1729\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1723 - val_loss: 2.1716\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1714 - val_loss: 2.1710\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1709 - val_loss: 2.1707\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1707 - val_loss: 2.1706\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1706 - val_loss: 2.1705\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1706 - val_loss: 2.1705\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1705 - val_loss: 2.1705\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1705 - val_loss: 2.1705\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1705 - val_loss: 2.1704\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1705 - val_loss: 2.1704\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1705 - val_loss: 2.1704\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1705 - val_loss: 2.1704\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1705 - val_loss: 2.1704\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1705 - val_loss: 2.1704\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1705 - val_loss: 2.1704\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1705 - val_loss: 2.1704\n",
      "Top-2 accuracy = 0.297\n",
      "7\n",
      "normalizeV|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_176 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1618 - val_loss: 2.1271\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1078 - val_loss: 2.0849\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0683 - val_loss: 2.0534\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0448 - val_loss: 2.0399\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0342 - val_loss: 2.0326\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0259 - val_loss: 2.0262\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0197 - val_loss: 2.0197\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0129 - val_loss: 2.0148\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0080 - val_loss: 2.0098\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0030 - val_loss: 2.0060\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9998 - val_loss: 2.0033\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9971 - val_loss: 2.0009\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9946 - val_loss: 1.9993\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9917 - val_loss: 1.9972\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9898 - val_loss: 1.9956\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9879 - val_loss: 1.9968\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9875 - val_loss: 1.9939\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9853 - val_loss: 1.9924\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9850 - val_loss: 1.9919\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9833 - val_loss: 1.9909\n",
      "Top-2 accuracy = 0.441\n",
      "8\n",
      "standardizeA|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_180 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 2.1664 - val_loss: 2.1312\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0999 - val_loss: 2.0723\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0470 - val_loss: 2.0293\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0209 - val_loss: 2.0183\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0096 - val_loss: 2.0127\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0053 - val_loss: 2.0066\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0000 - val_loss: 2.0068\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9986 - val_loss: 2.0027\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9947 - val_loss: 1.9992\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9916 - val_loss: 1.9993\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9899 - val_loss: 1.9972\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9879 - val_loss: 1.9965\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9862 - val_loss: 1.9961\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9850 - val_loss: 1.9947\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9839 - val_loss: 1.9933\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9823 - val_loss: 1.9935\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9821 - val_loss: 1.9940\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9818 - val_loss: 1.9940\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9786 - val_loss: 1.9908\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9776 - val_loss: 1.9901\n",
      "Top-2 accuracy = 0.437\n",
      "9\n",
      "robustE|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_187 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 2.2632 - val_loss: 2.2070\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1916 - val_loss: 2.1833\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1790 - val_loss: 2.1761\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1743 - val_loss: 2.1728\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1716 - val_loss: 2.1704\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1690 - val_loss: 2.1674\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1648 - val_loss: 2.1621\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1586 - val_loss: 2.1548\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1507 - val_loss: 2.1458\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1409 - val_loss: 2.1351\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1297 - val_loss: 2.1232\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1181 - val_loss: 2.1116\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1079 - val_loss: 2.1007\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0982 - val_loss: 2.0918\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0906 - val_loss: 2.0853\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0849 - val_loss: 2.0805\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0805 - val_loss: 2.0766\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0767 - val_loss: 2.0733\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0733 - val_loss: 2.0704\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0702 - val_loss: 2.0677\n",
      "Top-2 accuracy = 0.394\n",
      "10\n",
      "minmaxw|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_192 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1882 - val_loss: 2.1819\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1741 - val_loss: 2.1689\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1594 - val_loss: 2.1538\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1415 - val_loss: 2.1337\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1221 - val_loss: 2.1201\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1117 - val_loss: 2.1124\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1052 - val_loss: 2.1050\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1004 - val_loss: 2.1008\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0965 - val_loss: 2.0971\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0934 - val_loss: 2.0939\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0904 - val_loss: 2.0906\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0866 - val_loss: 2.0866\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0822 - val_loss: 2.0817\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0780 - val_loss: 2.0781\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0746 - val_loss: 2.0750\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0716 - val_loss: 2.0721\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0691 - val_loss: 2.0705\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0670 - val_loss: 2.0682\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0654 - val_loss: 2.0670\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0637 - val_loss: 2.0654\n",
      "Top-2 accuracy = 0.396\n",
      "11\n",
      "minmaxh|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_195 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1870 - val_loss: 2.1710\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1565 - val_loss: 2.1423\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1335 - val_loss: 2.1197\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1106 - val_loss: 2.0961\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0879 - val_loss: 2.0781\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0733 - val_loss: 2.0668\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0635 - val_loss: 2.0603\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0579 - val_loss: 2.0574\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0540 - val_loss: 2.0543\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0517 - val_loss: 2.0528\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0502 - val_loss: 2.0547\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0498 - val_loss: 2.0498\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0479 - val_loss: 2.0489\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0483 - val_loss: 2.0551\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0483 - val_loss: 2.0471\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0450 - val_loss: 2.0475\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0451 - val_loss: 2.0469\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0442 - val_loss: 2.0467\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0436 - val_loss: 2.0459\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0428 - val_loss: 2.0444\n",
      "Top-2 accuracy = 0.402\n",
      "12\n",
      "maxabsd|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 2.1842 - val_loss: 2.1723\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1550 - val_loss: 2.1378\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1214 - val_loss: 2.1076\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0950 - val_loss: 2.0853\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0786 - val_loss: 2.0737\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0702 - val_loss: 2.0688\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0658 - val_loss: 2.0659\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0627 - val_loss: 2.0630\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0599 - val_loss: 2.0613\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0577 - val_loss: 2.0589\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0556 - val_loss: 2.0578\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0540 - val_loss: 2.0570\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0528 - val_loss: 2.0571\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0516 - val_loss: 2.0551\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0505 - val_loss: 2.0545\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0502 - val_loss: 2.0529\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0494 - val_loss: 2.0517\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0480 - val_loss: 2.0510\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0471 - val_loss: 2.0499\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0462 - val_loss: 2.0491\n",
      "Top-2 accuracy = 0.41\n",
      "13\n",
      "robustY|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_205 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1600 - val_loss: 2.1250\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0864 - val_loss: 2.0626\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0475 - val_loss: 2.0423\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0323 - val_loss: 2.0276\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0233 - val_loss: 2.0230\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0164 - val_loss: 2.0152\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0107 - val_loss: 2.0103\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0046 - val_loss: 2.0070\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0020 - val_loss: 2.0050\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9984 - val_loss: 2.0020\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9953 - val_loss: 2.0015\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9936 - val_loss: 1.9984\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9912 - val_loss: 1.9978\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9893 - val_loss: 1.9969\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9876 - val_loss: 1.9946\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9855 - val_loss: 1.9928\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9849 - val_loss: 1.9921\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9834 - val_loss: 1.9924\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9827 - val_loss: 1.9919\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9821 - val_loss: 1.9918\n",
      "Top-2 accuracy = 0.441\n",
      "14\n",
      "standardizeH|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1624 - val_loss: 2.0970\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0412 - val_loss: 2.0177\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0066 - val_loss: 2.0079\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9989 - val_loss: 2.0001\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9925 - val_loss: 1.9966\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9894 - val_loss: 1.9953\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9880 - val_loss: 1.9911\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9849 - val_loss: 1.9883\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9816 - val_loss: 1.9870\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9805 - val_loss: 1.9863\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9787 - val_loss: 1.9851\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9776 - val_loss: 1.9842\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9771 - val_loss: 1.9830\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9748 - val_loss: 1.9817\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9738 - val_loss: 1.9836\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9729 - val_loss: 1.9813\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9713 - val_loss: 1.9816\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9713 - val_loss: 1.9786\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9702 - val_loss: 1.9807\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9686 - val_loss: 1.9781\n",
      "Top-2 accuracy = 0.445\n",
      "15\n",
      "standardizeE|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1624 - val_loss: 2.0984\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0561 - val_loss: 2.0294\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0236 - val_loss: 2.0154\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0143 - val_loss: 2.0103\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0095 - val_loss: 2.0092\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0081 - val_loss: 2.0088\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0031 - val_loss: 2.0029\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9998 - val_loss: 1.9998\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9982 - val_loss: 1.9972\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9944 - val_loss: 1.9953\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9918 - val_loss: 1.9936\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9895 - val_loss: 1.9910\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9874 - val_loss: 1.9900\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9842 - val_loss: 1.9933\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9861 - val_loss: 1.9872\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9802 - val_loss: 1.9939\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9805 - val_loss: 1.9839\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9775 - val_loss: 1.9818\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9783 - val_loss: 1.9820\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9772 - val_loss: 1.9844\n",
      "Top-2 accuracy = 0.439\n",
      "16\n",
      "robustX|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_222 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 191786211344384.0000 - val_loss: 59095688151040.0000\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 28845923106816.0000 - val_loss: 3616317112320.0000\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 3282973753344.0000 - val_loss: 3910188662784.0000\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2871654875136.0000 - val_loss: 2602163503104.0000\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 3029806612480.0000 - val_loss: 2584284233728.0000\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 3078391857152.0000 - val_loss: 3020178849792.0000\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 3288956403712.0000 - val_loss: 2050581069824.0000\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2815440977920.0000 - val_loss: 2486782918656.0000\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2396352151552.0000 - val_loss: 2282300112896.0000\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2536617803776.0000 - val_loss: 3993922437120.0000\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2794607083520.0000 - val_loss: 3269108170752.0000\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2470155124736.0000 - val_loss: 1195396825088.0000\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 3060636581888.0000 - val_loss: 2273796947968.0000\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2218934403072.0000 - val_loss: 1623183327232.0000\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2806817751040.0000 - val_loss: 1592566349824.0000\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2637188562944.0000 - val_loss: 1322565500928.0000\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2157869662208.0000 - val_loss: 2652863201280.0000\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 3026771247104.0000 - val_loss: 2278171607040.0000\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2536290123776.0000 - val_loss: 2303818203136.0000\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2577154441216.0000 - val_loss: 3263088033792.0000\n",
      "Top-2 accuracy = 0.26\n",
      "17\n",
      "minmaxc|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_226 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1847 - val_loss: 2.1774\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1699 - val_loss: 2.1662\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1606 - val_loss: 2.1585\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1521 - val_loss: 2.1504\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1444 - val_loss: 2.1432\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1372 - val_loss: 2.1347\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1274 - val_loss: 2.1241\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1178 - val_loss: 2.1150\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1092 - val_loss: 2.1066\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1019 - val_loss: 2.1000\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0956 - val_loss: 2.0938\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0903 - val_loss: 2.0889\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0855 - val_loss: 2.0849\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0816 - val_loss: 2.0816\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0782 - val_loss: 2.0789\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0756 - val_loss: 2.0765\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0735 - val_loss: 2.0752\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0716 - val_loss: 2.0732\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0698 - val_loss: 2.0718\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0680 - val_loss: 2.0702\n",
      "Top-2 accuracy = 0.408\n",
      "18\n",
      "standardizee|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1829 - val_loss: 2.1715\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1587 - val_loss: 2.1463\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1337 - val_loss: 2.1230\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1152 - val_loss: 2.1092\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1023 - val_loss: 2.0972\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0919 - val_loss: 2.0885\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0851 - val_loss: 2.0823\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0781 - val_loss: 2.0755\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0725 - val_loss: 2.0703\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0671 - val_loss: 2.0649\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0625 - val_loss: 2.0611\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0582 - val_loss: 2.0569\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0550 - val_loss: 2.0549\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0536 - val_loss: 2.0533\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0514 - val_loss: 2.0518\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0507 - val_loss: 2.0519\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0496 - val_loss: 2.0504\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0486 - val_loss: 2.0509\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0483 - val_loss: 2.0495\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0475 - val_loss: 2.0486\n",
      "Top-2 accuracy = 0.406\n",
      "19\n",
      "standardizeo|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_235 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1917 - val_loss: 2.1840\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1790 - val_loss: 2.1743\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1708 - val_loss: 2.1673\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1634 - val_loss: 2.1593\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1535 - val_loss: 2.1469\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1393 - val_loss: 2.1296\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1201 - val_loss: 2.1079\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0994 - val_loss: 2.0889\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0833 - val_loss: 2.0768\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0717 - val_loss: 2.0669\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0644 - val_loss: 2.0626\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0596 - val_loss: 2.0590\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0558 - val_loss: 2.0561\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0534 - val_loss: 2.0545\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0515 - val_loss: 2.0534\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0502 - val_loss: 2.0523\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0487 - val_loss: 2.0513\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0475 - val_loss: 2.0507\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0463 - val_loss: 2.0500\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0452 - val_loss: 2.0488\n",
      "Top-2 accuracy = 0.407\n",
      "20\n",
      "normalizeN|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 4ms/step - loss: 2.1697 - val_loss: 2.1192\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0774 - val_loss: 2.0482\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0242 - val_loss: 2.0151\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0098 - val_loss: 2.0129\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0084 - val_loss: 2.0100\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0038 - val_loss: 2.0060\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0007 - val_loss: 2.0064\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0004 - val_loss: 2.0091\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0003 - val_loss: 2.0030\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9969 - val_loss: 2.0017\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9953 - val_loss: 2.0010\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9934 - val_loss: 2.0012\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9929 - val_loss: 1.9997\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9888 - val_loss: 1.9952\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9837 - val_loss: 1.9943\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9802 - val_loss: 1.9988\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9789 - val_loss: 1.9894\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9765 - val_loss: 1.9912\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9750 - val_loss: 1.9865\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9748 - val_loss: 1.9891\n",
      "Top-2 accuracy = 0.441\n",
      "21\n",
      "maxabsY|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_249 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1897 - val_loss: 2.1790\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1670 - val_loss: 2.1509\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1310 - val_loss: 2.1074\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0812 - val_loss: 2.0590\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0479 - val_loss: 2.0399\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0347 - val_loss: 2.0286\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0248 - val_loss: 2.0208\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0176 - val_loss: 2.0167\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0124 - val_loss: 2.0111\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0087 - val_loss: 2.0084\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0061 - val_loss: 2.0070\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0030 - val_loss: 2.0050\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0010 - val_loss: 2.0028\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9993 - val_loss: 2.0020\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9978 - val_loss: 2.0001\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9970 - val_loss: 2.0001\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9950 - val_loss: 1.9991\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9944 - val_loss: 1.9980\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9934 - val_loss: 1.9974\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9929 - val_loss: 1.9969\n",
      "Top-2 accuracy = 0.432\n",
      "22\n",
      "maxabsB|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1907 - val_loss: 2.1849\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1800 - val_loss: 2.1720\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1438 - val_loss: 2.1071\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0906 - val_loss: 2.0759\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0684 - val_loss: 2.0631\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0569 - val_loss: 2.0554\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0505 - val_loss: 2.0509\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0466 - val_loss: 2.0456\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0439 - val_loss: 2.0416\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0415 - val_loss: 2.0416\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0397 - val_loss: 2.0382\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0384 - val_loss: 2.0378\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0365 - val_loss: 2.0351\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0356 - val_loss: 2.0386\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0349 - val_loss: 2.0341\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0342 - val_loss: 2.0336\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0326 - val_loss: 2.0322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0321 - val_loss: 2.0316\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0316 - val_loss: 2.0306\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0312 - val_loss: 2.0310\n",
      "Top-2 accuracy = 0.411\n",
      "23\n",
      "robustl|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_261 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1812 - val_loss: 2.1491\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1167 - val_loss: 2.0947\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0754 - val_loss: 2.0611\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0500 - val_loss: 2.0423\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0346 - val_loss: 2.0315\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0236 - val_loss: 2.0210\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0148 - val_loss: 2.0128\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0067 - val_loss: 2.0074\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0005 - val_loss: 2.0058\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9978 - val_loss: 1.9997\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9920 - val_loss: 1.9979\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9894 - val_loss: 1.9945\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9863 - val_loss: 1.9929\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9848 - val_loss: 1.9913\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9831 - val_loss: 1.9893\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9820 - val_loss: 1.9880\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9801 - val_loss: 1.9872\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9789 - val_loss: 1.9865\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9779 - val_loss: 1.9850\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9765 - val_loss: 1.9856\n",
      "Top-2 accuracy = 0.44\n",
      "24\n",
      "normalizek|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_267 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1658 - val_loss: 2.1407\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1187 - val_loss: 2.0893\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0683 - val_loss: 2.0538\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0411 - val_loss: 2.0404\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0304 - val_loss: 2.0327\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0237 - val_loss: 2.0270\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0186 - val_loss: 2.0220\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0143 - val_loss: 2.0200\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0108 - val_loss: 2.0160\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0088 - val_loss: 2.0154\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0062 - val_loss: 2.0120\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0045 - val_loss: 2.0105\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0029 - val_loss: 2.0085\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0016 - val_loss: 2.0065\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9996 - val_loss: 2.0073\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9990 - val_loss: 2.0056\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9978 - val_loss: 2.0040\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9963 - val_loss: 2.0029\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9953 - val_loss: 2.0019\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9947 - val_loss: 2.0023\n",
      "Top-2 accuracy = 0.433\n",
      "25\n",
      "normalizeF|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_272 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1690 - val_loss: 2.1332\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0968 - val_loss: 2.0676\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0436 - val_loss: 2.0369\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0247 - val_loss: 2.0257\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0153 - val_loss: 2.0198\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0103 - val_loss: 2.0152\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0050 - val_loss: 2.0112\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0014 - val_loss: 2.0070\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9983 - val_loss: 2.0048\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9952 - val_loss: 2.0028\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9924 - val_loss: 2.0018\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9914 - val_loss: 1.9989\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9895 - val_loss: 1.9993\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9883 - val_loss: 1.9984\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9861 - val_loss: 1.9957\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9852 - val_loss: 1.9945\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9838 - val_loss: 1.9925\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9823 - val_loss: 1.9938\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9819 - val_loss: 1.9906\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9809 - val_loss: 1.9896\n",
      "Top-2 accuracy = 0.442\n",
      "26\n",
      "maxabsT|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1712 - val_loss: 2.1159\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0781 - val_loss: 2.0465\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0341 - val_loss: 2.0289\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0185 - val_loss: 2.0199\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0126 - val_loss: 2.0107\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0074 - val_loss: 2.0081\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0005 - val_loss: 2.0036\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9967 - val_loss: 2.0035\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9936 - val_loss: 1.9997\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9899 - val_loss: 1.9987\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9880 - val_loss: 1.9966\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9859 - val_loss: 1.9961\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9835 - val_loss: 1.9920\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9818 - val_loss: 1.9922\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9809 - val_loss: 1.9949\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9798 - val_loss: 1.9909\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9788 - val_loss: 1.9880\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9771 - val_loss: 1.9862\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9773 - val_loss: 1.9851\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9768 - val_loss: 1.9859\n",
      "Top-2 accuracy = 0.442\n",
      "27\n",
      "robustp|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1761 - val_loss: 2.1575\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.1298 - val_loss: 2.0955\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0654 - val_loss: 2.0477\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0418 - val_loss: 2.0392\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0350 - val_loss: 2.0353\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0289 - val_loss: 2.0314\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0243 - val_loss: 2.0252\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0175 - val_loss: 2.0231\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0140 - val_loss: 2.0175\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0110 - val_loss: 2.0151\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0096 - val_loss: 2.0144\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0065 - val_loss: 2.0108\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0045 - val_loss: 2.0090\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0032 - val_loss: 2.0092\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 2.0027 - val_loss: 2.0062\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9988 - val_loss: 2.0111\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9979 - val_loss: 2.0030\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9952 - val_loss: 2.0038\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9953 - val_loss: 2.0046\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 1ms/step - loss: 1.9927 - val_loss: 2.0006\n",
      "Top-2 accuracy = 0.438\n",
      "28\n",
      "maxabsG|rf\n",
      "Epoch 1/20\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1911 - val_loss: 2.1856\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1821 - val_loss: 2.1788\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1769 - val_loss: 2.1750\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1739 - val_loss: 2.1728\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1722 - val_loss: 2.1716\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1714 - val_loss: 2.1710\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1710 - val_loss: 2.1707\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1707 - val_loss: 2.1705\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1706 - val_loss: 2.1705\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1705 - val_loss: 2.1705\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1705 - val_loss: 2.1705\n",
      "Epoch 12/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1705 - val_loss: 2.1705\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1705 - val_loss: 2.1704\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1705 - val_loss: 2.1704\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1705 - val_loss: 2.1704\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1705 - val_loss: 2.1705\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1705 - val_loss: 2.1705\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1705 - val_loss: 2.1705\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1705 - val_loss: 2.1705\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.1705 - val_loss: 2.1705\n",
      "Top-2 accuracy = 0.297\n",
      "29\n",
      "normalizeZ|rf\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Layer dense_295 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "83/83 [==============================] - 0s 3ms/step - loss: 2.1725 - val_loss: 2.1409\n",
      "Epoch 2/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0957 - val_loss: 2.0546\n",
      "Epoch 3/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0379 - val_loss: 2.0347\n",
      "Epoch 4/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0224 - val_loss: 2.0179\n",
      "Epoch 5/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0136 - val_loss: 2.0094\n",
      "Epoch 6/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0074 - val_loss: 2.0061\n",
      "Epoch 7/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0034 - val_loss: 2.0037\n",
      "Epoch 8/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 2.0004 - val_loss: 2.0003\n",
      "Epoch 9/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9966 - val_loss: 1.9998\n",
      "Epoch 10/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9939 - val_loss: 1.9980\n",
      "Epoch 11/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9917 - val_loss: 1.9970\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9901 - val_loss: 1.9935\n",
      "Epoch 13/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9882 - val_loss: 1.9914\n",
      "Epoch 14/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9861 - val_loss: 1.9976\n",
      "Epoch 15/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9855 - val_loss: 1.9903\n",
      "Epoch 16/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9826 - val_loss: 1.9979\n",
      "Epoch 17/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9848 - val_loss: 1.9895\n",
      "Epoch 18/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9806 - val_loss: 1.9925\n",
      "Epoch 19/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9797 - val_loss: 1.9848\n",
      "Epoch 20/20\n",
      "83/83 [==============================] - 0s 2ms/step - loss: 1.9785 - val_loss: 1.9864\n",
      "Top-2 accuracy = 0.436\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"n_runs\": 1,\n",
    "    \"transforms\": [\"normalize\", \"standardize\", \"robust\", \"maxabs\", \"minmax\"] * 30,\n",
    "    \"metrics\": [\"accuracy\", \"pd\", \"prec\"],\n",
    "    \"random\": True,\n",
    "    \"learners\": [MulticlassDL(n_classes=9, random={'n_layers': (2, 6), 'n_units': (3, 20)}, n_epochs=20)],\n",
    "    \"post_train_hooks\": [top2_hook],\n",
    "    \"log_path\": \"./\",\n",
    "    \"data\": [data],\n",
    "    \"name\": \"firefox-9class\"\n",
    "}\n",
    "for _ in range(50):\n",
    "    config[\"learners\"].append(\n",
    "        MulticlassDL(n_classes=9, random={'n_layers': (2, 6), 'n_units': (3, 20)}, n_epochs=20))\n",
    "\n",
    "dodge = DODGE(config)\n",
    "dodge.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
