{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raise_utils.data import DataLoader\n",
    "from raise_utils.learners import FeedforwardDL, DecisionTree, BiasedSVM\n",
    "from raise_utils.hyperparams import DODGE\n",
    "from raise_utils.transform import Transform\n",
    "from sklearn.decomposition import PCA\n",
    "from raise_utils.metrics import ClassificationMetrics\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataLoader.from_file('DataClass.csv', target='SMELLS', col_start=0, col_stop=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Transform('cfs')\n",
    "transform.apply(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43820224719101125"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data.y_train) / len(data.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36666666666666664"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data.y_test) / len(data.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 60)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((89, 60), (30, 60), (89,), (30,))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x_train.shape, data.x_test.shape, data.y_train.shape, data.y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79    0\n",
       "98    1\n",
       "42    0\n",
       "34    0\n",
       "49    0\n",
       "Name: SMELLS, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_runs\": 20,\n",
    "    \"transforms\": [\"normalize\", \"standardize\", \"robust\", \"maxabs\", \"minmax\"] * 30,\n",
    "    \"metrics\": [\"d2h\", \"pd\", \"pf\", \"accuracy\", \"auc\"],\n",
    "    \"random\": True,\n",
    "    \"learners\": [],\n",
    "    \"log_path\": \"./\",\n",
    "    \"data\": [data],\n",
    "    \"name\": \"dataclass\"\n",
    "}\n",
    "for _ in range(50):\n",
    "    config[\"learners\"].append(FeedforwardDL(weighted=True,\n",
    "                                            wfo=True,\n",
    "                                            random={'n_layers': (2, 6),\n",
    "                                                    'n_units': (10, 70)}, \n",
    "                                            n_units=10, n_epochs=50, optimizer='adam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x153542070>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 43, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x153542520>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 39, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x153542760>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 3, 'n_units': 56, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x153542a30>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 26, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x153542ca0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 6, 'n_units': 19, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x153542f40>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 6, 'n_units': 17, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x15354c220>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 17, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x15354c4c0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 23, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x15354c760>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 52, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x15354c910>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 20, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x15354cc10>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 6, 'n_units': 48, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x15354cf40>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 3, 'n_units': 49, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bca1c0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 41, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bca460>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 3, 'n_units': 47, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bca700>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 19, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bca9a0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 62, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bcac40>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 3, 'n_units': 40, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bcaee0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 3, 'n_units': 48, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151be51c0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 6, 'n_units': 50, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151be5460>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 23, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151be5700>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151be59a0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 62, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151be5c40>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 26, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151be5ee0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 6, 'n_units': 59, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bb61c0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 65, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bb6460>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 3, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bb6700>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 39, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bb69a0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 48, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bb6c40>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 6, 'n_units': 33, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bb6ee0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 48, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bf2220>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 65, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bf2400>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 48, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bf2700>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 24, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bf28e0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 47, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bf2b80>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 68, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bf2e20>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 68, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bea100>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 26, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bea3a0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 24, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bea640>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 3, 'n_units': 54, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bea8e0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 63, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151beab80>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 25, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151beae20>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 6, 'n_units': 22, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bd4100>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 3, 'n_units': 47, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bd43a0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 6, 'n_units': 57, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bd4640>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 6, 'n_units': 51, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bd48e0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 50, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bd4be0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 6, 'n_units': 38, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bd4dc0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bd4f70>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x151bb7250>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 59, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n"
     ]
    }
   ],
   "source": [
    "dodge = DODGE(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "minmaxu|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_656 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 1.1578 - val_loss: 1.0604\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1498 - val_loss: 1.0825\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.1429 - val_loss: 1.1031\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.1368 - val_loss: 1.1220\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1314 - val_loss: 1.1395\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.1266 - val_loss: 1.1555\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.1222 - val_loss: 1.1709\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1178 - val_loss: 1.1860\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.1133 - val_loss: 1.2009\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.1088 - val_loss: 1.2161\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1047 - val_loss: 1.2313\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.1008 - val_loss: 1.2469\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0968 - val_loss: 1.2628\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0930 - val_loss: 1.2792\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.0890 - val_loss: 1.2957\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0853 - val_loss: 1.3125\n",
      "1\n",
      "maxabsA|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_662 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 144ms/step - loss: 1.1468 - val_loss: 1.1654\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1314 - val_loss: 1.2049\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1194 - val_loss: 1.2457\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.1085 - val_loss: 1.2881\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.0995 - val_loss: 1.3304\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0914 - val_loss: 1.3716\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0843 - val_loss: 1.4108\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.0771 - val_loss: 1.4474\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0703 - val_loss: 1.4802\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.0636 - val_loss: 1.5074\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0570 - val_loss: 1.5271\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0501 - val_loss: 1.5387\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.0428 - val_loss: 1.5424\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0351 - val_loss: 1.5401\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.0270 - val_loss: 1.5317\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.0179 - val_loss: 1.5183\n",
      "2\n",
      "minmaxL|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_667 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 115ms/step - loss: 1.1645 - val_loss: 1.1062\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.1468 - val_loss: 1.1561\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.1313 - val_loss: 1.2052\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.1173 - val_loss: 1.2525\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.1057 - val_loss: 1.2974\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0965 - val_loss: 1.3399\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0883 - val_loss: 1.3792\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0808 - val_loss: 1.4157\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.0737 - val_loss: 1.4493\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0675 - val_loss: 1.4795\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0619 - val_loss: 1.5061\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.0566 - val_loss: 1.5290\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0513 - val_loss: 1.5477\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0462 - val_loss: 1.5625\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0410 - val_loss: 1.5728\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0356 - val_loss: 1.5781\n",
      "3\n",
      "robustQ|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_671 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 1.3990 - val_loss: 0.9171\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1543 - val_loss: 1.0844\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0204 - val_loss: 1.2511\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9484 - val_loss: 1.3937\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9095 - val_loss: 1.4951\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8881 - val_loss: 1.5419\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8693 - val_loss: 1.5285\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8471 - val_loss: 1.4686\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8185 - val_loss: 1.3780\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7851 - val_loss: 1.2782\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7518 - val_loss: 1.1794\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7213 - val_loss: 1.0898\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6968 - val_loss: 1.0146\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6769 - val_loss: 0.9600\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6564 - val_loss: 0.9248\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6335 - val_loss: 0.9046\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6083 - val_loss: 0.8955\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5830 - val_loss: 0.8910\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5617 - val_loss: 0.8849\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5428 - val_loss: 0.8715\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5242 - val_loss: 0.8494\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5048 - val_loss: 0.8205\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4844 - val_loss: 0.7859\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4635 - val_loss: 0.7483\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4429 - val_loss: 0.7097\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4229 - val_loss: 0.6721\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4029 - val_loss: 0.6388\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3831 - val_loss: 0.6110\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3637 - val_loss: 0.5872\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3443 - val_loss: 0.5684\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3248 - val_loss: 0.5492\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3062 - val_loss: 0.5326\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2880 - val_loss: 0.5193\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2698 - val_loss: 0.5080\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2520 - val_loss: 0.4902\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2343 - val_loss: 0.4600\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2171 - val_loss: 0.4234\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2006 - val_loss: 0.3933\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1854 - val_loss: 0.3749\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1703 - val_loss: 0.3613\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1565 - val_loss: 0.3423\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1436 - val_loss: 0.3199\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1312 - val_loss: 0.2979\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1203 - val_loss: 0.2783\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1105 - val_loss: 0.2574\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1005 - val_loss: 0.2415\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0914 - val_loss: 0.2336\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0831 - val_loss: 0.2175\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0753 - val_loss: 0.1988\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0683 - val_loss: 0.1892\n",
      "4\n",
      "standardizec|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_678 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 95ms/step - loss: 1.3235 - val_loss: 0.8831\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2762 - val_loss: 0.9286\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.2371 - val_loss: 0.9714\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2051 - val_loss: 1.0108\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1786 - val_loss: 1.0467\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1578 - val_loss: 1.0785\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1407 - val_loss: 1.1068\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1267 - val_loss: 1.1322\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1147 - val_loss: 1.1548\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.1049 - val_loss: 1.1749\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0971 - val_loss: 1.1924\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0899 - val_loss: 1.2085\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0830 - val_loss: 1.2234\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0759 - val_loss: 1.2368\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0687 - val_loss: 1.2484\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0613 - val_loss: 1.2585\n",
      "5\n",
      "robustL|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_685 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 91ms/step - loss: 831845776228352.0000 - val_loss: 0.5345\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 309419342561280.0000 - val_loss: 262056372600832.0000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 204868128079872.0000 - val_loss: 400198845071360.0000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 151453213655040.0000 - val_loss: 489652242874368.0000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 173873748246528.0000 - val_loss: 520150201663488.0000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 189286792036352.0000 - val_loss: 486582985424896.0000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 183096938856448.0000 - val_loss: 410333290168320.0000\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step - loss: 160527691022336.0000 - val_loss: 311640343969792.0000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 128761299206144.0000 - val_loss: 200128849772544.0000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 92656797483008.0000 - val_loss: 105537186299904.0000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 65617386274816.0000 - val_loss: 68198670008320.0000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 68946044649472.0000 - val_loss: 33584043786240.0000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 56678863077376.0000 - val_loss: 7234369093632.0000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 38577798905856.0000 - val_loss: 0.5071\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 102531254452224.0000 - val_loss: 592877649920.0000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 36014127054848.0000 - val_loss: 32570098057216.0000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 16369316790272.0000 - val_loss: 73362571264000.0000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 32477351510016.0000 - val_loss: 92129086930944.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 40024821202944.0000 - val_loss: 95275125309440.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 42275535060992.0000 - val_loss: 84490160439296.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 39355322204160.0000 - val_loss: 61405881434112.0000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 36853524725760.0000 - val_loss: 32889299271680.0000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 23817686614016.0000 - val_loss: 22614120595456.0000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 28549945753600.0000 - val_loss: 21407832473600.0000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 29941517254656.0000 - val_loss: 20201284304896.0000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 24694409396224.0000 - val_loss: 18869114634240.0000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 13788534276096.0000 - val_loss: 36364447907840.0000\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 15827170492416.0000 - val_loss: 42678142107648.0000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 14650778320896.0000 - val_loss: 32266732437504.0000\n",
      "6\n",
      "minmaxv|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_691 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1.1412 - val_loss: 1.1585\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1316 - val_loss: 1.1743\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.1242 - val_loss: 1.1902\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.1179 - val_loss: 1.2078\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1118 - val_loss: 1.2269\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1055 - val_loss: 1.2473\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0988 - val_loss: 1.2690\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0912 - val_loss: 1.2927\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0827 - val_loss: 1.3199\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0733 - val_loss: 1.3501\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0632 - val_loss: 1.3827\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0526 - val_loss: 1.4174\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0418 - val_loss: 1.4519\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.0307 - val_loss: 1.4829\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0196 - val_loss: 1.5065\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0076 - val_loss: 1.5210\n",
      "7\n",
      "normalizeR|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_698 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 89ms/step - loss: 1.1423 - val_loss: 1.1588\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.1314 - val_loss: 1.1887\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1216 - val_loss: 1.2173\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1126 - val_loss: 1.2452\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.1042 - val_loss: 1.2728\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0970 - val_loss: 1.3004\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0910 - val_loss: 1.3288\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0855 - val_loss: 1.3574\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0803 - val_loss: 1.3867\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0750 - val_loss: 1.4164\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0697 - val_loss: 1.4461\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0646 - val_loss: 1.4749\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0598 - val_loss: 1.5015\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0552 - val_loss: 1.5247\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0509 - val_loss: 1.5434\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0467 - val_loss: 1.5566\n",
      "8\n",
      "robustC|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_703 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 13771238014976.0000 - val_loss: 35602896519168.0000\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step - loss: 10650890797056.0000 - val_loss: 25220188471296.0000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7555843096576.0000 - val_loss: 14866729402368.0000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4466376966144.0000 - val_loss: 4649367633920.0000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1416460369920.0000 - val_loss: 647141916672.0000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 6441034842112.0000 - val_loss: 176937320448.0000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 12941480951808.0000 - val_loss: 215830937600.0000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 9300321763328.0000 - val_loss: 352980762624.0000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2581945647104.0000 - val_loss: 4642269822976.0000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1342135468032.0000 - val_loss: 9343814598656.0000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2742412378112.0000 - val_loss: 13023868616704.0000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3860173946880.0000 - val_loss: 15588622598144.0000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4642744303616.0000 - val_loss: 17345596620800.0000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5191502397440.0000 - val_loss: 18334233919488.0000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5516349145088.0000 - val_loss: 18641078714368.0000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - ETA: 0s - loss: 5640795717632.000 - 0s 27ms/step - loss: 5640795717632.0000 - val_loss: 18388271235072.0000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5585495392256.0000 - val_loss: 17656776228864.0000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5390524219392.0000 - val_loss: 16459303485440.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5058727510016.0000 - val_loss: 14850625372160.0000\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4602307018752.0000 - val_loss: 12909239336960.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4012921847808.0000 - val_loss: 10621249650688.0000\n",
      "9\n",
      "robustF|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_706 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 1011020136448.0000 - val_loss: 0.9590\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7459722756096.0000 - val_loss: 1149244014592.0000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2344406220800.0000 - val_loss: 3605866741760.0000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 1s 713ms/step - loss: 1153591017472.0000 - val_loss: 5292958416896.0000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1939887620096.0000 - val_loss: 6060273303552.0000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2327454154752.0000 - val_loss: 6081944748032.0000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2401844854784.0000 - val_loss: 5661135470592.0000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2277044912128.0000 - val_loss: 4914840862720.0000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2021129715712.0000 - val_loss: 3691093164032.0000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1631768805376.0000 - val_loss: 2231857053696.0000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1150136614912.0000 - val_loss: 905497608192.0000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1199413788672.0000 - val_loss: 864082264064.0000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 458464755712.0000 - val_loss: 759909646336.0000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 238505934848.0000 - val_loss: 838065651712.0000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1650563481600.0000 - val_loss: 817927487488.0000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1406402822144.0000 - val_loss: 701558882304.0000\n",
      "10\n",
      "robustR|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_713 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 12938689642496.0000 - val_loss: 14451439828992.0000\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9123626221568.0000 - val_loss: 9338175356928.0000\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5899422269440.0000 - val_loss: 6224327213056.0000\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3651283189760.0000 - val_loss: 3899950104576.0000\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2298028490752.0000 - val_loss: 2155662278656.0000\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 6070543056896.0000 - val_loss: 2173340745728.0000\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4713484386304.0000 - val_loss: 2635597348864.0000\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1954167390208.0000 - val_loss: 3049747120128.0000\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2377425354752.0000 - val_loss: 2997856763904.0000\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2385984618496.0000 - val_loss: 2484797440000.0000\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2053185732608.0000 - val_loss: 1776493658112.0000\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1468536455168.0000 - val_loss: 1120459161600.0000\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1401776373760.0000 - val_loss: 857094422528.0000\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1453571702784.0000 - val_loss: 628352286720.0000\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 563572899840.0000 - val_loss: 603595997184.0000\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 555101257728.0000 - val_loss: 465258348544.0000\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 640896466944.0000 - val_loss: 465247174656.0000\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2052106223616.0000 - val_loss: 579169222656.0000\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 917459566592.0000 - val_loss: 734758961152.0000\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step - loss: 1089608089600.0000 - val_loss: 803605381120.0000\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1140553154560.0000 - val_loss: 752452829184.0000\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1081542377472.0000 - val_loss: 611110289408.0000\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 945589780480.0000 - val_loss: 454048153600.0000\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 739490267136.0000 - val_loss: 239494512640.0000\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 443773747200.0000 - val_loss: 66699767808.0000\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 751370567680.0000 - val_loss: 104467374080.0000\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 595366576128.0000 - val_loss: 388251320320.0000\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 518341361664.0000 - val_loss: 766971478016.0000\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 648170504192.0000 - val_loss: 1016124997632.0000\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 941147226112.0000 - val_loss: 1138287575040.0000\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1089921089536.0000 - val_loss: 1144151474176.0000\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1106812731392.0000 - val_loss: 1042707775488.0000\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 1003369660416.0000 - val_loss: 840628371456.0000\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 785299079168.0000 - val_loss: 517241405440.0000\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 455174029312.0000 - val_loss: 171340267520.0000\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 477252321280.0000 - val_loss: 13399539712.0000\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 157794222080.0000 - val_loss: 114580774912.0000\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1296921001984.0000 - val_loss: 305809326080.0000\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 567621582848.0000 - val_loss: 598779691008.0000\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 855166812160.0000 - val_loss: 798642077696.0000\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1038776664064.0000 - val_loss: 886133882880.0000\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1111914315776.0000 - val_loss: 871139049472.0000\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1078348677120.0000 - val_loss: 761805668352.0000\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 946629771264.0000 - val_loss: 566713516032.0000\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 730297073664.0000 - val_loss: 308177633280.0000\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 456176336896.0000 - val_loss: 63335051264.0000\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 117886722048.0000 - val_loss: 0.5712\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2169535070208.0000 - val_loss: 51173519360.0000\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 635949350912.0000 - val_loss: 372447150080.0000\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 359897792512.0000 - val_loss: 677266915328.0000\n",
      "11\n",
      "maxabsK|rf\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 148ms/step - loss: 1.1455 - val_loss: 1.1370\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1380 - val_loss: 1.1488\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.1346 - val_loss: 1.1586\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.1322 - val_loss: 1.1656\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.1305 - val_loss: 1.1735\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.1287 - val_loss: 1.1832\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1264 - val_loss: 1.1932\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1241 - val_loss: 1.2046\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1214 - val_loss: 1.2177\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.1185 - val_loss: 1.2322\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.1152 - val_loss: 1.2484\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.1115 - val_loss: 1.2676\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.1074 - val_loss: 1.2900\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1028 - val_loss: 1.3159\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0978 - val_loss: 1.3449\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0926 - val_loss: 1.3773\n",
      "12\n",
      "normalizeW|rf\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 1.1268 - val_loss: 1.1868\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1232 - val_loss: 1.1986\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.1201 - val_loss: 1.2101\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.1171 - val_loss: 1.2220\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.1141 - val_loss: 1.2340\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.1111 - val_loss: 1.2461\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1082 - val_loss: 1.2585\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.1053 - val_loss: 1.2712\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1023 - val_loss: 1.2842\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0994 - val_loss: 1.2973\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0965 - val_loss: 1.3105\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0935 - val_loss: 1.3240\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0906 - val_loss: 1.3376\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0878 - val_loss: 1.3512\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0849 - val_loss: 1.3648\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0820 - val_loss: 1.3785\n",
      "13\n",
      "minmaxG|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_726 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.1377"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-c53d9ca4fa04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdodge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/raise_utils/hyperparams/dodge.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m                     model.set_data(data.x_train, data.y_train,\n\u001b[1;32m     74\u001b[0m                                    data.x_test, data.y_test)\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0;31m# Run post-training hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/raise_utils/learners/feedforward.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         self.model.fit(np.array(self.x_train), np.array(self.y_train), batch_size=512, epochs=self.n_epochs,\n\u001b[0m\u001b[1;32m     96\u001b[0m                        validation_split=0.2, verbose=self.verbose, callbacks=[\n\u001b[1;32m     97\u001b[0m             \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    860\u001b[0m           val_x, val_y, val_sample_weight = (\n\u001b[1;32m    861\u001b[0m               data_adapter.unpack_x_y_sample_weight(validation_data))\n\u001b[0;32m--> 862\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m    863\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1079\u001b[0m                 step_num=step):\n\u001b[1;32m   1080\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1081\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 505\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    506\u001b[0m             *args, **kwds))\n\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2655\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 2657\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2659\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    955\u001b[0m           \u001b[0;31m# TODO(mdan): Push this block higher in tf.function's call stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m           \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m             return autograph.converted_call(\n\u001b[0m\u001b[1;32m    958\u001b[0m                 \u001b[0moriginal_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFunctionScope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_function'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fscope'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConversionOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser_requested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minternal_convert_user_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_per_replica\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'first'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    416\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_whitelist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Whitelisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    949\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m    950\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m--> 951\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m   \u001b[0;31m# TODO(b/151224785): Remove deprecated alias.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2288\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2289\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2290\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2292\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m         replica_id_in_sync_group=constant_op.constant(0, dtypes.int32)):\n\u001b[0;32m-> 2649\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2651\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperimental_hints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    416\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_whitelist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Whitelisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    909\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0;31m# Updates stateful loss metrics.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 911\u001b[0;31m     self.compiled_loss(\n\u001b[0m\u001b[1;32m    912\u001b[0m         y, y_pred, sample_weight, regularization_losses=self.losses)\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/compile_utils.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight, regularization_losses)\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0msw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m       \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m       \u001b[0mloss_metric_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/losses.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    141\u001b[0m         y_true, y_pred, sample_weight)\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m       \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m       return losses_utils.compute_weighted_loss(\n\u001b[1;32m    145\u001b[0m           losses, sample_weight, reduction=self._get_reduction())\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/losses.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    244\u001b[0m       y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\n\u001b[1;32m    245\u001b[0m           y_pred, y_true)\n\u001b[0;32m--> 246\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/raise_utils/learners/feedforward.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         return K.mean(\n\u001b[0;32m---> 21\u001b[0;31m             K.binary_crossentropy(y_true, y_pred) * weights)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbinary_crossentropy\u001b[0;34m(target, output, from_logits)\u001b[0m\n\u001b[1;32m   4690\u001b[0m       \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4691\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4692\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4694\u001b[0m   \u001b[0mepsilon_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_constant_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36msigmoid_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0mcond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mrelu_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mneg_abs_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     return math_ops.add(\n\u001b[1;32m    187\u001b[0m         \u001b[0mrelu_logits\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\u001b[0m in \u001b[0;36mwhere\u001b[0;34m(condition, x, y, name)\u001b[0m\n\u001b[1;32m   4247\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4248\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4249\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4250\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4251\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must both be non-None or both be None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(condition, x, y, name)\u001b[0m\n\u001b[1;32m   8582\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8583\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8584\u001b[0;31m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[0m\u001b[1;32m   8585\u001b[0m         \"Select\", condition=condition, t=x, e=y, name=name)\n\u001b[1;32m   8586\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    565\u001b[0m         \u001b[0mattr_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_types\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_attr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mattr_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m             raise TypeError(\n\u001b[1;32m    569\u001b[0m                 \u001b[0;34m\"Input '%s' of '%s' Op has type %s that does not \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36m__ne__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    205\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__ne__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;34m\"\"\"Returns True iff self != other.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m   \u001b[0;31m# \"If a class that overrides __eq__() needs to retain the implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    195\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mDType\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unidiomatic-typecheck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dodge.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_inp = Input((60,))\n",
    "enc_int = Dense(30, activation='relu')(enc_inp)\n",
    "enc_out = Dense(10, activation='relu')(enc_int)\n",
    "\n",
    "dec_int = Dense(30, activation='relu')(enc_out)\n",
    "dec_out = Dense(60, activation='relu')(dec_int)\n",
    "\n",
    "autoencoder = Model(inputs=enc_inp, outputs=dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=enc_inp, outputs=enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_inp = Input((10,))\n",
    "decoder = Model(inputs=dec_inp, outputs=autoencoder.layers[-1](autoencoder.layers[-2](dec_inp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0417\n",
      "Epoch 2/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0413\n",
      "Epoch 3/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0410\n",
      "Epoch 4/200\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.0408\n",
      "Epoch 5/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0405\n",
      "Epoch 6/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0403\n",
      "Epoch 7/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0401\n",
      "Epoch 8/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0399\n",
      "Epoch 9/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0397\n",
      "Epoch 10/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0395\n",
      "Epoch 11/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0393\n",
      "Epoch 12/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0391\n",
      "Epoch 13/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0389\n",
      "Epoch 14/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0387\n",
      "Epoch 15/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0386\n",
      "Epoch 16/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0384\n",
      "Epoch 17/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0382\n",
      "Epoch 18/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0380\n",
      "Epoch 19/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0378\n",
      "Epoch 20/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0376\n",
      "Epoch 21/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0374\n",
      "Epoch 22/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0372\n",
      "Epoch 23/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0370\n",
      "Epoch 24/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0368\n",
      "Epoch 25/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0367\n",
      "Epoch 26/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0365\n",
      "Epoch 27/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0363\n",
      "Epoch 28/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0362\n",
      "Epoch 29/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0360\n",
      "Epoch 30/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0358\n",
      "Epoch 31/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0356\n",
      "Epoch 32/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0355\n",
      "Epoch 33/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0353\n",
      "Epoch 34/200\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.0352\n",
      "Epoch 35/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0350\n",
      "Epoch 36/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0349\n",
      "Epoch 37/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0347\n",
      "Epoch 38/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0346\n",
      "Epoch 39/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0345\n",
      "Epoch 40/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0344\n",
      "Epoch 41/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0343\n",
      "Epoch 42/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0341\n",
      "Epoch 43/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0340\n",
      "Epoch 44/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0339\n",
      "Epoch 45/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0338\n",
      "Epoch 46/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0337\n",
      "Epoch 47/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0336\n",
      "Epoch 48/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0335\n",
      "Epoch 49/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0334\n",
      "Epoch 50/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0333\n",
      "Epoch 51/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0332\n",
      "Epoch 52/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0332\n",
      "Epoch 53/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0331\n",
      "Epoch 54/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0330\n",
      "Epoch 55/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0329\n",
      "Epoch 56/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0328\n",
      "Epoch 57/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0328\n",
      "Epoch 58/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0327\n",
      "Epoch 59/200\n",
      "3/3 [==============================] - 0s 7ms/step - loss: 0.0326\n",
      "Epoch 60/200\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.032 - 0s 2ms/step - loss: 0.0325\n",
      "Epoch 61/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0325\n",
      "Epoch 62/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0324\n",
      "Epoch 63/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0323\n",
      "Epoch 64/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0322\n",
      "Epoch 65/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0322\n",
      "Epoch 66/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0321\n",
      "Epoch 67/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0320\n",
      "Epoch 68/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0320\n",
      "Epoch 69/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0319\n",
      "Epoch 70/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0318\n",
      "Epoch 71/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0318\n",
      "Epoch 72/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0317\n",
      "Epoch 73/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0317\n",
      "Epoch 74/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0316\n",
      "Epoch 75/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0316\n",
      "Epoch 76/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0315\n",
      "Epoch 77/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0315\n",
      "Epoch 78/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0314\n",
      "Epoch 79/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0311\n",
      "Epoch 80/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0308\n",
      "Epoch 81/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0306\n",
      "Epoch 82/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0305\n",
      "Epoch 83/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0304\n",
      "Epoch 84/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0303\n",
      "Epoch 85/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0302\n",
      "Epoch 86/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0302\n",
      "Epoch 87/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0301\n",
      "Epoch 88/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0300\n",
      "Epoch 89/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0299\n",
      "Epoch 90/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0298\n",
      "Epoch 91/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0298\n",
      "Epoch 92/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0297\n",
      "Epoch 93/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0297\n",
      "Epoch 94/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0296\n",
      "Epoch 95/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0295\n",
      "Epoch 96/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0295\n",
      "Epoch 97/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0294\n",
      "Epoch 98/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0294\n",
      "Epoch 99/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0293\n",
      "Epoch 100/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0293\n",
      "Epoch 101/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0292\n",
      "Epoch 102/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0292\n",
      "Epoch 103/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0291\n",
      "Epoch 104/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0291\n",
      "Epoch 105/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0290\n",
      "Epoch 106/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0290\n",
      "Epoch 107/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0290\n",
      "Epoch 108/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0289\n",
      "Epoch 109/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0289\n",
      "Epoch 110/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0288\n",
      "Epoch 111/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0288\n",
      "Epoch 112/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0288\n",
      "Epoch 113/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0287\n",
      "Epoch 114/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0287\n",
      "Epoch 115/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0287\n",
      "Epoch 116/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0286\n",
      "Epoch 117/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0286\n",
      "Epoch 118/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0286\n",
      "Epoch 119/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0285\n",
      "Epoch 120/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0285\n",
      "Epoch 121/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0285\n",
      "Epoch 122/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0284\n",
      "Epoch 123/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0284\n",
      "Epoch 124/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0284\n",
      "Epoch 125/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0284\n",
      "Epoch 126/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0283\n",
      "Epoch 127/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0283\n",
      "Epoch 128/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0283\n",
      "Epoch 129/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0282\n",
      "Epoch 130/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0282\n",
      "Epoch 131/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0282\n",
      "Epoch 132/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0281\n",
      "Epoch 133/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0281\n",
      "Epoch 134/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0281\n",
      "Epoch 135/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0281\n",
      "Epoch 136/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0280\n",
      "Epoch 137/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0280\n",
      "Epoch 138/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0280\n",
      "Epoch 139/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0280\n",
      "Epoch 140/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0279\n",
      "Epoch 141/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0279\n",
      "Epoch 142/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0279\n",
      "Epoch 143/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0279\n",
      "Epoch 144/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0278\n",
      "Epoch 145/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0278\n",
      "Epoch 146/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0278\n",
      "Epoch 147/200\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.0278\n",
      "Epoch 148/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0278\n",
      "Epoch 149/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0277\n",
      "Epoch 150/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0277\n",
      "Epoch 151/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0277\n",
      "Epoch 152/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0277\n",
      "Epoch 153/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0276\n",
      "Epoch 154/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0276\n",
      "Epoch 155/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0276\n",
      "Epoch 156/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0276\n",
      "Epoch 157/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0276\n",
      "Epoch 158/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0276\n",
      "Epoch 159/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0275\n",
      "Epoch 160/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0275\n",
      "Epoch 161/200\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.0275\n",
      "Epoch 162/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0275\n",
      "Epoch 163/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0275\n",
      "Epoch 164/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0275\n",
      "Epoch 165/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0274\n",
      "Epoch 166/200\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.0274\n",
      "Epoch 167/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0274\n",
      "Epoch 168/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0274\n",
      "Epoch 169/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0274\n",
      "Epoch 170/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0270\n",
      "Epoch 171/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0267\n",
      "Epoch 172/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0266\n",
      "Epoch 173/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0263\n",
      "Epoch 174/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0261\n",
      "Epoch 175/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0259\n",
      "Epoch 176/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0257\n",
      "Epoch 177/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0257\n",
      "Epoch 178/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0256\n",
      "Epoch 179/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0256\n",
      "Epoch 180/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0255\n",
      "Epoch 181/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0255\n",
      "Epoch 182/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0255\n",
      "Epoch 183/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0255\n",
      "Epoch 184/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0254\n",
      "Epoch 185/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0254\n",
      "Epoch 186/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0254\n",
      "Epoch 187/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0254\n",
      "Epoch 188/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0254\n",
      "Epoch 189/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0253\n",
      "Epoch 190/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0253\n",
      "Epoch 191/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0253\n",
      "Epoch 192/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0253\n",
      "Epoch 193/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0253\n",
      "Epoch 194/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0253\n",
      "Epoch 195/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0252\n",
      "Epoch 196/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0252\n",
      "Epoch 197/200\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.0252\n",
      "Epoch 198/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0252\n",
      "Epoch 199/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0252\n",
      "Epoch 200/200\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.0252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14c269c40>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(data.x_train, data.x_train, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model = Sequential([\n",
    "    Dense(10, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    Dense(10, activation='selu', kernel_initializer='lecun_normal'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5638\n",
      "Epoch 2/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5626\n",
      "Epoch 3/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5620\n",
      "Epoch 4/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5618\n",
      "Epoch 5/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5613\n",
      "Epoch 6/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5601\n",
      "Epoch 7/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5596\n",
      "Epoch 8/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5596\n",
      "Epoch 9/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5584\n",
      "Epoch 10/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5578\n",
      "Epoch 11/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5570\n",
      "Epoch 12/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5561\n",
      "Epoch 13/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5557\n",
      "Epoch 14/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5551\n",
      "Epoch 15/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5546\n",
      "Epoch 16/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5538\n",
      "Epoch 17/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5533\n",
      "Epoch 18/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5528\n",
      "Epoch 19/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5517\n",
      "Epoch 20/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5508\n",
      "Epoch 21/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5509\n",
      "Epoch 22/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5498\n",
      "Epoch 23/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5493\n",
      "Epoch 24/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5485\n",
      "Epoch 25/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5475\n",
      "Epoch 26/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5474\n",
      "Epoch 27/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5470\n",
      "Epoch 28/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5463\n",
      "Epoch 29/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5455\n",
      "Epoch 30/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5448\n",
      "Epoch 31/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5439\n",
      "Epoch 32/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5437\n",
      "Epoch 33/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5434\n",
      "Epoch 34/500\n",
      "3/3 [==============================] - 0s 5ms/step - loss: 0.5421\n",
      "Epoch 35/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5414\n",
      "Epoch 36/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5410\n",
      "Epoch 37/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5406\n",
      "Epoch 38/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5399\n",
      "Epoch 39/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5400\n",
      "Epoch 40/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5385\n",
      "Epoch 41/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5383\n",
      "Epoch 42/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5375\n",
      "Epoch 43/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5372\n",
      "Epoch 44/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5370\n",
      "Epoch 45/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5369\n",
      "Epoch 46/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5363\n",
      "Epoch 47/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5349\n",
      "Epoch 48/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5347\n",
      "Epoch 49/500\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.5341\n",
      "Epoch 50/500\n",
      "3/3 [==============================] - 0s 6ms/step - loss: 0.5337\n",
      "Epoch 51/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5330\n",
      "Epoch 52/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5326\n",
      "Epoch 53/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5319\n",
      "Epoch 54/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5316\n",
      "Epoch 55/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.5308\n",
      "Epoch 56/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5311\n",
      "Epoch 57/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5297\n",
      "Epoch 58/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5295\n",
      "Epoch 59/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5288\n",
      "Epoch 60/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5284\n",
      "Epoch 61/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5277\n",
      "Epoch 62/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5270\n",
      "Epoch 63/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5266\n",
      "Epoch 64/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5266\n",
      "Epoch 65/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5259\n",
      "Epoch 66/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5251\n",
      "Epoch 67/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5246\n",
      "Epoch 68/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5240\n",
      "Epoch 69/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5235\n",
      "Epoch 70/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5240\n",
      "Epoch 71/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5223\n",
      "Epoch 72/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5222\n",
      "Epoch 73/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5214\n",
      "Epoch 74/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5208\n",
      "Epoch 75/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5202\n",
      "Epoch 76/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5207\n",
      "Epoch 77/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5193\n",
      "Epoch 78/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5186\n",
      "Epoch 79/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5182\n",
      "Epoch 80/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5177\n",
      "Epoch 81/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5173\n",
      "Epoch 82/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5167\n",
      "Epoch 83/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5166\n",
      "Epoch 84/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5157\n",
      "Epoch 85/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5155\n",
      "Epoch 86/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5146\n",
      "Epoch 87/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5141\n",
      "Epoch 88/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5136\n",
      "Epoch 89/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5132\n",
      "Epoch 90/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5126\n",
      "Epoch 91/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5121\n",
      "Epoch 92/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5119\n",
      "Epoch 93/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5114\n",
      "Epoch 94/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5106\n",
      "Epoch 95/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5101\n",
      "Epoch 96/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5097\n",
      "Epoch 97/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5100\n",
      "Epoch 98/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5088\n",
      "Epoch 99/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5083\n",
      "Epoch 100/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5079\n",
      "Epoch 101/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5088\n",
      "Epoch 102/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5066\n",
      "Epoch 103/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5066\n",
      "Epoch 104/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5072\n",
      "Epoch 105/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5058\n",
      "Epoch 106/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.5056\n",
      "Epoch 107/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5046\n",
      "Epoch 108/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5042\n",
      "Epoch 109/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5044\n",
      "Epoch 110/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5040\n",
      "Epoch 111/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.5027\n",
      "Epoch 112/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5030\n",
      "Epoch 113/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5020\n",
      "Epoch 114/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5015\n",
      "Epoch 115/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5010\n",
      "Epoch 116/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.5005\n",
      "Epoch 117/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4999\n",
      "Epoch 118/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4996\n",
      "Epoch 119/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4989\n",
      "Epoch 120/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4985\n",
      "Epoch 121/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4982\n",
      "Epoch 122/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4976\n",
      "Epoch 123/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4973\n",
      "Epoch 124/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4978\n",
      "Epoch 125/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4975\n",
      "Epoch 126/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4959\n",
      "Epoch 127/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4972\n",
      "Epoch 128/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4951\n",
      "Epoch 129/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4948\n",
      "Epoch 130/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4943\n",
      "Epoch 131/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4939\n",
      "Epoch 132/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4935\n",
      "Epoch 133/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4928\n",
      "Epoch 134/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4932\n",
      "Epoch 135/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4920\n",
      "Epoch 136/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4914\n",
      "Epoch 137/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4923\n",
      "Epoch 138/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4906\n",
      "Epoch 139/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4904\n",
      "Epoch 140/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4900\n",
      "Epoch 141/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4896\n",
      "Epoch 142/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4893\n",
      "Epoch 143/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4887\n",
      "Epoch 144/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4882\n",
      "Epoch 145/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4886\n",
      "Epoch 146/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4884\n",
      "Epoch 147/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4873\n",
      "Epoch 148/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4868\n",
      "Epoch 149/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4863\n",
      "Epoch 150/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4863\n",
      "Epoch 151/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4870\n",
      "Epoch 152/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4854\n",
      "Epoch 153/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4855\n",
      "Epoch 154/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4856\n",
      "Epoch 155/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4841\n",
      "Epoch 156/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4836\n",
      "Epoch 157/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4837\n",
      "Epoch 158/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4831\n",
      "Epoch 159/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4833\n",
      "Epoch 160/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4837\n",
      "Epoch 161/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4822\n",
      "Epoch 162/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4815\n",
      "Epoch 163/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4819\n",
      "Epoch 164/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4810\n",
      "Epoch 165/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4807\n",
      "Epoch 166/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4809\n",
      "Epoch 167/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4800\n",
      "Epoch 168/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4795\n",
      "Epoch 169/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4789\n",
      "Epoch 170/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4791\n",
      "Epoch 171/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4785\n",
      "Epoch 172/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4782\n",
      "Epoch 173/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4776\n",
      "Epoch 174/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4781\n",
      "Epoch 175/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4772\n",
      "Epoch 176/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4776\n",
      "Epoch 177/500\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.577 - 0s 4ms/step - loss: 0.4767\n",
      "Epoch 178/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4763\n",
      "Epoch 179/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4765\n",
      "Epoch 180/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4755\n",
      "Epoch 181/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4762\n",
      "Epoch 182/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4748\n",
      "Epoch 183/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4749\n",
      "Epoch 184/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4755\n",
      "Epoch 185/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4750\n",
      "Epoch 186/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4740\n",
      "Epoch 187/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4731\n",
      "Epoch 188/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4751\n",
      "Epoch 189/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4727\n",
      "Epoch 190/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4722\n",
      "Epoch 191/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4724\n",
      "Epoch 192/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4718\n",
      "Epoch 193/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4713\n",
      "Epoch 194/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4712\n",
      "Epoch 195/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4706\n",
      "Epoch 196/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4704\n",
      "Epoch 197/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4708\n",
      "Epoch 198/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4701\n",
      "Epoch 199/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4699\n",
      "Epoch 200/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4692\n",
      "Epoch 201/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4691\n",
      "Epoch 202/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4683\n",
      "Epoch 203/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4693\n",
      "Epoch 204/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4685\n",
      "Epoch 205/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4691\n",
      "Epoch 206/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4671\n",
      "Epoch 207/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4679\n",
      "Epoch 208/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4665\n",
      "Epoch 209/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4667\n",
      "Epoch 210/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4663\n",
      "Epoch 211/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4662\n",
      "Epoch 212/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4655\n",
      "Epoch 213/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4655\n",
      "Epoch 214/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4651\n",
      "Epoch 215/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4650\n",
      "Epoch 216/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4645\n",
      "Epoch 217/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4645\n",
      "Epoch 218/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4637\n",
      "Epoch 219/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4636\n",
      "Epoch 220/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4641\n",
      "Epoch 221/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4629\n",
      "Epoch 222/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4623\n",
      "Epoch 223/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4631\n",
      "Epoch 224/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4623\n",
      "Epoch 225/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4628\n",
      "Epoch 226/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4626\n",
      "Epoch 227/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4617\n",
      "Epoch 228/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4621\n",
      "Epoch 229/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4626\n",
      "Epoch 230/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4607\n",
      "Epoch 231/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4603\n",
      "Epoch 232/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4599\n",
      "Epoch 233/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4593\n",
      "Epoch 234/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4594\n",
      "Epoch 235/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4594\n",
      "Epoch 236/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4589\n",
      "Epoch 237/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4588\n",
      "Epoch 238/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4586\n",
      "Epoch 239/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4578\n",
      "Epoch 240/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4579\n",
      "Epoch 241/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4587\n",
      "Epoch 242/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4573\n",
      "Epoch 243/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4572\n",
      "Epoch 244/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4565\n",
      "Epoch 245/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4570\n",
      "Epoch 246/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4569\n",
      "Epoch 247/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4563\n",
      "Epoch 248/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4568\n",
      "Epoch 249/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4554\n",
      "Epoch 250/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4551\n",
      "Epoch 251/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4559\n",
      "Epoch 252/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4548\n",
      "Epoch 253/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4545\n",
      "Epoch 254/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4546\n",
      "Epoch 255/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4552\n",
      "Epoch 256/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4548\n",
      "Epoch 257/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4546\n",
      "Epoch 258/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4553\n",
      "Epoch 259/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4534\n",
      "Epoch 260/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4524\n",
      "Epoch 261/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4525\n",
      "Epoch 262/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4518\n",
      "Epoch 263/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4533\n",
      "Epoch 264/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4516\n",
      "Epoch 265/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4519\n",
      "Epoch 266/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4512\n",
      "Epoch 267/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4509\n",
      "Epoch 268/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4507\n",
      "Epoch 269/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4501\n",
      "Epoch 270/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4501\n",
      "Epoch 271/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4502\n",
      "Epoch 272/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4498\n",
      "Epoch 273/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4515\n",
      "Epoch 274/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4510\n",
      "Epoch 275/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4492\n",
      "Epoch 276/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4499\n",
      "Epoch 277/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4481\n",
      "Epoch 278/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4475\n",
      "Epoch 279/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4484\n",
      "Epoch 280/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4484\n",
      "Epoch 281/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4481\n",
      "Epoch 282/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4472\n",
      "Epoch 283/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4483\n",
      "Epoch 284/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4480\n",
      "Epoch 285/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4469\n",
      "Epoch 286/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4468\n",
      "Epoch 287/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4460\n",
      "Epoch 288/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4468\n",
      "Epoch 289/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4465\n",
      "Epoch 290/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4459\n",
      "Epoch 291/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4460\n",
      "Epoch 292/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4450\n",
      "Epoch 293/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4445\n",
      "Epoch 294/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4448\n",
      "Epoch 295/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4445\n",
      "Epoch 296/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4447\n",
      "Epoch 297/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4439\n",
      "Epoch 298/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4432\n",
      "Epoch 299/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4431\n",
      "Epoch 300/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4445\n",
      "Epoch 301/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4431\n",
      "Epoch 302/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4429\n",
      "Epoch 303/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4417\n",
      "Epoch 304/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4428\n",
      "Epoch 305/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4434\n",
      "Epoch 306/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4421\n",
      "Epoch 307/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4414\n",
      "Epoch 308/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4407\n",
      "Epoch 309/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4407\n",
      "Epoch 310/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4405\n",
      "Epoch 311/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4412\n",
      "Epoch 312/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4404\n",
      "Epoch 313/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4413\n",
      "Epoch 314/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4397\n",
      "Epoch 315/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4398\n",
      "Epoch 316/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4391\n",
      "Epoch 317/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4386\n",
      "Epoch 318/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4383\n",
      "Epoch 319/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4400\n",
      "Epoch 320/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4385\n",
      "Epoch 321/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4377\n",
      "Epoch 322/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4378\n",
      "Epoch 323/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4379\n",
      "Epoch 324/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4370\n",
      "Epoch 325/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4370\n",
      "Epoch 326/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4363\n",
      "Epoch 327/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4378\n",
      "Epoch 328/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4369\n",
      "Epoch 329/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4361\n",
      "Epoch 330/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4363\n",
      "Epoch 331/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4357\n",
      "Epoch 332/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4357\n",
      "Epoch 333/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4352\n",
      "Epoch 334/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4351\n",
      "Epoch 335/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4352\n",
      "Epoch 336/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4347\n",
      "Epoch 337/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4349\n",
      "Epoch 338/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4342\n",
      "Epoch 339/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4340\n",
      "Epoch 340/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4336\n",
      "Epoch 341/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4339\n",
      "Epoch 342/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4332\n",
      "Epoch 343/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4329\n",
      "Epoch 344/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4328\n",
      "Epoch 345/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4328\n",
      "Epoch 346/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4323\n",
      "Epoch 347/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4317\n",
      "Epoch 348/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4322\n",
      "Epoch 349/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4321\n",
      "Epoch 350/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4320\n",
      "Epoch 351/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4313\n",
      "Epoch 352/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4332\n",
      "Epoch 353/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4323\n",
      "Epoch 354/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4304\n",
      "Epoch 355/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4314\n",
      "Epoch 356/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4311\n",
      "Epoch 357/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4305\n",
      "Epoch 358/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4323\n",
      "Epoch 359/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4298\n",
      "Epoch 360/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4300\n",
      "Epoch 361/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4307\n",
      "Epoch 362/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4295\n",
      "Epoch 363/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4298\n",
      "Epoch 364/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4291\n",
      "Epoch 365/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4292\n",
      "Epoch 366/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4291\n",
      "Epoch 367/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4290\n",
      "Epoch 368/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4287\n",
      "Epoch 369/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4282\n",
      "Epoch 370/500\n",
      "3/3 [==============================] - 0s 4ms/step - loss: 0.4272\n",
      "Epoch 371/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4270\n",
      "Epoch 372/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4273\n",
      "Epoch 373/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4266\n",
      "Epoch 374/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4268\n",
      "Epoch 375/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4265\n",
      "Epoch 376/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4265\n",
      "Epoch 377/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4270\n",
      "Epoch 378/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4262\n",
      "Epoch 379/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4254\n",
      "Epoch 380/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4260\n",
      "Epoch 381/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4256\n",
      "Epoch 382/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4249\n",
      "Epoch 383/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4255\n",
      "Epoch 384/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4252\n",
      "Epoch 385/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4246\n",
      "Epoch 386/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4251\n",
      "Epoch 387/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4263\n",
      "Epoch 388/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4244\n",
      "Epoch 389/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4244\n",
      "Epoch 390/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4238\n",
      "Epoch 391/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4235\n",
      "Epoch 392/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4238\n",
      "Epoch 393/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4231\n",
      "Epoch 394/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4240\n",
      "Epoch 395/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4234\n",
      "Epoch 396/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4236\n",
      "Epoch 397/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4233\n",
      "Epoch 398/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4227\n",
      "Epoch 399/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4225\n",
      "Epoch 400/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4223\n",
      "Epoch 401/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4213\n",
      "Epoch 402/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4210\n",
      "Epoch 403/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4218\n",
      "Epoch 404/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4214\n",
      "Epoch 405/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4211\n",
      "Epoch 406/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4202\n",
      "Epoch 407/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4202\n",
      "Epoch 408/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4201\n",
      "Epoch 409/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4199\n",
      "Epoch 410/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4203\n",
      "Epoch 411/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4199\n",
      "Epoch 412/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4211\n",
      "Epoch 413/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4207\n",
      "Epoch 414/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4189\n",
      "Epoch 415/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4191\n",
      "Epoch 416/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4187\n",
      "Epoch 417/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4187\n",
      "Epoch 418/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4195\n",
      "Epoch 419/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4173\n",
      "Epoch 420/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4187\n",
      "Epoch 421/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4184\n",
      "Epoch 422/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4175\n",
      "Epoch 423/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4169\n",
      "Epoch 424/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4228\n",
      "Epoch 425/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4184\n",
      "Epoch 426/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4162\n",
      "Epoch 427/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4185\n",
      "Epoch 428/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4181\n",
      "Epoch 429/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4187\n",
      "Epoch 430/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4161\n",
      "Epoch 431/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4161\n",
      "Epoch 432/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4161\n",
      "Epoch 433/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4161\n",
      "Epoch 434/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4152\n",
      "Epoch 435/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4148\n",
      "Epoch 436/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4143\n",
      "Epoch 437/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4171\n",
      "Epoch 438/500\n",
      "3/3 [==============================] - 0s 3ms/step - loss: 0.4149\n",
      "Epoch 439/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4144\n",
      "Epoch 440/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4134\n",
      "Epoch 441/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4142\n",
      "Epoch 442/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4130\n",
      "Epoch 443/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4128\n",
      "Epoch 444/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4124\n",
      "Epoch 445/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4128\n",
      "Epoch 446/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4125\n",
      "Epoch 447/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4124\n",
      "Epoch 448/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4123\n",
      "Epoch 449/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4117\n",
      "Epoch 450/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4111\n",
      "Epoch 451/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4111\n",
      "Epoch 452/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4114\n",
      "Epoch 453/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4108\n",
      "Epoch 454/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4104\n",
      "Epoch 455/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4108\n",
      "Epoch 456/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4103\n",
      "Epoch 457/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4125\n",
      "Epoch 458/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4104\n",
      "Epoch 459/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4110\n",
      "Epoch 460/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4095\n",
      "Epoch 461/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4097\n",
      "Epoch 462/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4086\n",
      "Epoch 463/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4112\n",
      "Epoch 464/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4085\n",
      "Epoch 465/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4093\n",
      "Epoch 466/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4078\n",
      "Epoch 467/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4074\n",
      "Epoch 468/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4081\n",
      "Epoch 469/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4076\n",
      "Epoch 470/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4080\n",
      "Epoch 471/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4073\n",
      "Epoch 472/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4082\n",
      "Epoch 473/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4082\n",
      "Epoch 474/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4060\n",
      "Epoch 475/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4056\n",
      "Epoch 476/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4056\n",
      "Epoch 477/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4062\n",
      "Epoch 478/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4049\n",
      "Epoch 479/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4046\n",
      "Epoch 480/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4053\n",
      "Epoch 481/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4049\n",
      "Epoch 482/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4048\n",
      "Epoch 483/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4044\n",
      "Epoch 484/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4040\n",
      "Epoch 485/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4039\n",
      "Epoch 486/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4037\n",
      "Epoch 487/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4041\n",
      "Epoch 488/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4028\n",
      "Epoch 489/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4025\n",
      "Epoch 490/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4023\n",
      "Epoch 491/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4019\n",
      "Epoch 492/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4019\n",
      "Epoch 493/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4015\n",
      "Epoch 494/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4012\n",
      "Epoch 495/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4013\n",
      "Epoch 496/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4010\n",
      "Epoch 497/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4010\n",
      "Epoch 498/500\n",
      "3/3 [==============================] - 0s 1ms/step - loss: 0.4004\n",
      "Epoch 499/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4003\n",
      "Epoch 500/500\n",
      "3/3 [==============================] - 0s 2ms/step - loss: 0.4013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1501bbe80>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = encoder(data.x_train)\n",
    "main_model.fit(encoded, data.y_train, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = main_model.predict_classes(encoder(data.x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0], dtype=int32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6363636363636364,\n",
       " 0.47368421052631576,\n",
       " 0.5666666666666667,\n",
       " 0.5185185185185185]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ClassificationMetrics(data.y_test, preds)\n",
    "metrics.add_metrics(['pd', 'pf', 'accuracy', 'f1'])\n",
    "metrics.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6363636363636364, 0.3684210526315789, 0.6333333333333333, 0.56]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = ClassificationMetrics(data.y_test, preds)\n",
    "metrics.add_metrics(['pd', 'pf', 'accuracy', 'f1'])\n",
    "metrics.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
