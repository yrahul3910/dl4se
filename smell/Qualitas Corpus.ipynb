{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raise_utils.data import DataLoader, Data\n",
    "from raise_utils.learners import FeedforwardDL, DecisionTree, BiasedSVM, LogisticRegressionClassifier, NaiveBayes, RandomForest\n",
    "from raise_utils.hyperparams import DODGE\n",
    "from raise_utils.transform import Transform\n",
    "from sklearn.decomposition import PCA\n",
    "from raise_utils.metrics import ClassificationMetrics\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./binary_class/data-class.csv')\n",
    "df.drop(df.columns[:4], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.where(df != '?', np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420, 62)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392, 62)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Data(*train_test_split(df[df.columns[:-1]], df[df.columns[-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "306    False\n",
       "373     True\n",
       "179    False\n",
       "343    False\n",
       "117    False\n",
       "Name: is_data_class, dtype: bool"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35714285714285715"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(data.y_train) / len(data.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_runs\": 20,\n",
    "    \"transforms\": [\"normalize\", \"standardize\", \"robust\", \"maxabs\", \"minmax\"] * 30,\n",
    "    \"metrics\": [\"d2h\", \"pd\", \"pf\", \"accuracy\", \"auc\"],\n",
    "    \"random\": True,\n",
    "    \"learners\": [],\n",
    "    \"log_path\": \"./\",\n",
    "    \"data\": [data],\n",
    "    \"name\": \"dataclass\"\n",
    "}\n",
    "for _ in range(50):\n",
    "    config[\"learners\"].append(FeedforwardDL(weighted=True,\n",
    "                                            wfo=True,\n",
    "                                            random={'n_layers': (2, 6),\n",
    "                                                    'n_units': (10, 70)}, \n",
    "                                            n_units=10, n_epochs=50, optimizer='adam'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145be8970>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 45, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145bd1cd0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 16, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145bd1c10>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 3, 'n_units': 19, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145bd1250>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 6, 'n_units': 32, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145bd12b0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 70, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145bce2e0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 30, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145136e50>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 56, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x1451369d0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 6, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x1457f8130>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 18, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145136250>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 12, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145ee0190>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145ee0fd0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 18, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145d6eb50>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 3, 'n_units': 29, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145d6eeb0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 58, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145ee2e20>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 25, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145ee2f70>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 35, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145ee2ee0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 3, 'n_units': 35, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145aebc70>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 12, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145aeb2e0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 6, 'n_units': 49, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145d7ed00>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 3, 'n_units': 49, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145d7e940>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 22, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145d7e0a0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 6, 'n_units': 58, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145efd4c0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 70, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x14583b160>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 40, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x14512f160>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 25, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145ef91f0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 3, 'n_units': 56, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145ef9f70>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 3, 'n_units': 60, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145ef9550>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 13, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145d680a0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145d68610>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 3, 'n_units': 67, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145d681f0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 6, 'n_units': 38, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145d5e850>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 65, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145d5eb80>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 68, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145d5e700>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 6, 'n_units': 10, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145ad45b0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 51, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145ad4790>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 57, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145ad4d90>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 3, 'n_units': 41, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145ecef10>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 45, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145ece220>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 35, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x14540d430>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 11, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x14540ddf0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 69, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x14540de20>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 67, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x14540d730>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 2, 'n_units': 58, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145d766a0>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 33, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145d76850>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 21, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145683c40>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 4, 'n_units': 66, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145d1b670>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 22, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145d1b040>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 47, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145d3e850>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 3, 'n_units': 19, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'activation': 'relu', 'hooks': None, 'learner': <raise_utils.learners.feedforward.FeedforwardDL object at 0x145d3e130>, 'loss': 'binary_crossentropy', 'n_epochs': 50, 'n_layers': 5, 'n_units': 54, 'name': 'rf', 'optimizer': 'adam', 'random': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'random_map': {'n_layers': (2, 6), 'n_units': (10, 70)}, 'verbose': 1, 'weighted': True, 'wfo': True, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minmaxR|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 1.2981 - val_loss: 1.2968\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.2938 - val_loss: 1.3062\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.2902 - val_loss: 1.3138\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.2866 - val_loss: 1.3196\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 1.2825 - val_loss: 1.3226\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.2779 - val_loss: 1.3245\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.2725 - val_loss: 1.3253\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.2664 - val_loss: 1.3243\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.2592 - val_loss: 1.3202\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.2506 - val_loss: 1.3114\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.2407 - val_loss: 1.2970\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.2286 - val_loss: 1.2766\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.2146 - val_loss: 1.2517\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.1982 - val_loss: 1.2223\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.1801 - val_loss: 1.1873\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.1602 - val_loss: 1.1493\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.1373 - val_loss: 1.1074\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.1134 - val_loss: 1.0589\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.0867 - val_loss: 1.0093\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.0579 - val_loss: 0.9618\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.0274 - val_loss: 0.9136\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 21ms/step - loss: 0.9962 - val_loss: 0.8633\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 19ms/step - loss: 0.9632 - val_loss: 0.8121\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.9296 - val_loss: 0.7574\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.8948 - val_loss: 0.7064\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.8597 - val_loss: 0.6580\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.8232 - val_loss: 0.6126\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.7865 - val_loss: 0.5679\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.7488 - val_loss: 0.5252\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7102 - val_loss: 0.4833\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6719 - val_loss: 0.4481\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6337 - val_loss: 0.4147\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.5965 - val_loss: 0.3844\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5605 - val_loss: 0.3536\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5254 - val_loss: 0.3246\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.4916 - val_loss: 0.2971\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4591 - val_loss: 0.2822\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4286 - val_loss: 0.2689\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3994 - val_loss: 0.2545\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3724 - val_loss: 0.2418\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3469 - val_loss: 0.2205\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3233 - val_loss: 0.2036\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3021 - val_loss: 0.1895\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2826 - val_loss: 0.1825\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2647 - val_loss: 0.1804\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.2485 - val_loss: 0.1764\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.2340 - val_loss: 0.1665\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2203 - val_loss: 0.1538\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2079 - val_loss: 0.1402\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1968 - val_loss: 0.1290\n",
      "1\n",
      "robustj|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 3.1804 - val_loss: 4.5507\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 2.1788 - val_loss: 2.5210\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.5129 - val_loss: 1.5283\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.1865 - val_loss: 1.1060\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.0321 - val_loss: 0.9209\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.9474 - val_loss: 0.8430\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.8846 - val_loss: 0.8115\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.8316 - val_loss: 0.8022\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.7822 - val_loss: 0.7999\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.7351 - val_loss: 0.7980\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6882 - val_loss: 0.7895\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6434 - val_loss: 0.7741\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5996 - val_loss: 0.7510\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5571 - val_loss: 0.7228\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.5179 - val_loss: 0.6894\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4820 - val_loss: 0.6483\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.4471 - val_loss: 0.6037\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4144 - val_loss: 0.5595\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3836 - val_loss: 0.5173\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3562 - val_loss: 0.4761\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3296 - val_loss: 0.4386\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3059 - val_loss: 0.4047\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2836 - val_loss: 0.3740\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2624 - val_loss: 0.3462\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2430 - val_loss: 0.3213\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2256 - val_loss: 0.2985\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2097 - val_loss: 0.2765\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1952 - val_loss: 0.2546\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1819 - val_loss: 0.2326\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1698 - val_loss: 0.2103\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1582 - val_loss: 0.1911\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1482 - val_loss: 0.1742\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1383 - val_loss: 0.1599\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1293 - val_loss: 0.1467\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1211 - val_loss: 0.1342\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1134 - val_loss: 0.1227\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1060 - val_loss: 0.1125\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0990 - val_loss: 0.1025\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0925 - val_loss: 0.0934\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0863 - val_loss: 0.0844\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0805 - val_loss: 0.0758\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0747 - val_loss: 0.0678\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0696 - val_loss: 0.0605\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0643 - val_loss: 0.0545\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0596 - val_loss: 0.0491\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0552 - val_loss: 0.0449\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0511 - val_loss: 0.0412\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0471 - val_loss: 0.0377\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0434 - val_loss: 0.0340\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0401 - val_loss: 0.0307\n",
      "2\n",
      "standardizev|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 1.3265 - val_loss: 1.5620\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.3020 - val_loss: 1.5383\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.2798 - val_loss: 1.5211\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.2587 - val_loss: 1.5093\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.2386 - val_loss: 1.5016\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.2180 - val_loss: 1.4982\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.1975 - val_loss: 1.4974\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.1752 - val_loss: 1.4990\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.1522 - val_loss: 1.5026\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.1280 - val_loss: 1.5058\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.1020 - val_loss: 1.5099\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.0731 - val_loss: 1.5146\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.0422 - val_loss: 1.5213\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.0093 - val_loss: 1.5279\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.9757 - val_loss: 1.5344\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.9432 - val_loss: 1.5378\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.9115 - val_loss: 1.5357\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8808 - val_loss: 1.5276\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8512 - val_loss: 1.5135\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8218 - val_loss: 1.4935\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7938 - val_loss: 1.4684\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7666 - val_loss: 1.4395\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7407 - val_loss: 1.4096\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.7162 - val_loss: 1.3828\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6922 - val_loss: 1.3576\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.6694 - val_loss: 1.3310\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6474 - val_loss: 1.3043\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6259 - val_loss: 1.2758\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6056 - val_loss: 1.2471\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5856 - val_loss: 1.2190\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5662 - val_loss: 1.1892\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5469 - val_loss: 1.1579\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.5279 - val_loss: 1.1239\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5085 - val_loss: 1.0867\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4894 - val_loss: 1.0482\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4704 - val_loss: 1.0084\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4518 - val_loss: 0.9681\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4335 - val_loss: 0.9260\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.4148 - val_loss: 0.8833\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3965 - val_loss: 0.8404\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3783 - val_loss: 0.7981\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3600 - val_loss: 0.7576\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3420 - val_loss: 0.7180\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.3244 - val_loss: 0.6783\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3069 - val_loss: 0.6385\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2901 - val_loss: 0.5974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2735 - val_loss: 0.5567\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2576 - val_loss: 0.5167\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2423 - val_loss: 0.4797\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2273 - val_loss: 0.4447\n",
      "3\n",
      "robustq|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_15 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "3/3 [==============================] - 0s 33ms/step - loss: 244126863851520.0000 - val_loss: 118690087436288.0000\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 29386457743360.0000 - val_loss: 245029289984.0000\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 13028484448256.0000 - val_loss: 71376560128.0000\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 11445688336384.0000 - val_loss: 813929529344.0000\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 9617134321664.0000 - val_loss: 2365298049024.0000\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 10308284317696.0000 - val_loss: 3229409083392.0000\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 10225945935872.0000 - val_loss: 3487446859776.0000\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 9502466244608.0000 - val_loss: 3473757437952.0000\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 8348745334784.0000 - val_loss: 3316873953280.0000\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 6965975580672.0000 - val_loss: 3070776836096.0000\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 5291286986752.0000 - val_loss: 2769498931200.0000\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 3328831389696.0000 - val_loss: 6065842814976.0000\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 3325812801536.0000 - val_loss: 7224266588160.0000\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 2467629105152.0000 - val_loss: 1910519758848.0000\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1655027793920.0000 - val_loss: 0.0000e+00\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1718420242432.0000 - val_loss: 750477443072.0000\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1751832461312.0000 - val_loss: 998048858112.0000\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1423435628544.0000 - val_loss: 60783726592.0000\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1866030383104.0000 - val_loss: 363554504704.0000\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1775302606848.0000 - val_loss: 726321004544.0000\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1656477319168.0000 - val_loss: 789398880256.0000\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1374457692160.0000 - val_loss: 941412122624.0000\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1158055723008.0000 - val_loss: 350835408896.0000\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 988596273152.0000 - val_loss: 259486121984.0000\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 890671726592.0000 - val_loss: 121204744192.0000\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 712830877696.0000 - val_loss: 1018223001600.0000\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 834282258432.0000 - val_loss: 798580408320.0000\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 712630009856.0000 - val_loss: 426637492224.0000\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 588497616896.0000 - val_loss: 528314990592.0000\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 416648298496.0000 - val_loss: 224592150528.0000\n",
      "4\n",
      "standardizew|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_22 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "3/3 [==============================] - 0s 31ms/step - loss: 1.2968 - val_loss: 1.5868\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 1.2535 - val_loss: 1.5449\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.2153 - val_loss: 1.5132\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.1783 - val_loss: 1.4904\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.1417 - val_loss: 1.4772\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.1042 - val_loss: 1.4705\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.0653 - val_loss: 1.4672\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.0249 - val_loss: 1.4638\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 8ms/step - loss: 0.9830 - val_loss: 1.4586\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.9383 - val_loss: 1.4506\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8926 - val_loss: 1.4395\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.8473 - val_loss: 1.4268\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.8014 - val_loss: 1.4086\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.7577 - val_loss: 1.3803\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.7134 - val_loss: 1.3322\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6693 - val_loss: 1.2660\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6253 - val_loss: 1.1848\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.5796 - val_loss: 1.0964\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5353 - val_loss: 1.0072\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4938 - val_loss: 0.9193\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4540 - val_loss: 0.8394\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4160 - val_loss: 0.7715\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.3806 - val_loss: 0.7076\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.3475 - val_loss: 0.6467\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.3170 - val_loss: 0.5909\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2880 - val_loss: 0.5337\n",
      "Epoch 27/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2618 - val_loss: 0.4767\n",
      "Epoch 28/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2363 - val_loss: 0.4234\n",
      "Epoch 29/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2123 - val_loss: 0.3752\n",
      "Epoch 30/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1901 - val_loss: 0.3367\n",
      "Epoch 31/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1696 - val_loss: 0.2938\n",
      "Epoch 32/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.1506 - val_loss: 0.2495\n",
      "Epoch 33/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1337 - val_loss: 0.2104\n",
      "Epoch 34/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1195 - val_loss: 0.1822\n",
      "Epoch 35/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1063 - val_loss: 0.1595\n",
      "Epoch 36/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0945 - val_loss: 0.1391\n",
      "Epoch 37/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0843 - val_loss: 0.1234\n",
      "Epoch 38/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0754 - val_loss: 0.1102\n",
      "Epoch 39/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0680 - val_loss: 0.0971\n",
      "Epoch 40/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0615 - val_loss: 0.0862\n",
      "Epoch 41/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0559 - val_loss: 0.0744\n",
      "Epoch 42/50\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0509 - val_loss: 0.0638\n",
      "Epoch 43/50\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 0.0472 - val_loss: 0.0543\n",
      "Epoch 44/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0437 - val_loss: 0.0503\n",
      "Epoch 45/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0404 - val_loss: 0.0482\n",
      "Epoch 46/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0370 - val_loss: 0.0450\n",
      "Epoch 47/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0340 - val_loss: 0.0397\n",
      "Epoch 48/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0315 - val_loss: 0.0358\n",
      "Epoch 49/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0292 - val_loss: 0.0342\n",
      "Epoch 50/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0271 - val_loss: 0.0331\n",
      "5\n",
      "normalizew|rf\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:Layer dense_28 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "3/3 [==============================] - 0s 36ms/step - loss: 1.2980 - val_loss: 1.3174\n",
      "Epoch 2/50\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.2913 - val_loss: 1.3380\n",
      "Epoch 3/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.2815 - val_loss: 1.3558\n",
      "Epoch 4/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.2684 - val_loss: 1.3742\n",
      "Epoch 5/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.2503 - val_loss: 1.3879\n",
      "Epoch 6/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.2250 - val_loss: 1.3995\n",
      "Epoch 7/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 1.1911 - val_loss: 1.4008\n",
      "Epoch 8/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 1.1448 - val_loss: 1.3791\n",
      "Epoch 9/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.0815 - val_loss: 1.3338\n",
      "Epoch 10/50\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.9988 - val_loss: 1.2606\n",
      "Epoch 11/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.8951 - val_loss: 1.1628\n",
      "Epoch 12/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.7715 - val_loss: 1.0084\n",
      "Epoch 13/50\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.6329 - val_loss: 0.8729\n",
      "Epoch 14/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4925 - val_loss: 0.6514\n",
      "Epoch 15/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.3584 - val_loss: 0.4378\n",
      "Epoch 16/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.2488 - val_loss: 0.2868\n",
      "Epoch 17/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1681 - val_loss: 0.1552\n",
      "Epoch 18/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1175 - val_loss: 0.0983\n",
      "Epoch 19/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0868 - val_loss: 0.0485\n",
      "Epoch 20/50\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0699 - val_loss: 0.0451\n",
      "Epoch 21/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0640 - val_loss: 0.0320\n",
      "Epoch 22/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0509 - val_loss: 0.0161\n",
      "Epoch 23/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0417 - val_loss: 0.0319\n",
      "Epoch 24/50\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.0295 - val_loss: 0.0184\n",
      "Epoch 25/50\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0224 - val_loss: 0.0112\n",
      "Epoch 26/50\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0200 - val_loss: 0.0177\n",
      "Epoch 27/50\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0210"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-bfbcb350bd6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdodge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDODGE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdodge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/raise_utils/hyperparams/dodge.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m                     model.set_data(data.x_train, data.y_train,\n\u001b[1;32m     74\u001b[0m                                    data.x_test, data.y_test)\n\u001b[0;32m---> 75\u001b[0;31m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                     \u001b[0;31m# Run post-training hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/raise_utils/learners/feedforward.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     93\u001b[0m                     \u001b[0mhook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         self.model.fit(np.array(self.x_train), np.array(self.y_train), batch_size=512, epochs=self.n_epochs,\n\u001b[0m\u001b[1;32m     96\u001b[0m                        validation_split=0.2, verbose=self.verbose, callbacks=[\n\u001b[1;32m     97\u001b[0m             \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dodge = DODGE(config)\n",
    "dodge.optimize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transform('standardize').apply(data)  # normalize does not work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_inp = Input((61,))\n",
    "enc_int = Dense(30, activation='relu')(enc_inp)\n",
    "enc_out = Dense(10, activation='relu')(enc_int)\n",
    "\n",
    "dec_int = Dense(30, activation='relu')(enc_out)\n",
    "dec_out = Dense(61, activation='relu')(dec_int)\n",
    "\n",
    "autoencoder = Model(inputs=enc_inp, outputs=dec_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=enc_inp, outputs=enc_out)\n",
    "dec_inp = Input((10,))\n",
    "decoder = Model(inputs=dec_inp, outputs=autoencoder.layers[-1](autoencoder.layers[-2](dec_inp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.9648\n",
      "Epoch 2/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.9323\n",
      "Epoch 3/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.9039\n",
      "Epoch 4/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.8779\n",
      "Epoch 5/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.8510\n",
      "Epoch 6/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.8316\n",
      "Epoch 7/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.8100\n",
      "Epoch 8/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.7916\n",
      "Epoch 9/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.7774\n",
      "Epoch 10/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.7656\n",
      "Epoch 11/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.7526\n",
      "Epoch 12/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.7420\n",
      "Epoch 13/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.7315\n",
      "Epoch 14/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.7224\n",
      "Epoch 15/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.7133\n",
      "Epoch 16/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.7010\n",
      "Epoch 17/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.6912\n",
      "Epoch 18/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6821\n",
      "Epoch 19/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6745\n",
      "Epoch 20/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6665\n",
      "Epoch 21/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6585\n",
      "Epoch 22/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6516\n",
      "Epoch 23/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6436\n",
      "Epoch 24/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6354\n",
      "Epoch 25/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6278\n",
      "Epoch 26/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6208\n",
      "Epoch 27/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.6124\n",
      "Epoch 28/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.6025\n",
      "Epoch 29/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.5919\n",
      "Epoch 30/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.5832\n",
      "Epoch 31/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.5720\n",
      "Epoch 32/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.5644\n",
      "Epoch 33/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.5539\n",
      "Epoch 34/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.5465\n",
      "Epoch 35/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.5402\n",
      "Epoch 36/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.5347\n",
      "Epoch 37/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.5296\n",
      "Epoch 38/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.5254\n",
      "Epoch 39/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.5198\n",
      "Epoch 40/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.5157\n",
      "Epoch 41/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.5104\n",
      "Epoch 42/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.5058\n",
      "Epoch 43/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.5016\n",
      "Epoch 44/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4973\n",
      "Epoch 45/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4940\n",
      "Epoch 46/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4901\n",
      "Epoch 47/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4868\n",
      "Epoch 48/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4887\n",
      "Epoch 49/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4809\n",
      "Epoch 50/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4756\n",
      "Epoch 51/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4723\n",
      "Epoch 52/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4685\n",
      "Epoch 53/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.4653\n",
      "Epoch 54/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4631\n",
      "Epoch 55/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.4615\n",
      "Epoch 56/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.4562\n",
      "Epoch 57/500\n",
      "10/10 [==============================] - 0s 932us/step - loss: 0.4532\n",
      "Epoch 58/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4498\n",
      "Epoch 59/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4475\n",
      "Epoch 60/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.4460\n",
      "Epoch 61/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4435\n",
      "Epoch 62/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.4396\n",
      "Epoch 63/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4375\n",
      "Epoch 64/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4349\n",
      "Epoch 65/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4337\n",
      "Epoch 66/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4307\n",
      "Epoch 67/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4282\n",
      "Epoch 68/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.4255\n",
      "Epoch 69/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.4242\n",
      "Epoch 70/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.4232\n",
      "Epoch 71/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.4203\n",
      "Epoch 72/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.4187\n",
      "Epoch 73/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.4162\n",
      "Epoch 74/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.4148\n",
      "Epoch 75/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.4128\n",
      "Epoch 76/500\n",
      "10/10 [==============================] - 0s 927us/step - loss: 0.4113\n",
      "Epoch 77/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.4084\n",
      "Epoch 78/500\n",
      "10/10 [==============================] - 0s 1000us/step - loss: 0.4074\n",
      "Epoch 79/500\n",
      "10/10 [==============================] - 0s 987us/step - loss: 0.4054\n",
      "Epoch 80/500\n",
      "10/10 [==============================] - 0s 990us/step - loss: 0.4076\n",
      "Epoch 81/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.4021\n",
      "Epoch 82/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.4008\n",
      "Epoch 83/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3991\n",
      "Epoch 84/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3971\n",
      "Epoch 85/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3955\n",
      "Epoch 86/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3945\n",
      "Epoch 87/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3935\n",
      "Epoch 88/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3922\n",
      "Epoch 89/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3909\n",
      "Epoch 90/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3904\n",
      "Epoch 91/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3901\n",
      "Epoch 92/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3885\n",
      "Epoch 93/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3887\n",
      "Epoch 94/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3869\n",
      "Epoch 95/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3859\n",
      "Epoch 96/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3856\n",
      "Epoch 97/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3849\n",
      "Epoch 98/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3836\n",
      "Epoch 99/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3827\n",
      "Epoch 100/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3848\n",
      "Epoch 101/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3828\n",
      "Epoch 102/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3806\n",
      "Epoch 103/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3806\n",
      "Epoch 104/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3800\n",
      "Epoch 105/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3785\n",
      "Epoch 106/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3775\n",
      "Epoch 107/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3776\n",
      "Epoch 108/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3760\n",
      "Epoch 109/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3757\n",
      "Epoch 110/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3749\n",
      "Epoch 111/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3744\n",
      "Epoch 112/500\n",
      "10/10 [==============================] - 0s 985us/step - loss: 0.3735\n",
      "Epoch 113/500\n",
      "10/10 [==============================] - 0s 941us/step - loss: 0.3728\n",
      "Epoch 114/500\n",
      "10/10 [==============================] - 0s 973us/step - loss: 0.3720\n",
      "Epoch 115/500\n",
      "10/10 [==============================] - 0s 971us/step - loss: 0.3715\n",
      "Epoch 116/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3713\n",
      "Epoch 117/500\n",
      "10/10 [==============================] - 0s 978us/step - loss: 0.3711\n",
      "Epoch 118/500\n",
      "10/10 [==============================] - 0s 982us/step - loss: 0.3713\n",
      "Epoch 119/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3709\n",
      "Epoch 120/500\n",
      "10/10 [==============================] - 0s 961us/step - loss: 0.3695\n",
      "Epoch 121/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3677\n",
      "Epoch 122/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3686\n",
      "Epoch 123/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3670\n",
      "Epoch 124/500\n",
      "10/10 [==============================] - 0s 899us/step - loss: 0.3665\n",
      "Epoch 125/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3661\n",
      "Epoch 126/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3661\n",
      "Epoch 127/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3655\n",
      "Epoch 128/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3649\n",
      "Epoch 129/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3644\n",
      "Epoch 130/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3635\n",
      "Epoch 131/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3630\n",
      "Epoch 132/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3626\n",
      "Epoch 133/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3628\n",
      "Epoch 134/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3626\n",
      "Epoch 135/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3616\n",
      "Epoch 136/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3614\n",
      "Epoch 137/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3607\n",
      "Epoch 138/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3603\n",
      "Epoch 139/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3603\n",
      "Epoch 140/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3595\n",
      "Epoch 141/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3593\n",
      "Epoch 142/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3593\n",
      "Epoch 143/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3590\n",
      "Epoch 144/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3582\n",
      "Epoch 145/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3575\n",
      "Epoch 146/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3567\n",
      "Epoch 147/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3533\n",
      "Epoch 148/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3507\n",
      "Epoch 149/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3481\n",
      "Epoch 150/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3465\n",
      "Epoch 151/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3451\n",
      "Epoch 152/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3446\n",
      "Epoch 153/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3451\n",
      "Epoch 154/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3441\n",
      "Epoch 155/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3458\n",
      "Epoch 156/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3454\n",
      "Epoch 157/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3444\n",
      "Epoch 158/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3442\n",
      "Epoch 159/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3429\n",
      "Epoch 160/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3417\n",
      "Epoch 161/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3458\n",
      "Epoch 162/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3434\n",
      "Epoch 163/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3419\n",
      "Epoch 164/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3413\n",
      "Epoch 165/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3407\n",
      "Epoch 166/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3402\n",
      "Epoch 167/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3386\n",
      "Epoch 168/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3384\n",
      "Epoch 169/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3376\n",
      "Epoch 170/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3377\n",
      "Epoch 171/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3373\n",
      "Epoch 172/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3371\n",
      "Epoch 173/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3368\n",
      "Epoch 174/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3366\n",
      "Epoch 175/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3365\n",
      "Epoch 176/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3360\n",
      "Epoch 177/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3358\n",
      "Epoch 178/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3351\n",
      "Epoch 179/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3351\n",
      "Epoch 180/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3350\n",
      "Epoch 181/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3348\n",
      "Epoch 182/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3344\n",
      "Epoch 183/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3344\n",
      "Epoch 184/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3345\n",
      "Epoch 185/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3345\n",
      "Epoch 186/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3339\n",
      "Epoch 187/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3344\n",
      "Epoch 188/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3336\n",
      "Epoch 189/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3336\n",
      "Epoch 190/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3338\n",
      "Epoch 191/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3330\n",
      "Epoch 192/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3328\n",
      "Epoch 193/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3323\n",
      "Epoch 194/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3321\n",
      "Epoch 195/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3317\n",
      "Epoch 196/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3317\n",
      "Epoch 197/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3318\n",
      "Epoch 198/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3313\n",
      "Epoch 199/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3311\n",
      "Epoch 200/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3315\n",
      "Epoch 201/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3317\n",
      "Epoch 202/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3321\n",
      "Epoch 203/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3312\n",
      "Epoch 204/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3323\n",
      "Epoch 205/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3312\n",
      "Epoch 206/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3302\n",
      "Epoch 207/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3301\n",
      "Epoch 208/500\n",
      "10/10 [==============================] - 0s 932us/step - loss: 0.3309\n",
      "Epoch 209/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3312\n",
      "Epoch 210/500\n",
      "10/10 [==============================] - 0s 986us/step - loss: 0.3315\n",
      "Epoch 211/500\n",
      "10/10 [==============================] - 0s 995us/step - loss: 0.3316\n",
      "Epoch 212/500\n",
      "10/10 [==============================] - 0s 998us/step - loss: 0.3305\n",
      "Epoch 213/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3297\n",
      "Epoch 214/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3297\n",
      "Epoch 215/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3294\n",
      "Epoch 216/500\n",
      "10/10 [==============================] - 0s 970us/step - loss: 0.3292\n",
      "Epoch 217/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3294\n",
      "Epoch 218/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3288\n",
      "Epoch 219/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3289\n",
      "Epoch 220/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3281\n",
      "Epoch 221/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3281\n",
      "Epoch 222/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3282\n",
      "Epoch 223/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3283\n",
      "Epoch 224/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3280\n",
      "Epoch 225/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3275\n",
      "Epoch 226/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3275\n",
      "Epoch 227/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3277\n",
      "Epoch 228/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3276\n",
      "Epoch 229/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3274\n",
      "Epoch 230/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3276\n",
      "Epoch 231/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3271\n",
      "Epoch 232/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3269\n",
      "Epoch 233/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3263\n",
      "Epoch 234/500\n",
      "10/10 [==============================] - 0s 990us/step - loss: 0.3262\n",
      "Epoch 235/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3260\n",
      "Epoch 236/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3258\n",
      "Epoch 237/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3259\n",
      "Epoch 238/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3258\n",
      "Epoch 239/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3257\n",
      "Epoch 240/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3253\n",
      "Epoch 241/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3254\n",
      "Epoch 242/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3253\n",
      "Epoch 243/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3249\n",
      "Epoch 244/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3251\n",
      "Epoch 245/500\n",
      "10/10 [==============================] - 0s 969us/step - loss: 0.3250\n",
      "Epoch 246/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3244\n",
      "Epoch 247/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3247\n",
      "Epoch 248/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3247\n",
      "Epoch 249/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3245\n",
      "Epoch 250/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3248\n",
      "Epoch 251/500\n",
      "10/10 [==============================] - 0s 985us/step - loss: 0.3246\n",
      "Epoch 252/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3251\n",
      "Epoch 253/500\n",
      "10/10 [==============================] - 0s 993us/step - loss: 0.3248\n",
      "Epoch 254/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3247\n",
      "Epoch 255/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3244\n",
      "Epoch 256/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3239\n",
      "Epoch 257/500\n",
      "10/10 [==============================] - 0s 975us/step - loss: 0.3239\n",
      "Epoch 258/500\n",
      "10/10 [==============================] - 0s 983us/step - loss: 0.3242\n",
      "Epoch 259/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3238\n",
      "Epoch 260/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3254\n",
      "Epoch 261/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3275\n",
      "Epoch 262/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3314\n",
      "Epoch 263/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3440\n",
      "Epoch 264/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3344\n",
      "Epoch 265/500\n",
      "10/10 [==============================] - 0s 982us/step - loss: 0.3295\n",
      "Epoch 266/500\n",
      "10/10 [==============================] - 0s 976us/step - loss: 0.3265\n",
      "Epoch 267/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3249\n",
      "Epoch 268/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3246\n",
      "Epoch 269/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3234\n",
      "Epoch 270/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3230\n",
      "Epoch 271/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3221\n",
      "Epoch 272/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3222\n",
      "Epoch 273/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3221\n",
      "Epoch 274/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3219\n",
      "Epoch 275/500\n",
      "10/10 [==============================] - 0s 947us/step - loss: 0.3218\n",
      "Epoch 276/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3218\n",
      "Epoch 277/500\n",
      "10/10 [==============================] - 0s 946us/step - loss: 0.3214\n",
      "Epoch 278/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3213\n",
      "Epoch 279/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3212\n",
      "Epoch 280/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3212\n",
      "Epoch 281/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3211\n",
      "Epoch 282/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3213\n",
      "Epoch 283/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3212\n",
      "Epoch 284/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3210\n",
      "Epoch 285/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3207\n",
      "Epoch 286/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3206\n",
      "Epoch 287/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3207\n",
      "Epoch 288/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3205\n",
      "Epoch 289/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3205\n",
      "Epoch 290/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3205\n",
      "Epoch 291/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3209\n",
      "Epoch 292/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3207\n",
      "Epoch 293/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3205\n",
      "Epoch 294/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3201\n",
      "Epoch 295/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3199\n",
      "Epoch 296/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3198\n",
      "Epoch 297/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3201\n",
      "Epoch 298/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3197\n",
      "Epoch 299/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3200\n",
      "Epoch 300/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3197\n",
      "Epoch 301/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3196\n",
      "Epoch 302/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3201\n",
      "Epoch 303/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3204\n",
      "Epoch 304/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3202\n",
      "Epoch 305/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3203\n",
      "Epoch 306/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3205\n",
      "Epoch 307/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3199\n",
      "Epoch 308/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3194\n",
      "Epoch 309/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3194\n",
      "Epoch 310/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3190\n",
      "Epoch 311/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3190\n",
      "Epoch 312/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3188\n",
      "Epoch 313/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3186\n",
      "Epoch 314/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3187\n",
      "Epoch 315/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3213\n",
      "Epoch 316/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3209\n",
      "Epoch 317/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3200\n",
      "Epoch 318/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3206\n",
      "Epoch 319/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3197\n",
      "Epoch 320/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3188\n",
      "Epoch 321/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3185\n",
      "Epoch 322/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3186\n",
      "Epoch 323/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3187\n",
      "Epoch 324/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3185\n",
      "Epoch 325/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3184\n",
      "Epoch 326/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3188\n",
      "Epoch 327/500\n",
      "10/10 [==============================] - 0s 960us/step - loss: 0.3183\n",
      "Epoch 328/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3181\n",
      "Epoch 329/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3181\n",
      "Epoch 330/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3191\n",
      "Epoch 331/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3183\n",
      "Epoch 332/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3179\n",
      "Epoch 333/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3182\n",
      "Epoch 334/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3235\n",
      "Epoch 335/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3242\n",
      "Epoch 336/500\n",
      "10/10 [==============================] - 0s 974us/step - loss: 0.3217\n",
      "Epoch 337/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3209\n",
      "Epoch 338/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3191\n",
      "Epoch 339/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3185\n",
      "Epoch 340/500\n",
      "10/10 [==============================] - 0s 946us/step - loss: 0.3180\n",
      "Epoch 341/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3178\n",
      "Epoch 342/500\n",
      "10/10 [==============================] - 0s 979us/step - loss: 0.3177\n",
      "Epoch 343/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3184\n",
      "Epoch 344/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3180\n",
      "Epoch 345/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3177\n",
      "Epoch 346/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3181\n",
      "Epoch 347/500\n",
      "10/10 [==============================] - 0s 948us/step - loss: 0.3184\n",
      "Epoch 348/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3182\n",
      "Epoch 349/500\n",
      "10/10 [==============================] - 0s 938us/step - loss: 0.3181\n",
      "Epoch 350/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3183\n",
      "Epoch 351/500\n",
      "10/10 [==============================] - 0s 942us/step - loss: 0.3184\n",
      "Epoch 352/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3178\n",
      "Epoch 353/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3177\n",
      "Epoch 354/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3170\n",
      "Epoch 355/500\n",
      "10/10 [==============================] - 0s 967us/step - loss: 0.3169\n",
      "Epoch 356/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3167\n",
      "Epoch 357/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3176\n",
      "Epoch 358/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3178\n",
      "Epoch 359/500\n",
      "10/10 [==============================] - 0s 963us/step - loss: 0.3176\n",
      "Epoch 360/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3176\n",
      "Epoch 361/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3170\n",
      "Epoch 362/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3172\n",
      "Epoch 363/500\n",
      "10/10 [==============================] - 0s 934us/step - loss: 0.3168\n",
      "Epoch 364/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3164\n",
      "Epoch 365/500\n",
      "10/10 [==============================] - 0s 927us/step - loss: 0.3164\n",
      "Epoch 366/500\n",
      "10/10 [==============================] - 0s 930us/step - loss: 0.3162\n",
      "Epoch 367/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3166\n",
      "Epoch 368/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3164\n",
      "Epoch 369/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3159\n",
      "Epoch 370/500\n",
      "10/10 [==============================] - 0s 985us/step - loss: 0.3163\n",
      "Epoch 371/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3160\n",
      "Epoch 372/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3159\n",
      "Epoch 373/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3156\n",
      "Epoch 374/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3156\n",
      "Epoch 375/500\n",
      "10/10 [==============================] - 0s 968us/step - loss: 0.3162\n",
      "Epoch 376/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3160\n",
      "Epoch 377/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3161\n",
      "Epoch 378/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3163\n",
      "Epoch 379/500\n",
      "10/10 [==============================] - 0s 945us/step - loss: 0.3166\n",
      "Epoch 380/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3162\n",
      "Epoch 381/500\n",
      "10/10 [==============================] - 0s 969us/step - loss: 0.3166\n",
      "Epoch 382/500\n",
      "10/10 [==============================] - 0s 957us/step - loss: 0.3160\n",
      "Epoch 383/500\n",
      "10/10 [==============================] - 0s 986us/step - loss: 0.3161\n",
      "Epoch 384/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3161\n",
      "Epoch 385/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3166\n",
      "Epoch 386/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3159\n",
      "Epoch 387/500\n",
      "10/10 [==============================] - 0s 903us/step - loss: 0.3156\n",
      "Epoch 388/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3155\n",
      "Epoch 389/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3151\n",
      "Epoch 390/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3151\n",
      "Epoch 391/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3153\n",
      "Epoch 392/500\n",
      "10/10 [==============================] - 0s 978us/step - loss: 0.3153\n",
      "Epoch 393/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3152\n",
      "Epoch 394/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3151\n",
      "Epoch 395/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3155\n",
      "Epoch 396/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3154\n",
      "Epoch 397/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3215\n",
      "Epoch 398/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3192\n",
      "Epoch 399/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3188\n",
      "Epoch 400/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3206\n",
      "Epoch 401/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3179\n",
      "Epoch 402/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3170\n",
      "Epoch 403/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3181\n",
      "Epoch 404/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3176\n",
      "Epoch 405/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3158\n",
      "Epoch 406/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3159\n",
      "Epoch 407/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3152\n",
      "Epoch 408/500\n",
      "10/10 [==============================] - 0s 998us/step - loss: 0.3151\n",
      "Epoch 409/500\n",
      "10/10 [==============================] - 0s 968us/step - loss: 0.3147\n",
      "Epoch 410/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3148\n",
      "Epoch 411/500\n",
      "10/10 [==============================] - 0s 985us/step - loss: 0.3145\n",
      "Epoch 412/500\n",
      "10/10 [==============================] - 0s 982us/step - loss: 0.3146\n",
      "Epoch 413/500\n",
      "10/10 [==============================] - 0s 947us/step - loss: 0.3195\n",
      "Epoch 414/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3194\n",
      "Epoch 415/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3175\n",
      "Epoch 416/500\n",
      "10/10 [==============================] - 0s 893us/step - loss: 0.3166\n",
      "Epoch 417/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3161\n",
      "Epoch 418/500\n",
      "10/10 [==============================] - 0s 963us/step - loss: 0.3158\n",
      "Epoch 419/500\n",
      "10/10 [==============================] - 0s 983us/step - loss: 0.3153\n",
      "Epoch 420/500\n",
      "10/10 [==============================] - 0s 933us/step - loss: 0.3148\n",
      "Epoch 421/500\n",
      "10/10 [==============================] - 0s 957us/step - loss: 0.3145\n",
      "Epoch 422/500\n",
      "10/10 [==============================] - 0s 948us/step - loss: 0.3146\n",
      "Epoch 423/500\n",
      "10/10 [==============================] - 0s 976us/step - loss: 0.3158\n",
      "Epoch 424/500\n",
      "10/10 [==============================] - 0s 936us/step - loss: 0.3157\n",
      "Epoch 425/500\n",
      "10/10 [==============================] - 0s 949us/step - loss: 0.3148\n",
      "Epoch 426/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3152\n",
      "Epoch 427/500\n",
      "10/10 [==============================] - 0s 971us/step - loss: 0.3150\n",
      "Epoch 428/500\n",
      "10/10 [==============================] - 0s 984us/step - loss: 0.3146\n",
      "Epoch 429/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3153\n",
      "Epoch 430/500\n",
      "10/10 [==============================] - 0s 962us/step - loss: 0.3147\n",
      "Epoch 431/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3144\n",
      "Epoch 432/500\n",
      "10/10 [==============================] - 0s 974us/step - loss: 0.3156\n",
      "Epoch 433/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3146\n",
      "Epoch 434/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3140\n",
      "Epoch 435/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3137\n",
      "Epoch 436/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3134\n",
      "Epoch 437/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3134\n",
      "Epoch 438/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3134\n",
      "Epoch 439/500\n",
      "10/10 [==============================] - 0s 999us/step - loss: 0.3139\n",
      "Epoch 440/500\n",
      "10/10 [==============================] - 0s 967us/step - loss: 0.3139\n",
      "Epoch 441/500\n",
      "10/10 [==============================] - 0s 911us/step - loss: 0.3141\n",
      "Epoch 442/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3140\n",
      "Epoch 443/500\n",
      "10/10 [==============================] - 0s 990us/step - loss: 0.3147\n",
      "Epoch 444/500\n",
      "10/10 [==============================] - 0s 915us/step - loss: 0.3146\n",
      "Epoch 445/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3176\n",
      "Epoch 446/500\n",
      "10/10 [==============================] - 0s 959us/step - loss: 0.3168\n",
      "Epoch 447/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3160\n",
      "Epoch 448/500\n",
      "10/10 [==============================] - 0s 964us/step - loss: 0.3149\n",
      "Epoch 449/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3143\n",
      "Epoch 450/500\n",
      "10/10 [==============================] - 0s 939us/step - loss: 0.3145\n",
      "Epoch 451/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3141\n",
      "Epoch 452/500\n",
      "10/10 [==============================] - 0s 963us/step - loss: 0.3136\n",
      "Epoch 453/500\n",
      "10/10 [==============================] - 0s 946us/step - loss: 0.3133\n",
      "Epoch 454/500\n",
      "10/10 [==============================] - 0s 973us/step - loss: 0.3134\n",
      "Epoch 455/500\n",
      "10/10 [==============================] - 0s 945us/step - loss: 0.3135\n",
      "Epoch 456/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3135\n",
      "Epoch 457/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3132\n",
      "Epoch 458/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3134\n",
      "Epoch 459/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3130\n",
      "Epoch 460/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3129\n",
      "Epoch 461/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3129\n",
      "Epoch 462/500\n",
      "10/10 [==============================] - 0s 962us/step - loss: 0.3129\n",
      "Epoch 463/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3125\n",
      "Epoch 464/500\n",
      "10/10 [==============================] - 0s 992us/step - loss: 0.3124\n",
      "Epoch 465/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3130\n",
      "Epoch 466/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3150\n",
      "Epoch 467/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3142\n",
      "Epoch 468/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3139\n",
      "Epoch 469/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3134\n",
      "Epoch 470/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3129\n",
      "Epoch 471/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3127\n",
      "Epoch 472/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3126\n",
      "Epoch 473/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3125\n",
      "Epoch 474/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3126\n",
      "Epoch 475/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3124\n",
      "Epoch 476/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3123\n",
      "Epoch 477/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3124\n",
      "Epoch 478/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3124\n",
      "Epoch 479/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3124\n",
      "Epoch 480/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3123\n",
      "Epoch 481/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3121\n",
      "Epoch 482/500\n",
      "10/10 [==============================] - 0s 995us/step - loss: 0.3122\n",
      "Epoch 483/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3121\n",
      "Epoch 484/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3121\n",
      "Epoch 485/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3123\n",
      "Epoch 486/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3119\n",
      "Epoch 487/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3118\n",
      "Epoch 488/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3128\n",
      "Epoch 489/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3135\n",
      "Epoch 490/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3141\n",
      "Epoch 491/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3142\n",
      "Epoch 492/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3130\n",
      "Epoch 493/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3127\n",
      "Epoch 494/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3125\n",
      "Epoch 495/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3122\n",
      "Epoch 496/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3126\n",
      "Epoch 497/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3133\n",
      "Epoch 498/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3130\n",
      "Epoch 499/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3125\n",
      "Epoch 500/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1504c9bb0>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.fit(data.x_train.astype(float), data.x_train.astype(float), epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model = Sequential([\n",
    "    Dense(10, activation='selu', kernel_initializer='lecun_normal', kernel_regularizer=l2()),\n",
    "    Dense(10, activation='selu', kernel_initializer='lecun_normal', kernel_regularizer=l2()),\n",
    "    Dense(10, activation='selu', kernel_initializer='lecun_normal', kernel_regularizer=l2()),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1270 - acc: 0.9796 - val_loss: 0.2624 - val_acc: 0.9388\n",
      "Epoch 2/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1272 - acc: 0.9830 - val_loss: 0.2547 - val_acc: 0.9490\n",
      "Epoch 3/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1295 - acc: 0.9830 - val_loss: 0.2760 - val_acc: 0.9286\n",
      "Epoch 4/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1267 - acc: 0.9830 - val_loss: 0.2493 - val_acc: 0.9388\n",
      "Epoch 5/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1301 - acc: 0.9796 - val_loss: 0.2553 - val_acc: 0.9490\n",
      "Epoch 6/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1209 - acc: 0.9864 - val_loss: 0.2502 - val_acc: 0.9490\n",
      "Epoch 7/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1217 - acc: 0.9864 - val_loss: 0.2624 - val_acc: 0.9388\n",
      "Epoch 8/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1207 - acc: 0.9898 - val_loss: 0.2578 - val_acc: 0.9388\n",
      "Epoch 9/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1239 - acc: 0.9898 - val_loss: 0.2531 - val_acc: 0.9388\n",
      "Epoch 10/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1199 - acc: 0.9898 - val_loss: 0.2509 - val_acc: 0.9388\n",
      "Epoch 11/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1202 - acc: 0.9898 - val_loss: 0.2543 - val_acc: 0.9388\n",
      "Epoch 12/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1207 - acc: 0.9898 - val_loss: 0.2521 - val_acc: 0.9388\n",
      "Epoch 13/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1199 - acc: 0.9898 - val_loss: 0.2626 - val_acc: 0.9388\n",
      "Epoch 14/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1288 - acc: 0.9830 - val_loss: 0.2501 - val_acc: 0.9388\n",
      "Epoch 15/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1313 - acc: 0.9830 - val_loss: 0.2576 - val_acc: 0.9388\n",
      "Epoch 16/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1334 - acc: 0.9762 - val_loss: 0.2417 - val_acc: 0.9490\n",
      "Epoch 17/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1250 - acc: 0.9864 - val_loss: 0.2631 - val_acc: 0.9388\n",
      "Epoch 18/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1196 - acc: 0.9898 - val_loss: 0.2482 - val_acc: 0.9490\n",
      "Epoch 19/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1193 - acc: 0.9898 - val_loss: 0.2668 - val_acc: 0.9388\n",
      "Epoch 20/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1190 - acc: 0.9898 - val_loss: 0.2622 - val_acc: 0.9388\n",
      "Epoch 21/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1191 - acc: 0.9898 - val_loss: 0.2500 - val_acc: 0.9388\n",
      "Epoch 22/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1199 - acc: 0.9898 - val_loss: 0.2444 - val_acc: 0.9490\n",
      "Epoch 23/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1181 - acc: 0.9898 - val_loss: 0.2546 - val_acc: 0.9388\n",
      "Epoch 24/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1210 - acc: 0.9898 - val_loss: 0.2613 - val_acc: 0.9388\n",
      "Epoch 25/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1179 - acc: 0.9898 - val_loss: 0.2635 - val_acc: 0.9388\n",
      "Epoch 26/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1168 - acc: 0.9898 - val_loss: 0.2501 - val_acc: 0.9490\n",
      "Epoch 27/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1225 - acc: 0.9864 - val_loss: 0.2610 - val_acc: 0.9388\n",
      "Epoch 28/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1189 - acc: 0.9898 - val_loss: 0.2660 - val_acc: 0.9388\n",
      "Epoch 29/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1178 - acc: 0.9898 - val_loss: 0.2647 - val_acc: 0.9388\n",
      "Epoch 30/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1189 - acc: 0.9898 - val_loss: 0.2684 - val_acc: 0.9286\n",
      "Epoch 31/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1180 - acc: 0.9898 - val_loss: 0.2692 - val_acc: 0.9286\n",
      "Epoch 32/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1166 - acc: 0.9898 - val_loss: 0.2581 - val_acc: 0.9388\n",
      "Epoch 33/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1157 - acc: 0.9898 - val_loss: 0.2563 - val_acc: 0.9388\n",
      "Epoch 34/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1172 - acc: 0.9898 - val_loss: 0.2556 - val_acc: 0.9388\n",
      "Epoch 35/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1169 - acc: 0.9898 - val_loss: 0.2537 - val_acc: 0.9388\n",
      "Epoch 36/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1168 - acc: 0.9898 - val_loss: 0.2665 - val_acc: 0.9388\n",
      "Epoch 37/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1256 - acc: 0.9864 - val_loss: 0.2723 - val_acc: 0.9388\n",
      "Epoch 38/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1225 - acc: 0.9830 - val_loss: 0.2762 - val_acc: 0.9388\n",
      "Epoch 39/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1171 - acc: 0.9898 - val_loss: 0.2497 - val_acc: 0.9490\n",
      "Epoch 40/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1155 - acc: 0.9898 - val_loss: 0.2498 - val_acc: 0.9388\n",
      "Epoch 41/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1164 - acc: 0.9898 - val_loss: 0.2711 - val_acc: 0.9388\n",
      "Epoch 42/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1156 - acc: 0.9898 - val_loss: 0.2620 - val_acc: 0.9388\n",
      "Epoch 43/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1155 - acc: 0.9932 - val_loss: 0.2588 - val_acc: 0.9388\n",
      "Epoch 44/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1142 - acc: 0.9898 - val_loss: 0.2596 - val_acc: 0.9388\n",
      "Epoch 45/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1179 - acc: 0.9898 - val_loss: 0.2647 - val_acc: 0.9388\n",
      "Epoch 46/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1181 - acc: 0.9898 - val_loss: 0.2462 - val_acc: 0.9490\n",
      "Epoch 47/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1151 - acc: 0.9898 - val_loss: 0.2562 - val_acc: 0.9388\n",
      "Epoch 48/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1140 - acc: 0.9898 - val_loss: 0.2606 - val_acc: 0.9388\n",
      "Epoch 49/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1182 - acc: 0.9898 - val_loss: 0.2473 - val_acc: 0.9388\n",
      "Epoch 50/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1167 - acc: 0.9932 - val_loss: 0.2670 - val_acc: 0.9388\n",
      "Epoch 51/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1137 - acc: 0.9898 - val_loss: 0.2666 - val_acc: 0.9388\n",
      "Epoch 52/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1146 - acc: 0.9898 - val_loss: 0.2673 - val_acc: 0.9388\n",
      "Epoch 53/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1145 - acc: 0.9898 - val_loss: 0.2747 - val_acc: 0.9388\n",
      "Epoch 54/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1145 - acc: 0.9898 - val_loss: 0.2671 - val_acc: 0.9388\n",
      "Epoch 55/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1123 - acc: 0.9898 - val_loss: 0.2616 - val_acc: 0.9388\n",
      "Epoch 56/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1135 - acc: 0.9932 - val_loss: 0.2622 - val_acc: 0.9388\n",
      "Epoch 57/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1122 - acc: 0.9898 - val_loss: 0.2575 - val_acc: 0.9388\n",
      "Epoch 58/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1126 - acc: 0.9898 - val_loss: 0.2550 - val_acc: 0.9388\n",
      "Epoch 59/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1180 - acc: 0.9898 - val_loss: 0.2902 - val_acc: 0.9388\n",
      "Epoch 60/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1217 - acc: 0.9796 - val_loss: 0.2481 - val_acc: 0.9388\n",
      "Epoch 61/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1216 - acc: 0.9796 - val_loss: 0.2820 - val_acc: 0.9388\n",
      "Epoch 62/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1212 - acc: 0.9864 - val_loss: 0.2496 - val_acc: 0.9490\n",
      "Epoch 63/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1235 - acc: 0.9796 - val_loss: 0.2735 - val_acc: 0.9388\n",
      "Epoch 64/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1268 - acc: 0.9796 - val_loss: 0.2482 - val_acc: 0.9490\n",
      "Epoch 65/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1174 - acc: 0.9898 - val_loss: 0.2625 - val_acc: 0.9388\n",
      "Epoch 66/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1132 - acc: 0.9898 - val_loss: 0.2550 - val_acc: 0.9388\n",
      "Epoch 67/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1117 - acc: 0.9932 - val_loss: 0.2590 - val_acc: 0.9388\n",
      "Epoch 68/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1126 - acc: 0.9898 - val_loss: 0.2643 - val_acc: 0.9388\n",
      "Epoch 69/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1111 - acc: 0.9898 - val_loss: 0.2562 - val_acc: 0.9388\n",
      "Epoch 70/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1122 - acc: 0.9898 - val_loss: 0.2627 - val_acc: 0.9388\n",
      "Epoch 71/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1157 - acc: 0.9864 - val_loss: 0.2575 - val_acc: 0.9388\n",
      "Epoch 72/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1155 - acc: 0.9864 - val_loss: 0.2500 - val_acc: 0.9490\n",
      "Epoch 73/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1141 - acc: 0.9864 - val_loss: 0.2354 - val_acc: 0.9490\n",
      "Epoch 74/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1147 - acc: 0.9830 - val_loss: 0.2454 - val_acc: 0.9388\n",
      "Epoch 75/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1141 - acc: 0.9864 - val_loss: 0.2709 - val_acc: 0.9388\n",
      "Epoch 76/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1120 - acc: 0.9864 - val_loss: 0.2514 - val_acc: 0.9490\n",
      "Epoch 77/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1134 - acc: 0.9864 - val_loss: 0.2571 - val_acc: 0.9388\n",
      "Epoch 78/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1161 - acc: 0.9830 - val_loss: 0.2650 - val_acc: 0.9388\n",
      "Epoch 79/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1123 - acc: 0.9864 - val_loss: 0.2528 - val_acc: 0.9388\n",
      "Epoch 80/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1118 - acc: 0.9864 - val_loss: 0.2645 - val_acc: 0.9388\n",
      "Epoch 81/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1105 - acc: 0.9898 - val_loss: 0.2564 - val_acc: 0.9388\n",
      "Epoch 82/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1141 - acc: 0.9830 - val_loss: 0.2507 - val_acc: 0.9388\n",
      "Epoch 83/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1099 - acc: 0.9932 - val_loss: 0.2578 - val_acc: 0.9388\n",
      "Epoch 84/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1096 - acc: 0.9932 - val_loss: 0.2610 - val_acc: 0.9388\n",
      "Epoch 85/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1089 - acc: 0.9898 - val_loss: 0.2556 - val_acc: 0.9388\n",
      "Epoch 86/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1115 - acc: 0.9864 - val_loss: 0.2541 - val_acc: 0.9388\n",
      "Epoch 87/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1123 - acc: 0.9898 - val_loss: 0.2740 - val_acc: 0.9388\n",
      "Epoch 88/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1098 - acc: 0.9864 - val_loss: 0.2575 - val_acc: 0.9388\n",
      "Epoch 89/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1083 - acc: 0.9898 - val_loss: 0.2729 - val_acc: 0.9388\n",
      "Epoch 90/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1087 - acc: 0.9898 - val_loss: 0.2589 - val_acc: 0.9388\n",
      "Epoch 91/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1090 - acc: 0.9932 - val_loss: 0.2620 - val_acc: 0.9388\n",
      "Epoch 92/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1091 - acc: 0.9898 - val_loss: 0.2627 - val_acc: 0.9388\n",
      "Epoch 93/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1127 - acc: 0.9932 - val_loss: 0.2695 - val_acc: 0.9388\n",
      "Epoch 94/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1096 - acc: 0.9898 - val_loss: 0.2568 - val_acc: 0.9388\n",
      "Epoch 95/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1100 - acc: 0.9932 - val_loss: 0.2808 - val_acc: 0.9388\n",
      "Epoch 96/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1136 - acc: 0.9898 - val_loss: 0.2630 - val_acc: 0.9388\n",
      "Epoch 97/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1142 - acc: 0.9864 - val_loss: 0.2398 - val_acc: 0.9490\n",
      "Epoch 98/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1116 - acc: 0.9830 - val_loss: 0.2657 - val_acc: 0.9388\n",
      "Epoch 99/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1102 - acc: 0.9864 - val_loss: 0.2533 - val_acc: 0.9388\n",
      "Epoch 100/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1090 - acc: 0.9932 - val_loss: 0.2861 - val_acc: 0.9388\n",
      "Epoch 101/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1395 - acc: 0.9660 - val_loss: 0.2612 - val_acc: 0.9388\n",
      "Epoch 102/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1253 - acc: 0.9762 - val_loss: 0.2718 - val_acc: 0.9388\n",
      "Epoch 103/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1127 - acc: 0.9864 - val_loss: 0.2461 - val_acc: 0.9490\n",
      "Epoch 104/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1111 - acc: 0.9932 - val_loss: 0.2806 - val_acc: 0.9388\n",
      "Epoch 105/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1149 - acc: 0.9864 - val_loss: 0.2484 - val_acc: 0.9490\n",
      "Epoch 106/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1095 - acc: 0.9932 - val_loss: 0.2672 - val_acc: 0.9388\n",
      "Epoch 107/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1125 - acc: 0.9898 - val_loss: 0.2611 - val_acc: 0.9388\n",
      "Epoch 108/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1063 - acc: 0.9898 - val_loss: 0.2903 - val_acc: 0.9388\n",
      "Epoch 109/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1111 - acc: 0.9830 - val_loss: 0.2767 - val_acc: 0.9388\n",
      "Epoch 110/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1121 - acc: 0.9898 - val_loss: 0.2812 - val_acc: 0.9388\n",
      "Epoch 111/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1099 - acc: 0.9830 - val_loss: 0.2666 - val_acc: 0.9388\n",
      "Epoch 112/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1090 - acc: 0.9932 - val_loss: 0.2453 - val_acc: 0.9490\n",
      "Epoch 113/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1079 - acc: 0.9864 - val_loss: 0.2615 - val_acc: 0.9388\n",
      "Epoch 114/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1059 - acc: 0.9898 - val_loss: 0.2560 - val_acc: 0.9490\n",
      "Epoch 115/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1066 - acc: 0.9932 - val_loss: 0.2796 - val_acc: 0.9388\n",
      "Epoch 116/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1075 - acc: 0.9932 - val_loss: 0.2638 - val_acc: 0.9388\n",
      "Epoch 117/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1058 - acc: 0.9898 - val_loss: 0.2727 - val_acc: 0.9388\n",
      "Epoch 118/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1068 - acc: 0.9898 - val_loss: 0.2642 - val_acc: 0.9388\n",
      "Epoch 119/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1060 - acc: 0.9932 - val_loss: 0.2954 - val_acc: 0.9388\n",
      "Epoch 120/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1342 - acc: 0.9592 - val_loss: 0.2665 - val_acc: 0.9286\n",
      "Epoch 121/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1195 - acc: 0.9762 - val_loss: 0.2900 - val_acc: 0.9388\n",
      "Epoch 122/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1105 - acc: 0.9830 - val_loss: 0.2561 - val_acc: 0.9490\n",
      "Epoch 123/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1065 - acc: 0.9932 - val_loss: 0.2893 - val_acc: 0.9388\n",
      "Epoch 124/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1068 - acc: 0.9864 - val_loss: 0.2728 - val_acc: 0.9388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1051 - acc: 0.9932 - val_loss: 0.2878 - val_acc: 0.9388\n",
      "Epoch 126/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1068 - acc: 0.9932 - val_loss: 0.2685 - val_acc: 0.9388\n",
      "Epoch 127/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1049 - acc: 0.9898 - val_loss: 0.2609 - val_acc: 0.9388\n",
      "Epoch 128/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1100 - acc: 0.9898 - val_loss: 0.2742 - val_acc: 0.9388\n",
      "Epoch 129/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1069 - acc: 0.9898 - val_loss: 0.2766 - val_acc: 0.9388\n",
      "Epoch 130/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1058 - acc: 0.9864 - val_loss: 0.2696 - val_acc: 0.9388\n",
      "Epoch 131/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1038 - acc: 0.9932 - val_loss: 0.2699 - val_acc: 0.9388\n",
      "Epoch 132/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1065 - acc: 0.9898 - val_loss: 0.2883 - val_acc: 0.9388\n",
      "Epoch 133/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1070 - acc: 0.9864 - val_loss: 0.2628 - val_acc: 0.9286\n",
      "Epoch 134/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1048 - acc: 0.9932 - val_loss: 0.2941 - val_acc: 0.9388\n",
      "Epoch 135/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1119 - acc: 0.9864 - val_loss: 0.2644 - val_acc: 0.9286\n",
      "Epoch 136/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1039 - acc: 0.9932 - val_loss: 0.2841 - val_acc: 0.9388\n",
      "Epoch 137/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1027 - acc: 0.9932 - val_loss: 0.2737 - val_acc: 0.9388\n",
      "Epoch 138/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1036 - acc: 0.9864 - val_loss: 0.2713 - val_acc: 0.9388\n",
      "Epoch 139/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1045 - acc: 0.9864 - val_loss: 0.2790 - val_acc: 0.9388\n",
      "Epoch 140/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1030 - acc: 0.9864 - val_loss: 0.2622 - val_acc: 0.9388\n",
      "Epoch 141/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1067 - acc: 0.9932 - val_loss: 0.2806 - val_acc: 0.9388\n",
      "Epoch 142/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1060 - acc: 0.9898 - val_loss: 0.2572 - val_acc: 0.9388\n",
      "Epoch 143/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1130 - acc: 0.9796 - val_loss: 0.2933 - val_acc: 0.9388\n",
      "Epoch 144/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1060 - acc: 0.9898 - val_loss: 0.2601 - val_acc: 0.9388\n",
      "Epoch 145/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1041 - acc: 0.9932 - val_loss: 0.2802 - val_acc: 0.9388\n",
      "Epoch 146/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1024 - acc: 0.9898 - val_loss: 0.2710 - val_acc: 0.9286\n",
      "Epoch 147/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1032 - acc: 0.9932 - val_loss: 0.2769 - val_acc: 0.9286\n",
      "Epoch 148/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1034 - acc: 0.9898 - val_loss: 0.2784 - val_acc: 0.9388\n",
      "Epoch 149/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1013 - acc: 0.9932 - val_loss: 0.2819 - val_acc: 0.9388\n",
      "Epoch 150/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1059 - acc: 0.9932 - val_loss: 0.2855 - val_acc: 0.9388\n",
      "Epoch 151/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1201 - acc: 0.9694 - val_loss: 0.2226 - val_acc: 0.9388\n",
      "Epoch 152/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1369 - acc: 0.9796 - val_loss: 0.3274 - val_acc: 0.9388\n",
      "Epoch 153/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1109 - acc: 0.9864 - val_loss: 0.2554 - val_acc: 0.9490\n",
      "Epoch 154/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1118 - acc: 0.9864 - val_loss: 0.2982 - val_acc: 0.9388\n",
      "Epoch 155/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1052 - acc: 0.9864 - val_loss: 0.2586 - val_acc: 0.9490\n",
      "Epoch 156/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1072 - acc: 0.9898 - val_loss: 0.2876 - val_acc: 0.9388\n",
      "Epoch 157/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1041 - acc: 0.9864 - val_loss: 0.2608 - val_acc: 0.9388\n",
      "Epoch 158/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1047 - acc: 0.9932 - val_loss: 0.2807 - val_acc: 0.9388\n",
      "Epoch 159/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1037 - acc: 0.9864 - val_loss: 0.2618 - val_acc: 0.9388\n",
      "Epoch 160/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1083 - acc: 0.9932 - val_loss: 0.2938 - val_acc: 0.9388\n",
      "Epoch 161/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1034 - acc: 0.9898 - val_loss: 0.2744 - val_acc: 0.9388\n",
      "Epoch 162/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1008 - acc: 0.9932 - val_loss: 0.2750 - val_acc: 0.9388\n",
      "Epoch 163/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1083 - acc: 0.9898 - val_loss: 0.2700 - val_acc: 0.9388\n",
      "Epoch 164/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1107 - acc: 0.9898 - val_loss: 0.2861 - val_acc: 0.9388\n",
      "Epoch 165/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1112 - acc: 0.9864 - val_loss: 0.2630 - val_acc: 0.9388\n",
      "Epoch 166/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1030 - acc: 0.9932 - val_loss: 0.2721 - val_acc: 0.9388\n",
      "Epoch 167/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1015 - acc: 0.9864 - val_loss: 0.2693 - val_acc: 0.9388\n",
      "Epoch 168/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1033 - acc: 0.9932 - val_loss: 0.2819 - val_acc: 0.9388\n",
      "Epoch 169/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1054 - acc: 0.9898 - val_loss: 0.2788 - val_acc: 0.9388\n",
      "Epoch 170/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1022 - acc: 0.9932 - val_loss: 0.2596 - val_acc: 0.9388\n",
      "Epoch 171/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1026 - acc: 0.9864 - val_loss: 0.2949 - val_acc: 0.9388\n",
      "Epoch 172/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1045 - acc: 0.9932 - val_loss: 0.2588 - val_acc: 0.9388\n",
      "Epoch 173/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1039 - acc: 0.9932 - val_loss: 0.2801 - val_acc: 0.9388\n",
      "Epoch 174/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0997 - acc: 0.9932 - val_loss: 0.2737 - val_acc: 0.9388\n",
      "Epoch 175/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1036 - acc: 0.9898 - val_loss: 0.2946 - val_acc: 0.9388\n",
      "Epoch 176/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1071 - acc: 0.9830 - val_loss: 0.2647 - val_acc: 0.9388\n",
      "Epoch 177/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1080 - acc: 0.9830 - val_loss: 0.3035 - val_acc: 0.9388\n",
      "Epoch 178/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1092 - acc: 0.9864 - val_loss: 0.2556 - val_acc: 0.9490\n",
      "Epoch 179/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1078 - acc: 0.9796 - val_loss: 0.2936 - val_acc: 0.9388\n",
      "Epoch 180/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1081 - acc: 0.9830 - val_loss: 0.2615 - val_acc: 0.9388\n",
      "Epoch 181/500\n",
      "10/10 [==============================] - 0s 8ms/step - loss: 0.1066 - acc: 0.9864 - val_loss: 0.3182 - val_acc: 0.9388\n",
      "Epoch 182/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1101 - acc: 0.9864 - val_loss: 0.2516 - val_acc: 0.9490\n",
      "Epoch 183/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1046 - acc: 0.9932 - val_loss: 0.2842 - val_acc: 0.9388\n",
      "Epoch 184/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1009 - acc: 0.9898 - val_loss: 0.2701 - val_acc: 0.9388\n",
      "Epoch 185/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1034 - acc: 0.9864 - val_loss: 0.2694 - val_acc: 0.9388\n",
      "Epoch 186/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1009 - acc: 0.9898 - val_loss: 0.2749 - val_acc: 0.9388\n",
      "Epoch 187/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1010 - acc: 0.9898 - val_loss: 0.2907 - val_acc: 0.9388\n",
      "Epoch 188/500\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.1043 - acc: 0.9932 - val_loss: 0.2894 - val_acc: 0.9286\n",
      "Epoch 189/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1052 - acc: 0.9898 - val_loss: 0.3125 - val_acc: 0.9388\n",
      "Epoch 190/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1040 - acc: 0.9830 - val_loss: 0.2670 - val_acc: 0.9388\n",
      "Epoch 191/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1023 - acc: 0.9898 - val_loss: 0.2672 - val_acc: 0.9388\n",
      "Epoch 192/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1049 - acc: 0.9864 - val_loss: 0.2669 - val_acc: 0.9490\n",
      "Epoch 193/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0997 - acc: 0.9932 - val_loss: 0.2747 - val_acc: 0.9388\n",
      "Epoch 194/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0987 - acc: 0.9932 - val_loss: 0.2849 - val_acc: 0.9388\n",
      "Epoch 195/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1004 - acc: 0.9932 - val_loss: 0.2980 - val_acc: 0.9388\n",
      "Epoch 196/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1059 - acc: 0.9864 - val_loss: 0.2599 - val_acc: 0.9490\n",
      "Epoch 197/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1047 - acc: 0.9898 - val_loss: 0.2923 - val_acc: 0.9388\n",
      "Epoch 198/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1026 - acc: 0.9898 - val_loss: 0.2930 - val_acc: 0.9388\n",
      "Epoch 199/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1012 - acc: 0.9898 - val_loss: 0.2834 - val_acc: 0.9388\n",
      "Epoch 200/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0992 - acc: 0.9932 - val_loss: 0.2912 - val_acc: 0.9388\n",
      "Epoch 201/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0981 - acc: 0.9898 - val_loss: 0.2737 - val_acc: 0.9286\n",
      "Epoch 202/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1040 - acc: 0.9932 - val_loss: 0.2919 - val_acc: 0.9388\n",
      "Epoch 203/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0991 - acc: 0.9932 - val_loss: 0.2935 - val_acc: 0.9388\n",
      "Epoch 204/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1046 - acc: 0.9898 - val_loss: 0.2999 - val_acc: 0.9388\n",
      "Epoch 205/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1190 - acc: 0.9796 - val_loss: 0.2573 - val_acc: 0.9388\n",
      "Epoch 206/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1319 - acc: 0.9762 - val_loss: 0.3490 - val_acc: 0.9184\n",
      "Epoch 207/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1056 - acc: 0.9830 - val_loss: 0.2648 - val_acc: 0.9490\n",
      "Epoch 208/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1063 - acc: 0.9830 - val_loss: 0.2751 - val_acc: 0.9490\n",
      "Epoch 209/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0997 - acc: 0.9898 - val_loss: 0.2923 - val_acc: 0.9388\n",
      "Epoch 210/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0971 - acc: 0.9898 - val_loss: 0.2783 - val_acc: 0.9388\n",
      "Epoch 211/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0989 - acc: 0.9864 - val_loss: 0.2766 - val_acc: 0.9388\n",
      "Epoch 212/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1014 - acc: 0.9932 - val_loss: 0.2819 - val_acc: 0.9388\n",
      "Epoch 213/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0974 - acc: 0.9898 - val_loss: 0.2847 - val_acc: 0.9388\n",
      "Epoch 214/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0980 - acc: 0.9898 - val_loss: 0.2772 - val_acc: 0.9388\n",
      "Epoch 215/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0969 - acc: 0.9932 - val_loss: 0.2905 - val_acc: 0.9388\n",
      "Epoch 216/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0969 - acc: 0.9932 - val_loss: 0.2896 - val_acc: 0.9388\n",
      "Epoch 217/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0964 - acc: 0.9932 - val_loss: 0.2955 - val_acc: 0.9388\n",
      "Epoch 218/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0944 - acc: 0.9932 - val_loss: 0.2724 - val_acc: 0.9490\n",
      "Epoch 219/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0977 - acc: 0.9932 - val_loss: 0.3000 - val_acc: 0.9388\n",
      "Epoch 220/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1136 - acc: 0.9762 - val_loss: 0.2894 - val_acc: 0.9388\n",
      "Epoch 221/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1061 - acc: 0.9864 - val_loss: 0.2861 - val_acc: 0.9388\n",
      "Epoch 222/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1011 - acc: 0.9932 - val_loss: 0.2614 - val_acc: 0.9388\n",
      "Epoch 223/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1040 - acc: 0.9898 - val_loss: 0.3052 - val_acc: 0.9388\n",
      "Epoch 224/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1099 - acc: 0.9830 - val_loss: 0.2734 - val_acc: 0.9388\n",
      "Epoch 225/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1087 - acc: 0.9864 - val_loss: 0.2670 - val_acc: 0.9490\n",
      "Epoch 226/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0981 - acc: 0.9932 - val_loss: 0.2947 - val_acc: 0.9388\n",
      "Epoch 227/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0954 - acc: 0.9932 - val_loss: 0.2825 - val_acc: 0.9388\n",
      "Epoch 228/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0956 - acc: 0.9932 - val_loss: 0.2889 - val_acc: 0.9388\n",
      "Epoch 229/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0975 - acc: 0.9932 - val_loss: 0.2818 - val_acc: 0.9388\n",
      "Epoch 230/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1005 - acc: 0.9932 - val_loss: 0.2992 - val_acc: 0.9388\n",
      "Epoch 231/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1058 - acc: 0.9898 - val_loss: 0.2880 - val_acc: 0.9388\n",
      "Epoch 232/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0984 - acc: 0.9898 - val_loss: 0.2765 - val_acc: 0.9388\n",
      "Epoch 233/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0993 - acc: 0.9932 - val_loss: 0.3074 - val_acc: 0.9388\n",
      "Epoch 234/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0971 - acc: 0.9932 - val_loss: 0.2782 - val_acc: 0.9388\n",
      "Epoch 235/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0967 - acc: 0.9932 - val_loss: 0.2820 - val_acc: 0.9388\n",
      "Epoch 236/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0976 - acc: 0.9864 - val_loss: 0.2796 - val_acc: 0.9388\n",
      "Epoch 237/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0959 - acc: 0.9932 - val_loss: 0.2797 - val_acc: 0.9388\n",
      "Epoch 238/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0983 - acc: 0.9932 - val_loss: 0.2983 - val_acc: 0.9388\n",
      "Epoch 239/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1018 - acc: 0.9864 - val_loss: 0.2754 - val_acc: 0.9388\n",
      "Epoch 240/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0980 - acc: 0.9932 - val_loss: 0.3038 - val_acc: 0.9388\n",
      "Epoch 241/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0961 - acc: 0.9898 - val_loss: 0.2784 - val_acc: 0.9388\n",
      "Epoch 242/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0968 - acc: 0.9898 - val_loss: 0.2890 - val_acc: 0.9388\n",
      "Epoch 243/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0948 - acc: 0.9898 - val_loss: 0.2760 - val_acc: 0.9388\n",
      "Epoch 244/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0958 - acc: 0.9932 - val_loss: 0.2899 - val_acc: 0.9388\n",
      "Epoch 245/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0993 - acc: 0.9932 - val_loss: 0.3093 - val_acc: 0.9388\n",
      "Epoch 246/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0988 - acc: 0.9898 - val_loss: 0.2949 - val_acc: 0.9388\n",
      "Epoch 247/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0972 - acc: 0.9932 - val_loss: 0.2689 - val_acc: 0.9490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 248/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0941 - acc: 0.9932 - val_loss: 0.2931 - val_acc: 0.9388\n",
      "Epoch 249/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0977 - acc: 0.9864 - val_loss: 0.2853 - val_acc: 0.9388\n",
      "Epoch 250/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0947 - acc: 0.9932 - val_loss: 0.3091 - val_acc: 0.9388\n",
      "Epoch 251/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0949 - acc: 0.9932 - val_loss: 0.2882 - val_acc: 0.9388\n",
      "Epoch 252/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0988 - acc: 0.9932 - val_loss: 0.3003 - val_acc: 0.9388\n",
      "Epoch 253/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1051 - acc: 0.9830 - val_loss: 0.2587 - val_acc: 0.9490\n",
      "Epoch 254/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1052 - acc: 0.9898 - val_loss: 0.3183 - val_acc: 0.9388\n",
      "Epoch 255/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0966 - acc: 0.9898 - val_loss: 0.2753 - val_acc: 0.9388\n",
      "Epoch 256/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0951 - acc: 0.9898 - val_loss: 0.2936 - val_acc: 0.9388\n",
      "Epoch 257/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0928 - acc: 0.9932 - val_loss: 0.2999 - val_acc: 0.9388\n",
      "Epoch 258/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0931 - acc: 0.9932 - val_loss: 0.3013 - val_acc: 0.9388\n",
      "Epoch 259/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0931 - acc: 0.9932 - val_loss: 0.2909 - val_acc: 0.9388\n",
      "Epoch 260/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0946 - acc: 0.9932 - val_loss: 0.3079 - val_acc: 0.9388\n",
      "Epoch 261/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0945 - acc: 0.9932 - val_loss: 0.2885 - val_acc: 0.9388\n",
      "Epoch 262/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0989 - acc: 0.9898 - val_loss: 0.2854 - val_acc: 0.9388\n",
      "Epoch 263/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0965 - acc: 0.9932 - val_loss: 0.2921 - val_acc: 0.9388\n",
      "Epoch 264/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0948 - acc: 0.9932 - val_loss: 0.3103 - val_acc: 0.9388\n",
      "Epoch 265/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0924 - acc: 0.9932 - val_loss: 0.3027 - val_acc: 0.9388\n",
      "Epoch 266/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0935 - acc: 0.9932 - val_loss: 0.2946 - val_acc: 0.9388\n",
      "Epoch 267/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0926 - acc: 0.9932 - val_loss: 0.2843 - val_acc: 0.9388\n",
      "Epoch 268/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0953 - acc: 0.9898 - val_loss: 0.2853 - val_acc: 0.9388\n",
      "Epoch 269/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0941 - acc: 0.9898 - val_loss: 0.2928 - val_acc: 0.9388\n",
      "Epoch 270/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0966 - acc: 0.9932 - val_loss: 0.3206 - val_acc: 0.9388\n",
      "Epoch 271/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0936 - acc: 0.9932 - val_loss: 0.2861 - val_acc: 0.9388\n",
      "Epoch 272/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0988 - acc: 0.9932 - val_loss: 0.3052 - val_acc: 0.9388\n",
      "Epoch 273/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0986 - acc: 0.9898 - val_loss: 0.2762 - val_acc: 0.9388\n",
      "Epoch 274/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0972 - acc: 0.9932 - val_loss: 0.3070 - val_acc: 0.9388\n",
      "Epoch 275/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0936 - acc: 0.9932 - val_loss: 0.3020 - val_acc: 0.9388\n",
      "Epoch 276/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0941 - acc: 0.9898 - val_loss: 0.2969 - val_acc: 0.9388\n",
      "Epoch 277/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0941 - acc: 0.9932 - val_loss: 0.3138 - val_acc: 0.9388\n",
      "Epoch 278/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0973 - acc: 0.9898 - val_loss: 0.2783 - val_acc: 0.9388\n",
      "Epoch 279/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0964 - acc: 0.9932 - val_loss: 0.3008 - val_acc: 0.9388\n",
      "Epoch 280/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0926 - acc: 0.9932 - val_loss: 0.3006 - val_acc: 0.9388\n",
      "Epoch 281/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1128 - acc: 0.9728 - val_loss: 0.3030 - val_acc: 0.9388\n",
      "Epoch 282/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0962 - acc: 0.9932 - val_loss: 0.2925 - val_acc: 0.9388\n",
      "Epoch 283/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0954 - acc: 0.9898 - val_loss: 0.2772 - val_acc: 0.9388\n",
      "Epoch 284/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0925 - acc: 0.9932 - val_loss: 0.2896 - val_acc: 0.9388\n",
      "Epoch 285/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0919 - acc: 0.9932 - val_loss: 0.2938 - val_acc: 0.9388\n",
      "Epoch 286/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0925 - acc: 0.9932 - val_loss: 0.2815 - val_acc: 0.9388\n",
      "Epoch 287/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0946 - acc: 0.9932 - val_loss: 0.2894 - val_acc: 0.9388\n",
      "Epoch 288/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0954 - acc: 0.9932 - val_loss: 0.3043 - val_acc: 0.9388\n",
      "Epoch 289/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0961 - acc: 0.9932 - val_loss: 0.2861 - val_acc: 0.9388\n",
      "Epoch 290/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0929 - acc: 0.9932 - val_loss: 0.2788 - val_acc: 0.9388\n",
      "Epoch 291/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0935 - acc: 0.9932 - val_loss: 0.2866 - val_acc: 0.9388\n",
      "Epoch 292/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0922 - acc: 0.9932 - val_loss: 0.2856 - val_acc: 0.9388\n",
      "Epoch 293/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0953 - acc: 0.9898 - val_loss: 0.2692 - val_acc: 0.9490\n",
      "Epoch 294/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0942 - acc: 0.9864 - val_loss: 0.2878 - val_acc: 0.9388\n",
      "Epoch 295/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0913 - acc: 0.9932 - val_loss: 0.3020 - val_acc: 0.9388\n",
      "Epoch 296/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0920 - acc: 0.9932 - val_loss: 0.2964 - val_acc: 0.9388\n",
      "Epoch 297/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0947 - acc: 0.9932 - val_loss: 0.3097 - val_acc: 0.9388\n",
      "Epoch 298/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1024 - acc: 0.9898 - val_loss: 0.2805 - val_acc: 0.9388\n",
      "Epoch 299/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0908 - acc: 0.9932 - val_loss: 0.3082 - val_acc: 0.9388\n",
      "Epoch 300/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0912 - acc: 0.9932 - val_loss: 0.2912 - val_acc: 0.9388\n",
      "Epoch 301/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0910 - acc: 0.9932 - val_loss: 0.3068 - val_acc: 0.9388\n",
      "Epoch 302/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0913 - acc: 0.9932 - val_loss: 0.3004 - val_acc: 0.9388\n",
      "Epoch 303/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0903 - acc: 0.9898 - val_loss: 0.3045 - val_acc: 0.9388\n",
      "Epoch 304/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0895 - acc: 0.9932 - val_loss: 0.3018 - val_acc: 0.9388\n",
      "Epoch 305/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0905 - acc: 0.9932 - val_loss: 0.2957 - val_acc: 0.9388\n",
      "Epoch 306/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0907 - acc: 0.9932 - val_loss: 0.3032 - val_acc: 0.9388\n",
      "Epoch 307/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0921 - acc: 0.9932 - val_loss: 0.2954 - val_acc: 0.9388\n",
      "Epoch 308/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0904 - acc: 0.9898 - val_loss: 0.3000 - val_acc: 0.9388\n",
      "Epoch 309/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0899 - acc: 0.9932 - val_loss: 0.3036 - val_acc: 0.9388\n",
      "Epoch 310/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0895 - acc: 0.9932 - val_loss: 0.3022 - val_acc: 0.9388\n",
      "Epoch 311/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0901 - acc: 0.9932 - val_loss: 0.2826 - val_acc: 0.9388\n",
      "Epoch 312/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0930 - acc: 0.9932 - val_loss: 0.3008 - val_acc: 0.9388\n",
      "Epoch 313/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0910 - acc: 0.9932 - val_loss: 0.2899 - val_acc: 0.9388\n",
      "Epoch 314/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0942 - acc: 0.9898 - val_loss: 0.3068 - val_acc: 0.9388\n",
      "Epoch 315/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0925 - acc: 0.9932 - val_loss: 0.3027 - val_acc: 0.9388\n",
      "Epoch 316/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0888 - acc: 0.9932 - val_loss: 0.3086 - val_acc: 0.9388\n",
      "Epoch 317/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0916 - acc: 0.9898 - val_loss: 0.2944 - val_acc: 0.9388\n",
      "Epoch 318/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0910 - acc: 0.9932 - val_loss: 0.3105 - val_acc: 0.9388\n",
      "Epoch 319/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0915 - acc: 0.9898 - val_loss: 0.2960 - val_acc: 0.9388\n",
      "Epoch 320/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0942 - acc: 0.9932 - val_loss: 0.2861 - val_acc: 0.9388\n",
      "Epoch 321/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0934 - acc: 0.9932 - val_loss: 0.3139 - val_acc: 0.9388\n",
      "Epoch 322/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.0920 - acc: 0.9932 - val_loss: 0.2867 - val_acc: 0.9388\n",
      "Epoch 323/500\n",
      "10/10 [==============================] - 0s 7ms/step - loss: 0.1139 - acc: 0.9830 - val_loss: 0.4926 - val_acc: 0.8776\n",
      "Epoch 324/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1473 - acc: 0.9524 - val_loss: 0.2907 - val_acc: 0.9286\n",
      "Epoch 325/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1232 - acc: 0.9762 - val_loss: 0.3790 - val_acc: 0.9184\n",
      "Epoch 326/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1061 - acc: 0.9796 - val_loss: 0.2970 - val_acc: 0.9388\n",
      "Epoch 327/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1056 - acc: 0.9864 - val_loss: 0.3428 - val_acc: 0.9184\n",
      "Epoch 328/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0976 - acc: 0.9898 - val_loss: 0.3336 - val_acc: 0.9286\n",
      "Epoch 329/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1063 - acc: 0.9796 - val_loss: 0.3577 - val_acc: 0.9184\n",
      "Epoch 330/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0950 - acc: 0.9898 - val_loss: 0.3099 - val_acc: 0.9388\n",
      "Epoch 331/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0920 - acc: 0.9932 - val_loss: 0.2972 - val_acc: 0.9388\n",
      "Epoch 332/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0956 - acc: 0.9932 - val_loss: 0.3086 - val_acc: 0.9388\n",
      "Epoch 333/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0976 - acc: 0.9864 - val_loss: 0.2934 - val_acc: 0.9388\n",
      "Epoch 334/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1073 - acc: 0.9796 - val_loss: 0.2861 - val_acc: 0.9388\n",
      "Epoch 335/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1038 - acc: 0.9898 - val_loss: 0.3068 - val_acc: 0.9388\n",
      "Epoch 336/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1015 - acc: 0.9830 - val_loss: 0.3211 - val_acc: 0.9388\n",
      "Epoch 337/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0892 - acc: 0.9932 - val_loss: 0.3000 - val_acc: 0.9388\n",
      "Epoch 338/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0920 - acc: 0.9932 - val_loss: 0.3000 - val_acc: 0.9388\n",
      "Epoch 339/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0899 - acc: 0.9932 - val_loss: 0.2968 - val_acc: 0.9388\n",
      "Epoch 340/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0892 - acc: 0.9932 - val_loss: 0.3103 - val_acc: 0.9388\n",
      "Epoch 341/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0915 - acc: 0.9932 - val_loss: 0.2915 - val_acc: 0.9388\n",
      "Epoch 342/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0912 - acc: 0.9932 - val_loss: 0.2937 - val_acc: 0.9388\n",
      "Epoch 343/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0913 - acc: 0.9898 - val_loss: 0.2919 - val_acc: 0.9388\n",
      "Epoch 344/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0895 - acc: 0.9932 - val_loss: 0.3105 - val_acc: 0.9388\n",
      "Epoch 345/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0917 - acc: 0.9898 - val_loss: 0.2924 - val_acc: 0.9388\n",
      "Epoch 346/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0942 - acc: 0.9932 - val_loss: 0.3185 - val_acc: 0.9388\n",
      "Epoch 347/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0930 - acc: 0.9932 - val_loss: 0.2900 - val_acc: 0.9388\n",
      "Epoch 348/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0891 - acc: 0.9932 - val_loss: 0.3113 - val_acc: 0.9388\n",
      "Epoch 349/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0896 - acc: 0.9932 - val_loss: 0.2998 - val_acc: 0.9388\n",
      "Epoch 350/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0879 - acc: 0.9932 - val_loss: 0.3155 - val_acc: 0.9388\n",
      "Epoch 351/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0875 - acc: 0.9932 - val_loss: 0.2964 - val_acc: 0.9388\n",
      "Epoch 352/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0897 - acc: 0.9932 - val_loss: 0.3209 - val_acc: 0.9388\n",
      "Epoch 353/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0932 - acc: 0.9830 - val_loss: 0.3010 - val_acc: 0.9388\n",
      "Epoch 354/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1001 - acc: 0.9864 - val_loss: 0.3228 - val_acc: 0.9388\n",
      "Epoch 355/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0938 - acc: 0.9898 - val_loss: 0.2950 - val_acc: 0.9388\n",
      "Epoch 356/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0919 - acc: 0.9932 - val_loss: 0.3062 - val_acc: 0.9388\n",
      "Epoch 357/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0877 - acc: 0.9932 - val_loss: 0.2888 - val_acc: 0.9388\n",
      "Epoch 358/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0880 - acc: 0.9932 - val_loss: 0.3088 - val_acc: 0.9388\n",
      "Epoch 359/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0944 - acc: 0.9932 - val_loss: 0.2959 - val_acc: 0.9388\n",
      "Epoch 360/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0955 - acc: 0.9898 - val_loss: 0.2849 - val_acc: 0.9490\n",
      "Epoch 361/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0911 - acc: 0.9932 - val_loss: 0.3078 - val_acc: 0.9388\n",
      "Epoch 362/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0883 - acc: 0.9932 - val_loss: 0.2945 - val_acc: 0.9388\n",
      "Epoch 363/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0885 - acc: 0.9932 - val_loss: 0.3081 - val_acc: 0.9388\n",
      "Epoch 364/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0877 - acc: 0.9932 - val_loss: 0.3038 - val_acc: 0.9388\n",
      "Epoch 365/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0874 - acc: 0.9932 - val_loss: 0.3016 - val_acc: 0.9388\n",
      "Epoch 366/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0875 - acc: 0.9932 - val_loss: 0.3011 - val_acc: 0.9388\n",
      "Epoch 367/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0872 - acc: 0.9932 - val_loss: 0.3004 - val_acc: 0.9388\n",
      "Epoch 368/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0869 - acc: 0.9932 - val_loss: 0.3148 - val_acc: 0.9388\n",
      "Epoch 369/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0888 - acc: 0.9932 - val_loss: 0.3094 - val_acc: 0.9388\n",
      "Epoch 370/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0864 - acc: 0.9932 - val_loss: 0.3013 - val_acc: 0.9388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 371/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0869 - acc: 0.9932 - val_loss: 0.3114 - val_acc: 0.9388\n",
      "Epoch 372/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0864 - acc: 0.9932 - val_loss: 0.3011 - val_acc: 0.9388\n",
      "Epoch 373/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0875 - acc: 0.9932 - val_loss: 0.3182 - val_acc: 0.9388\n",
      "Epoch 374/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0863 - acc: 0.9932 - val_loss: 0.2982 - val_acc: 0.9388\n",
      "Epoch 375/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0888 - acc: 0.9932 - val_loss: 0.3233 - val_acc: 0.9388\n",
      "Epoch 376/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0922 - acc: 0.9932 - val_loss: 0.2919 - val_acc: 0.9388\n",
      "Epoch 377/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0897 - acc: 0.9932 - val_loss: 0.3244 - val_acc: 0.9388\n",
      "Epoch 378/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0879 - acc: 0.9898 - val_loss: 0.3034 - val_acc: 0.9388\n",
      "Epoch 379/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0864 - acc: 0.9932 - val_loss: 0.3051 - val_acc: 0.9388\n",
      "Epoch 380/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0879 - acc: 0.9932 - val_loss: 0.3089 - val_acc: 0.9388\n",
      "Epoch 381/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0865 - acc: 0.9932 - val_loss: 0.3137 - val_acc: 0.9388\n",
      "Epoch 382/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0867 - acc: 0.9932 - val_loss: 0.3042 - val_acc: 0.9388\n",
      "Epoch 383/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0883 - acc: 0.9932 - val_loss: 0.3048 - val_acc: 0.9388\n",
      "Epoch 384/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0901 - acc: 0.9932 - val_loss: 0.3063 - val_acc: 0.9388\n",
      "Epoch 385/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0900 - acc: 0.9898 - val_loss: 0.2918 - val_acc: 0.9388\n",
      "Epoch 386/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0887 - acc: 0.9932 - val_loss: 0.3168 - val_acc: 0.9388\n",
      "Epoch 387/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0860 - acc: 0.9932 - val_loss: 0.2992 - val_acc: 0.9388\n",
      "Epoch 388/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0866 - acc: 0.9932 - val_loss: 0.3260 - val_acc: 0.9388\n",
      "Epoch 389/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0851 - acc: 0.9932 - val_loss: 0.2947 - val_acc: 0.9388\n",
      "Epoch 390/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0885 - acc: 0.9932 - val_loss: 0.3186 - val_acc: 0.9388\n",
      "Epoch 391/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0863 - acc: 0.9932 - val_loss: 0.2868 - val_acc: 0.9388\n",
      "Epoch 392/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0907 - acc: 0.9932 - val_loss: 0.3137 - val_acc: 0.9388\n",
      "Epoch 393/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0919 - acc: 0.9932 - val_loss: 0.2872 - val_acc: 0.9490\n",
      "Epoch 394/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0892 - acc: 0.9932 - val_loss: 0.3186 - val_acc: 0.9388\n",
      "Epoch 395/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0880 - acc: 0.9932 - val_loss: 0.2981 - val_acc: 0.9388\n",
      "Epoch 396/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0862 - acc: 0.9932 - val_loss: 0.3048 - val_acc: 0.9388\n",
      "Epoch 397/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0905 - acc: 0.9932 - val_loss: 0.2886 - val_acc: 0.9388\n",
      "Epoch 398/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0871 - acc: 0.9932 - val_loss: 0.3072 - val_acc: 0.9388\n",
      "Epoch 399/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0911 - acc: 0.9898 - val_loss: 0.2915 - val_acc: 0.9388\n",
      "Epoch 400/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0923 - acc: 0.9898 - val_loss: 0.3144 - val_acc: 0.9388\n",
      "Epoch 401/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0868 - acc: 0.9932 - val_loss: 0.3152 - val_acc: 0.9388\n",
      "Epoch 402/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0865 - acc: 0.9932 - val_loss: 0.3096 - val_acc: 0.9388\n",
      "Epoch 403/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0892 - acc: 0.9932 - val_loss: 0.3213 - val_acc: 0.9388\n",
      "Epoch 404/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0884 - acc: 0.9932 - val_loss: 0.2883 - val_acc: 0.9388\n",
      "Epoch 405/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0851 - acc: 0.9932 - val_loss: 0.3089 - val_acc: 0.9388\n",
      "Epoch 406/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0892 - acc: 0.9932 - val_loss: 0.3219 - val_acc: 0.9388\n",
      "Epoch 407/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0918 - acc: 0.9864 - val_loss: 0.2954 - val_acc: 0.9388\n",
      "Epoch 408/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0954 - acc: 0.9898 - val_loss: 0.2595 - val_acc: 0.9490\n",
      "Epoch 409/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0926 - acc: 0.9932 - val_loss: 0.3183 - val_acc: 0.9388\n",
      "Epoch 410/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0900 - acc: 0.9864 - val_loss: 0.2944 - val_acc: 0.9388\n",
      "Epoch 411/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0877 - acc: 0.9932 - val_loss: 0.2999 - val_acc: 0.9388\n",
      "Epoch 412/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0868 - acc: 0.9932 - val_loss: 0.2943 - val_acc: 0.9388\n",
      "Epoch 413/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0878 - acc: 0.9932 - val_loss: 0.3067 - val_acc: 0.9388\n",
      "Epoch 414/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0932 - acc: 0.9898 - val_loss: 0.2948 - val_acc: 0.9490\n",
      "Epoch 415/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0926 - acc: 0.9932 - val_loss: 0.2888 - val_acc: 0.9388\n",
      "Epoch 416/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0879 - acc: 0.9932 - val_loss: 0.3025 - val_acc: 0.9388\n",
      "Epoch 417/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0848 - acc: 0.9932 - val_loss: 0.3019 - val_acc: 0.9388\n",
      "Epoch 418/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0852 - acc: 0.9932 - val_loss: 0.3126 - val_acc: 0.9388\n",
      "Epoch 419/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0876 - acc: 0.9932 - val_loss: 0.2938 - val_acc: 0.9388\n",
      "Epoch 420/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0880 - acc: 0.9932 - val_loss: 0.3189 - val_acc: 0.9388\n",
      "Epoch 421/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0870 - acc: 0.9898 - val_loss: 0.2841 - val_acc: 0.9490\n",
      "Epoch 422/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0876 - acc: 0.9966 - val_loss: 0.2961 - val_acc: 0.9388\n",
      "Epoch 423/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0864 - acc: 0.9932 - val_loss: 0.3361 - val_acc: 0.9388\n",
      "Epoch 424/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0879 - acc: 0.9932 - val_loss: 0.2856 - val_acc: 0.9286\n",
      "Epoch 425/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1022 - acc: 0.9830 - val_loss: 0.3169 - val_acc: 0.9388\n",
      "Epoch 426/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0898 - acc: 0.9932 - val_loss: 0.2796 - val_acc: 0.9490\n",
      "Epoch 427/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0909 - acc: 0.9932 - val_loss: 0.3455 - val_acc: 0.9286\n",
      "Epoch 428/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0894 - acc: 0.9898 - val_loss: 0.2846 - val_acc: 0.9388\n",
      "Epoch 429/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0864 - acc: 0.9898 - val_loss: 0.3127 - val_acc: 0.9388\n",
      "Epoch 430/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0884 - acc: 0.9932 - val_loss: 0.3003 - val_acc: 0.9388\n",
      "Epoch 431/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0874 - acc: 0.9898 - val_loss: 0.3372 - val_acc: 0.9388\n",
      "Epoch 432/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1313 - acc: 0.9660 - val_loss: 0.3185 - val_acc: 0.9286\n",
      "Epoch 433/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1092 - acc: 0.9796 - val_loss: 0.3920 - val_acc: 0.9286\n",
      "Epoch 434/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1261 - acc: 0.9660 - val_loss: 0.2645 - val_acc: 0.9388\n",
      "Epoch 435/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.0973 - acc: 0.9864 - val_loss: 0.3165 - val_acc: 0.9388\n",
      "Epoch 436/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.1387 - acc: 0.9592 - val_loss: 0.4156 - val_acc: 0.9082\n",
      "Epoch 437/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1394 - acc: 0.9626 - val_loss: 0.2790 - val_acc: 0.8980\n",
      "Epoch 438/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1287 - acc: 0.9762 - val_loss: 0.3670 - val_acc: 0.9184\n",
      "Epoch 439/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1020 - acc: 0.9830 - val_loss: 0.2984 - val_acc: 0.9388\n",
      "Epoch 440/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0891 - acc: 0.9932 - val_loss: 0.3108 - val_acc: 0.9388\n",
      "Epoch 441/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0925 - acc: 0.9932 - val_loss: 0.2897 - val_acc: 0.9490\n",
      "Epoch 442/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0878 - acc: 0.9932 - val_loss: 0.3122 - val_acc: 0.9388\n",
      "Epoch 443/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.1012 - acc: 0.9898 - val_loss: 0.2958 - val_acc: 0.9388\n",
      "Epoch 444/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0865 - acc: 0.9898 - val_loss: 0.2961 - val_acc: 0.9388\n",
      "Epoch 445/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0860 - acc: 0.9932 - val_loss: 0.2945 - val_acc: 0.9388\n",
      "Epoch 446/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0889 - acc: 0.9898 - val_loss: 0.3082 - val_acc: 0.9388\n",
      "Epoch 447/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0885 - acc: 0.9932 - val_loss: 0.2927 - val_acc: 0.9388\n",
      "Epoch 448/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0874 - acc: 0.9932 - val_loss: 0.2939 - val_acc: 0.9388\n",
      "Epoch 449/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0851 - acc: 0.9932 - val_loss: 0.2886 - val_acc: 0.9388\n",
      "Epoch 450/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0842 - acc: 0.9932 - val_loss: 0.2894 - val_acc: 0.9388\n",
      "Epoch 451/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0841 - acc: 0.9932 - val_loss: 0.2988 - val_acc: 0.9388\n",
      "Epoch 452/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0850 - acc: 0.9932 - val_loss: 0.2934 - val_acc: 0.9388\n",
      "Epoch 453/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0834 - acc: 0.9932 - val_loss: 0.3030 - val_acc: 0.9388\n",
      "Epoch 454/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0832 - acc: 0.9932 - val_loss: 0.2999 - val_acc: 0.9388\n",
      "Epoch 455/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0856 - acc: 0.9932 - val_loss: 0.2934 - val_acc: 0.9388\n",
      "Epoch 456/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0934 - acc: 0.9864 - val_loss: 0.2968 - val_acc: 0.9388\n",
      "Epoch 457/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0881 - acc: 0.9932 - val_loss: 0.3104 - val_acc: 0.9388\n",
      "Epoch 458/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0859 - acc: 0.9932 - val_loss: 0.3020 - val_acc: 0.9388\n",
      "Epoch 459/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0873 - acc: 0.9932 - val_loss: 0.3159 - val_acc: 0.9388\n",
      "Epoch 460/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0873 - acc: 0.9898 - val_loss: 0.2929 - val_acc: 0.9388\n",
      "Epoch 461/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0854 - acc: 0.9932 - val_loss: 0.3246 - val_acc: 0.9388\n",
      "Epoch 462/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0846 - acc: 0.9966 - val_loss: 0.2956 - val_acc: 0.9388\n",
      "Epoch 463/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0836 - acc: 0.9932 - val_loss: 0.3067 - val_acc: 0.9388\n",
      "Epoch 464/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0828 - acc: 0.9932 - val_loss: 0.3084 - val_acc: 0.9388\n",
      "Epoch 465/500\n",
      "10/10 [==============================] - 0s 26ms/step - loss: 0.0833 - acc: 0.9932 - val_loss: 0.3036 - val_acc: 0.9388\n",
      "Epoch 466/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0844 - acc: 0.9932 - val_loss: 0.3063 - val_acc: 0.9388\n",
      "Epoch 467/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0836 - acc: 0.9932 - val_loss: 0.3072 - val_acc: 0.9388\n",
      "Epoch 468/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0850 - acc: 0.9932 - val_loss: 0.3189 - val_acc: 0.9388\n",
      "Epoch 469/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0831 - acc: 0.9932 - val_loss: 0.3132 - val_acc: 0.9388\n",
      "Epoch 470/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0826 - acc: 0.9932 - val_loss: 0.3066 - val_acc: 0.9388\n",
      "Epoch 471/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0818 - acc: 0.9932 - val_loss: 0.3127 - val_acc: 0.9388\n",
      "Epoch 472/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0835 - acc: 0.9932 - val_loss: 0.2994 - val_acc: 0.9388\n",
      "Epoch 473/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0940 - acc: 0.9932 - val_loss: 0.3153 - val_acc: 0.9388\n",
      "Epoch 474/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0908 - acc: 0.9932 - val_loss: 0.2978 - val_acc: 0.9388\n",
      "Epoch 475/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0880 - acc: 0.9932 - val_loss: 0.3316 - val_acc: 0.9388\n",
      "Epoch 476/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0850 - acc: 0.9932 - val_loss: 0.3020 - val_acc: 0.9388\n",
      "Epoch 477/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0829 - acc: 0.9932 - val_loss: 0.3179 - val_acc: 0.9388\n",
      "Epoch 478/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0816 - acc: 0.9932 - val_loss: 0.3198 - val_acc: 0.9388\n",
      "Epoch 479/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0838 - acc: 0.9932 - val_loss: 0.3161 - val_acc: 0.9388\n",
      "Epoch 480/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0829 - acc: 0.9932 - val_loss: 0.3012 - val_acc: 0.9388\n",
      "Epoch 481/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0826 - acc: 0.9932 - val_loss: 0.3042 - val_acc: 0.9388\n",
      "Epoch 482/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0859 - acc: 0.9932 - val_loss: 0.3260 - val_acc: 0.9388\n",
      "Epoch 483/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0839 - acc: 0.9932 - val_loss: 0.3077 - val_acc: 0.9388\n",
      "Epoch 484/500\n",
      "10/10 [==============================] - 0s 5ms/step - loss: 0.0838 - acc: 0.9932 - val_loss: 0.3201 - val_acc: 0.9388\n",
      "Epoch 485/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0830 - acc: 0.9932 - val_loss: 0.3147 - val_acc: 0.9388\n",
      "Epoch 486/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0837 - acc: 0.9932 - val_loss: 0.3107 - val_acc: 0.9388\n",
      "Epoch 487/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0810 - acc: 0.9966 - val_loss: 0.3268 - val_acc: 0.9388\n",
      "Epoch 488/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0806 - acc: 0.9932 - val_loss: 0.3001 - val_acc: 0.9388\n",
      "Epoch 489/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0842 - acc: 0.9932 - val_loss: 0.3253 - val_acc: 0.9388\n",
      "Epoch 490/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0829 - acc: 0.9966 - val_loss: 0.2962 - val_acc: 0.9388\n",
      "Epoch 491/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0902 - acc: 0.9932 - val_loss: 0.3555 - val_acc: 0.9388\n",
      "Epoch 492/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0931 - acc: 0.9864 - val_loss: 0.2577 - val_acc: 0.9388\n",
      "Epoch 493/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.1112 - acc: 0.9660 - val_loss: 0.3454 - val_acc: 0.9388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 494/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0829 - acc: 0.9932 - val_loss: 0.2720 - val_acc: 0.9388\n",
      "Epoch 495/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0866 - acc: 0.9898 - val_loss: 0.3360 - val_acc: 0.9388\n",
      "Epoch 496/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0880 - acc: 0.9898 - val_loss: 0.2810 - val_acc: 0.9490\n",
      "Epoch 497/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0850 - acc: 0.9932 - val_loss: 0.3063 - val_acc: 0.9388\n",
      "Epoch 498/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0819 - acc: 0.9932 - val_loss: 0.3054 - val_acc: 0.9388\n",
      "Epoch 499/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.0816 - acc: 0.9966 - val_loss: 0.3045 - val_acc: 0.9388\n",
      "Epoch 500/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.0845 - acc: 0.9932 - val_loss: 0.3035 - val_acc: 0.9388\n"
     ]
    }
   ],
   "source": [
    "encoded = encoder(data.x_train)\n",
    "hist = main_model.fit(encoded, data.y_train, \n",
    "                      validation_data=(encoder(data.x_test), data.y_test), \n",
    "                      epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'acc', 'val_loss', 'val_acc'])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(hist.history['val_acc']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x600 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.plot(hist.history['loss']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-62-b15e724c4b8d>:1: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7575757575757576,\n",
       " 0.12307692307692308,\n",
       " 0.8367346938775511,\n",
       " 0.7575757575757576]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = main_model.predict_classes(encoder(data.x_test))\n",
    "preds = preds.squeeze()\n",
    "metrics = ClassificationMetrics(data.y_test, preds)\n",
    "metrics.add_metrics(['pd', 'pf', 'accuracy', 'f1'])\n",
    "metrics.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we train further?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3832 - acc: 0.8265\n",
      "Epoch 2/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3840 - acc: 0.8299\n",
      "Epoch 3/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3832 - acc: 0.8231\n",
      "Epoch 4/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3829 - acc: 0.8231\n",
      "Epoch 5/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3831 - acc: 0.8265\n",
      "Epoch 6/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3832 - acc: 0.8265\n",
      "Epoch 7/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3822 - acc: 0.8265\n",
      "Epoch 8/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3829 - acc: 0.8367\n",
      "Epoch 9/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.3821 - acc: 0.8333\n",
      "Epoch 10/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3830 - acc: 0.8197\n",
      "Epoch 11/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3829 - acc: 0.8197\n",
      "Epoch 12/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3826 - acc: 0.8231\n",
      "Epoch 13/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3827 - acc: 0.8333\n",
      "Epoch 14/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3823 - acc: 0.8333\n",
      "Epoch 15/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3823 - acc: 0.8333\n",
      "Epoch 16/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3829 - acc: 0.8231\n",
      "Epoch 17/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3817 - acc: 0.8265\n",
      "Epoch 18/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3837 - acc: 0.8299\n",
      "Epoch 19/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3833 - acc: 0.8265\n",
      "Epoch 20/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3825 - acc: 0.8333\n",
      "Epoch 21/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3829 - acc: 0.8299\n",
      "Epoch 22/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3822 - acc: 0.8333\n",
      "Epoch 23/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3845 - acc: 0.8197\n",
      "Epoch 24/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3823 - acc: 0.8197\n",
      "Epoch 25/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3840 - acc: 0.8299\n",
      "Epoch 26/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3842 - acc: 0.8231\n",
      "Epoch 27/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3823 - acc: 0.8197\n",
      "Epoch 28/500\n",
      "10/10 [==============================] - ETA: 0s - loss: 0.3251 - acc: 0.843 - 0s 1ms/step - loss: 0.3830 - acc: 0.8197\n",
      "Epoch 29/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3827 - acc: 0.8231\n",
      "Epoch 30/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3826 - acc: 0.8231\n",
      "Epoch 31/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3819 - acc: 0.8197\n",
      "Epoch 32/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3828 - acc: 0.8197\n",
      "Epoch 33/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3821 - acc: 0.8265\n",
      "Epoch 34/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3826 - acc: 0.8231\n",
      "Epoch 35/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3831 - acc: 0.8231\n",
      "Epoch 36/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3824 - acc: 0.8231\n",
      "Epoch 37/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3830 - acc: 0.8197\n",
      "Epoch 38/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3826 - acc: 0.8231\n",
      "Epoch 39/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3823 - acc: 0.8265\n",
      "Epoch 40/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3826 - acc: 0.8299\n",
      "Epoch 41/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3839 - acc: 0.8197\n",
      "Epoch 42/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3821 - acc: 0.8197\n",
      "Epoch 43/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3821 - acc: 0.8265\n",
      "Epoch 44/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3823 - acc: 0.8333\n",
      "Epoch 45/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3853 - acc: 0.8333\n",
      "Epoch 46/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3861 - acc: 0.8265\n",
      "Epoch 47/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3831 - acc: 0.8163\n",
      "Epoch 48/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3827 - acc: 0.8197\n",
      "Epoch 49/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3828 - acc: 0.8367\n",
      "Epoch 50/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3827 - acc: 0.8299\n",
      "Epoch 51/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3818 - acc: 0.8265\n",
      "Epoch 52/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3824 - acc: 0.8401\n",
      "Epoch 53/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3830 - acc: 0.8265\n",
      "Epoch 54/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3818 - acc: 0.8197\n",
      "Epoch 55/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3812 - acc: 0.8367\n",
      "Epoch 56/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3818 - acc: 0.8367\n",
      "Epoch 57/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3822 - acc: 0.8333\n",
      "Epoch 58/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3809 - acc: 0.8333\n",
      "Epoch 59/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3822 - acc: 0.8299\n",
      "Epoch 60/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3863 - acc: 0.8129\n",
      "Epoch 61/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3829 - acc: 0.8197\n",
      "Epoch 62/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3819 - acc: 0.8231\n",
      "Epoch 63/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3814 - acc: 0.8333\n",
      "Epoch 64/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3816 - acc: 0.8197\n",
      "Epoch 65/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3824 - acc: 0.8197\n",
      "Epoch 66/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3823 - acc: 0.8197\n",
      "Epoch 67/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3817 - acc: 0.8299\n",
      "Epoch 68/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3816 - acc: 0.8333\n",
      "Epoch 69/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3814 - acc: 0.8333\n",
      "Epoch 70/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3817 - acc: 0.8333\n",
      "Epoch 71/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3813 - acc: 0.8333\n",
      "Epoch 72/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3815 - acc: 0.8333\n",
      "Epoch 73/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3813 - acc: 0.8197\n",
      "Epoch 74/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3821 - acc: 0.8197\n",
      "Epoch 75/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3810 - acc: 0.8333\n",
      "Epoch 76/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3819 - acc: 0.8435\n",
      "Epoch 77/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3811 - acc: 0.8265\n",
      "Epoch 78/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3816 - acc: 0.8197\n",
      "Epoch 79/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3811 - acc: 0.8299\n",
      "Epoch 80/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3812 - acc: 0.8333\n",
      "Epoch 81/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3815 - acc: 0.8333\n",
      "Epoch 82/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3819 - acc: 0.8333\n",
      "Epoch 83/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3828 - acc: 0.8333\n",
      "Epoch 84/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3827 - acc: 0.8299\n",
      "Epoch 85/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3836 - acc: 0.8265\n",
      "Epoch 86/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3829 - acc: 0.8265\n",
      "Epoch 87/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3811 - acc: 0.8197\n",
      "Epoch 88/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3811 - acc: 0.8231\n",
      "Epoch 89/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3809 - acc: 0.8333\n",
      "Epoch 90/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3811 - acc: 0.8367\n",
      "Epoch 91/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3813 - acc: 0.8367\n",
      "Epoch 92/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3813 - acc: 0.8367\n",
      "Epoch 93/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3813 - acc: 0.8367\n",
      "Epoch 94/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3811 - acc: 0.8367\n",
      "Epoch 95/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3810 - acc: 0.8265\n",
      "Epoch 96/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3809 - acc: 0.8197\n",
      "Epoch 97/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3811 - acc: 0.8299\n",
      "Epoch 98/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3807 - acc: 0.8265\n",
      "Epoch 99/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3823 - acc: 0.8197\n",
      "Epoch 100/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3830 - acc: 0.8129\n",
      "Epoch 101/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3819 - acc: 0.8197\n",
      "Epoch 102/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3810 - acc: 0.8197\n",
      "Epoch 103/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3810 - acc: 0.8197\n",
      "Epoch 104/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3807 - acc: 0.8197\n",
      "Epoch 105/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3806 - acc: 0.8367\n",
      "Epoch 106/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3822 - acc: 0.8367\n",
      "Epoch 107/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3813 - acc: 0.8299\n",
      "Epoch 108/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3839 - acc: 0.8095\n",
      "Epoch 109/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3827 - acc: 0.8197\n",
      "Epoch 110/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3811 - acc: 0.8333\n",
      "Epoch 111/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3811 - acc: 0.8299\n",
      "Epoch 112/500\n",
      "10/10 [==============================] - 0s 946us/step - loss: 0.3823 - acc: 0.8367\n",
      "Epoch 113/500\n",
      "10/10 [==============================] - 0s 979us/step - loss: 0.3828 - acc: 0.8197\n",
      "Epoch 114/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3809 - acc: 0.8197\n",
      "Epoch 115/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3807 - acc: 0.8367\n",
      "Epoch 116/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3812 - acc: 0.8367\n",
      "Epoch 117/500\n",
      "10/10 [==============================] - 0s 929us/step - loss: 0.3808 - acc: 0.8299\n",
      "Epoch 118/500\n",
      "10/10 [==============================] - 0s 967us/step - loss: 0.3811 - acc: 0.8367\n",
      "Epoch 119/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3809 - acc: 0.8435\n",
      "Epoch 120/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3809 - acc: 0.8333\n",
      "Epoch 121/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3809 - acc: 0.8231\n",
      "Epoch 122/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3808 - acc: 0.8197\n",
      "Epoch 123/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3810 - acc: 0.8333\n",
      "Epoch 124/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3805 - acc: 0.8333\n",
      "Epoch 125/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3814 - acc: 0.8265\n",
      "Epoch 126/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3807 - acc: 0.8231\n",
      "Epoch 127/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3831 - acc: 0.8401\n",
      "Epoch 128/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3803 - acc: 0.8367\n",
      "Epoch 129/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3803 - acc: 0.8231\n",
      "Epoch 130/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3812 - acc: 0.8197\n",
      "Epoch 131/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3804 - acc: 0.8231\n",
      "Epoch 132/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3807 - acc: 0.8231\n",
      "Epoch 133/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3803 - acc: 0.8299\n",
      "Epoch 134/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3807 - acc: 0.8299\n",
      "Epoch 135/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3804 - acc: 0.8265\n",
      "Epoch 136/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3805 - acc: 0.8333\n",
      "Epoch 137/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3808 - acc: 0.8435\n",
      "Epoch 138/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3793 - acc: 0.8333\n",
      "Epoch 139/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3812 - acc: 0.8197\n",
      "Epoch 140/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3802 - acc: 0.8231\n",
      "Epoch 141/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3805 - acc: 0.8299\n",
      "Epoch 142/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3809 - acc: 0.8265\n",
      "Epoch 143/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3812 - acc: 0.8197\n",
      "Epoch 144/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3799 - acc: 0.8231\n",
      "Epoch 145/500\n",
      "10/10 [==============================] - 0s 990us/step - loss: 0.3802 - acc: 0.8333\n",
      "Epoch 146/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3803 - acc: 0.8231\n",
      "Epoch 147/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3808 - acc: 0.8333\n",
      "Epoch 148/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3806 - acc: 0.8333\n",
      "Epoch 149/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3805 - acc: 0.8367\n",
      "Epoch 150/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3801 - acc: 0.8367\n",
      "Epoch 151/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3799 - acc: 0.8367\n",
      "Epoch 152/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3806 - acc: 0.8333\n",
      "Epoch 153/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3809 - acc: 0.8401\n",
      "Epoch 154/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3806 - acc: 0.8299\n",
      "Epoch 155/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3799 - acc: 0.8231\n",
      "Epoch 156/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3800 - acc: 0.8299\n",
      "Epoch 157/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3795 - acc: 0.8367\n",
      "Epoch 158/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3805 - acc: 0.8367\n",
      "Epoch 159/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3793 - acc: 0.8231\n",
      "Epoch 160/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3800 - acc: 0.8197\n",
      "Epoch 161/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3802 - acc: 0.8197\n",
      "Epoch 162/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3805 - acc: 0.8265\n",
      "Epoch 163/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3801 - acc: 0.8333\n",
      "Epoch 164/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3792 - acc: 0.8333\n",
      "Epoch 165/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3801 - acc: 0.8197\n",
      "Epoch 166/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3817 - acc: 0.8197\n",
      "Epoch 167/500\n",
      "10/10 [==============================] - 0s 979us/step - loss: 0.3806 - acc: 0.8231\n",
      "Epoch 168/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3806 - acc: 0.8231\n",
      "Epoch 169/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3796 - acc: 0.8231\n",
      "Epoch 170/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3799 - acc: 0.8367\n",
      "Epoch 171/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3801 - acc: 0.8333\n",
      "Epoch 172/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3799 - acc: 0.8299\n",
      "Epoch 173/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3797 - acc: 0.8299\n",
      "Epoch 174/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3798 - acc: 0.8197\n",
      "Epoch 175/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3800 - acc: 0.8197\n",
      "Epoch 176/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3795 - acc: 0.8197\n",
      "Epoch 177/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3798 - acc: 0.8299\n",
      "Epoch 178/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3800 - acc: 0.8333\n",
      "Epoch 179/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3820 - acc: 0.8265\n",
      "Epoch 180/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3794 - acc: 0.8299\n",
      "Epoch 181/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3798 - acc: 0.8299\n",
      "Epoch 182/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3795 - acc: 0.8265\n",
      "Epoch 183/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3793 - acc: 0.8299\n",
      "Epoch 184/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3796 - acc: 0.8197\n",
      "Epoch 185/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3795 - acc: 0.8265\n",
      "Epoch 186/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3794 - acc: 0.8333\n",
      "Epoch 187/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3796 - acc: 0.8333\n",
      "Epoch 188/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3792 - acc: 0.8333\n",
      "Epoch 189/500\n",
      "10/10 [==============================] - 0s 949us/step - loss: 0.3799 - acc: 0.8197\n",
      "Epoch 190/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3813 - acc: 0.8163\n",
      "Epoch 191/500\n",
      "10/10 [==============================] - 0s 991us/step - loss: 0.3805 - acc: 0.8299\n",
      "Epoch 192/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3801 - acc: 0.8401\n",
      "Epoch 193/500\n",
      "10/10 [==============================] - 0s 914us/step - loss: 0.3806 - acc: 0.8299\n",
      "Epoch 194/500\n",
      "10/10 [==============================] - 0s 975us/step - loss: 0.3823 - acc: 0.8197\n",
      "Epoch 195/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3810 - acc: 0.8231\n",
      "Epoch 196/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3817 - acc: 0.8333\n",
      "Epoch 197/500\n",
      "10/10 [==============================] - 0s 908us/step - loss: 0.3812 - acc: 0.8299\n",
      "Epoch 198/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3800 - acc: 0.8299\n",
      "Epoch 199/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3796 - acc: 0.8197\n",
      "Epoch 200/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3796 - acc: 0.8299\n",
      "Epoch 201/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3794 - acc: 0.8231\n",
      "Epoch 202/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3795 - acc: 0.8265\n",
      "Epoch 203/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3796 - acc: 0.8299\n",
      "Epoch 204/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3794 - acc: 0.8197\n",
      "Epoch 205/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3814 - acc: 0.8197\n",
      "Epoch 206/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3821 - acc: 0.8265\n",
      "Epoch 207/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3808 - acc: 0.8333\n",
      "Epoch 208/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3800 - acc: 0.8299\n",
      "Epoch 209/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3800 - acc: 0.8197\n",
      "Epoch 210/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3803 - acc: 0.8265\n",
      "Epoch 211/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3802 - acc: 0.8231\n",
      "Epoch 212/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3802 - acc: 0.8333\n",
      "Epoch 213/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3820 - acc: 0.8333\n",
      "Epoch 214/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3798 - acc: 0.8265\n",
      "Epoch 215/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3792 - acc: 0.8197\n",
      "Epoch 216/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3806 - acc: 0.8265\n",
      "Epoch 217/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3800 - acc: 0.8333\n",
      "Epoch 218/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3836 - acc: 0.8129\n",
      "Epoch 219/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3800 - acc: 0.8265\n",
      "Epoch 220/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3794 - acc: 0.8299\n",
      "Epoch 221/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3785 - acc: 0.8333\n",
      "Epoch 222/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3788 - acc: 0.8231\n",
      "Epoch 223/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3803 - acc: 0.8265\n",
      "Epoch 224/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3792 - acc: 0.8197\n",
      "Epoch 225/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3827 - acc: 0.8231\n",
      "Epoch 226/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3795 - acc: 0.8231\n",
      "Epoch 227/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3821 - acc: 0.8163\n",
      "Epoch 228/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3793 - acc: 0.8299\n",
      "Epoch 229/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3798 - acc: 0.8333\n",
      "Epoch 230/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3801 - acc: 0.8333\n",
      "Epoch 231/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3804 - acc: 0.8299\n",
      "Epoch 232/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3816 - acc: 0.8197\n",
      "Epoch 233/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3793 - acc: 0.8197\n",
      "Epoch 234/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3792 - acc: 0.8265\n",
      "Epoch 235/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3787 - acc: 0.8299\n",
      "Epoch 236/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3791 - acc: 0.8401\n",
      "Epoch 237/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3795 - acc: 0.8401\n",
      "Epoch 238/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3795 - acc: 0.8435\n",
      "Epoch 239/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3788 - acc: 0.8333\n",
      "Epoch 240/500\n",
      "10/10 [==============================] - 0s 6ms/step - loss: 0.3794 - acc: 0.8197\n",
      "Epoch 241/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3789 - acc: 0.8197\n",
      "Epoch 242/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3793 - acc: 0.8197\n",
      "Epoch 243/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3787 - acc: 0.8231\n",
      "Epoch 244/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3796 - acc: 0.8299\n",
      "Epoch 245/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3794 - acc: 0.8265\n",
      "Epoch 246/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3791 - acc: 0.8197\n",
      "Epoch 247/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3790 - acc: 0.8197\n",
      "Epoch 248/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3789 - acc: 0.8197\n",
      "Epoch 249/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3791 - acc: 0.8231\n",
      "Epoch 250/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3785 - acc: 0.8197\n",
      "Epoch 251/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3800 - acc: 0.8197\n",
      "Epoch 252/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3785 - acc: 0.8197\n",
      "Epoch 253/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3793 - acc: 0.8333\n",
      "Epoch 254/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3790 - acc: 0.8299\n",
      "Epoch 255/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3778 - acc: 0.8333\n",
      "Epoch 256/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3804 - acc: 0.8163\n",
      "Epoch 257/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3781 - acc: 0.8367\n",
      "Epoch 258/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3817 - acc: 0.8265\n",
      "Epoch 259/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3803 - acc: 0.8333\n",
      "Epoch 260/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3800 - acc: 0.8333\n",
      "Epoch 261/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3822 - acc: 0.8197\n",
      "Epoch 262/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3799 - acc: 0.8299\n",
      "Epoch 263/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3781 - acc: 0.8265\n",
      "Epoch 264/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3802 - acc: 0.8163\n",
      "Epoch 265/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3815 - acc: 0.8061\n",
      "Epoch 266/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3804 - acc: 0.8299\n",
      "Epoch 267/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3795 - acc: 0.8333\n",
      "Epoch 268/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3783 - acc: 0.8299\n",
      "Epoch 269/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3797 - acc: 0.8231\n",
      "Epoch 270/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3788 - acc: 0.8129\n",
      "Epoch 271/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3797 - acc: 0.8265\n",
      "Epoch 272/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3789 - acc: 0.8333\n",
      "Epoch 273/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3799 - acc: 0.8265\n",
      "Epoch 274/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3800 - acc: 0.8265\n",
      "Epoch 275/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3787 - acc: 0.8367\n",
      "Epoch 276/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3785 - acc: 0.8231\n",
      "Epoch 277/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3788 - acc: 0.8197\n",
      "Epoch 278/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3801 - acc: 0.8129\n",
      "Epoch 279/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3797 - acc: 0.8197\n",
      "Epoch 280/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3783 - acc: 0.8333\n",
      "Epoch 281/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3787 - acc: 0.8367\n",
      "Epoch 282/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3787 - acc: 0.8197\n",
      "Epoch 283/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3778 - acc: 0.8197\n",
      "Epoch 284/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3781 - acc: 0.8333\n",
      "Epoch 285/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3785 - acc: 0.8197\n",
      "Epoch 286/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3784 - acc: 0.8197\n",
      "Epoch 287/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3788 - acc: 0.8197\n",
      "Epoch 288/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3784 - acc: 0.8197\n",
      "Epoch 289/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3779 - acc: 0.8265\n",
      "Epoch 290/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3779 - acc: 0.8231\n",
      "Epoch 291/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3778 - acc: 0.8231\n",
      "Epoch 292/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3777 - acc: 0.8299\n",
      "Epoch 293/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3782 - acc: 0.8231\n",
      "Epoch 294/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3788 - acc: 0.8265\n",
      "Epoch 295/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3779 - acc: 0.8231\n",
      "Epoch 296/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3780 - acc: 0.8265\n",
      "Epoch 297/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3785 - acc: 0.8231\n",
      "Epoch 298/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3778 - acc: 0.8299\n",
      "Epoch 299/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3782 - acc: 0.8299\n",
      "Epoch 300/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3778 - acc: 0.8197\n",
      "Epoch 301/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3777 - acc: 0.8299\n",
      "Epoch 302/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3794 - acc: 0.8231\n",
      "Epoch 303/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3770 - acc: 0.8333\n",
      "Epoch 304/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3790 - acc: 0.8367\n",
      "Epoch 305/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3818 - acc: 0.8299\n",
      "Epoch 306/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3783 - acc: 0.8265\n",
      "Epoch 307/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3818 - acc: 0.8095\n",
      "Epoch 308/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3809 - acc: 0.8095\n",
      "Epoch 309/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3780 - acc: 0.8299\n",
      "Epoch 310/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3787 - acc: 0.8367\n",
      "Epoch 311/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3774 - acc: 0.8299\n",
      "Epoch 312/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3775 - acc: 0.8197\n",
      "Epoch 313/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3787 - acc: 0.8197\n",
      "Epoch 314/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3772 - acc: 0.8333\n",
      "Epoch 315/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3776 - acc: 0.8333\n",
      "Epoch 316/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3774 - acc: 0.8333\n",
      "Epoch 317/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3775 - acc: 0.8197\n",
      "Epoch 318/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3776 - acc: 0.8333\n",
      "Epoch 319/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3783 - acc: 0.8367\n",
      "Epoch 320/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3778 - acc: 0.8299\n",
      "Epoch 321/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3778 - acc: 0.8197\n",
      "Epoch 322/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3775 - acc: 0.8299\n",
      "Epoch 323/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3773 - acc: 0.8299\n",
      "Epoch 324/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3783 - acc: 0.8333\n",
      "Epoch 325/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3771 - acc: 0.8197\n",
      "Epoch 326/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3790 - acc: 0.8197\n",
      "Epoch 327/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3772 - acc: 0.8197\n",
      "Epoch 328/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3778 - acc: 0.8231\n",
      "Epoch 329/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3814 - acc: 0.8231\n",
      "Epoch 330/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3814 - acc: 0.8231\n",
      "Epoch 331/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3774 - acc: 0.8197\n",
      "Epoch 332/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3783 - acc: 0.8299\n",
      "Epoch 333/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3784 - acc: 0.8333\n",
      "Epoch 334/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3770 - acc: 0.8299\n",
      "Epoch 335/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3777 - acc: 0.8197\n",
      "Epoch 336/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3782 - acc: 0.8163\n",
      "Epoch 337/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3782 - acc: 0.8265\n",
      "Epoch 338/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3778 - acc: 0.8333\n",
      "Epoch 339/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3776 - acc: 0.8231\n",
      "Epoch 340/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3764 - acc: 0.8333\n",
      "Epoch 341/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3773 - acc: 0.8367\n",
      "Epoch 342/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3796 - acc: 0.8129\n",
      "Epoch 343/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3783 - acc: 0.8163\n",
      "Epoch 344/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3771 - acc: 0.8435\n",
      "Epoch 345/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3779 - acc: 0.8299\n",
      "Epoch 346/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3779 - acc: 0.8367\n",
      "Epoch 347/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3769 - acc: 0.8333\n",
      "Epoch 348/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3768 - acc: 0.8333\n",
      "Epoch 349/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3772 - acc: 0.8401\n",
      "Epoch 350/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3772 - acc: 0.8299\n",
      "Epoch 351/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3805 - acc: 0.8129\n",
      "Epoch 352/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3775 - acc: 0.8197\n",
      "Epoch 353/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3776 - acc: 0.8299\n",
      "Epoch 354/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3778 - acc: 0.8299\n",
      "Epoch 355/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3777 - acc: 0.8333\n",
      "Epoch 356/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3769 - acc: 0.8367\n",
      "Epoch 357/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3768 - acc: 0.8265\n",
      "Epoch 358/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3775 - acc: 0.8197\n",
      "Epoch 359/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3781 - acc: 0.8333\n",
      "Epoch 360/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3779 - acc: 0.8367\n",
      "Epoch 361/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3769 - acc: 0.8333\n",
      "Epoch 362/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3768 - acc: 0.8197\n",
      "Epoch 363/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3770 - acc: 0.8197\n",
      "Epoch 364/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3798 - acc: 0.8129\n",
      "Epoch 365/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3752 - acc: 0.8333\n",
      "Epoch 366/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3778 - acc: 0.8367\n",
      "Epoch 367/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3786 - acc: 0.8333\n",
      "Epoch 368/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3779 - acc: 0.8401\n",
      "Epoch 369/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3768 - acc: 0.8333\n",
      "Epoch 370/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3767 - acc: 0.8231\n",
      "Epoch 371/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3775 - acc: 0.8435\n",
      "Epoch 372/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3820 - acc: 0.8197\n",
      "Epoch 373/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3787 - acc: 0.8401\n",
      "Epoch 374/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3764 - acc: 0.8333\n",
      "Epoch 375/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3764 - acc: 0.8265\n",
      "Epoch 376/500\n",
      "10/10 [==============================] - 0s 4ms/step - loss: 0.3766 - acc: 0.8265\n",
      "Epoch 377/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3770 - acc: 0.8299\n",
      "Epoch 378/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3786 - acc: 0.8299\n",
      "Epoch 379/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3764 - acc: 0.8299\n",
      "Epoch 380/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3765 - acc: 0.8333\n",
      "Epoch 381/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3762 - acc: 0.8299\n",
      "Epoch 382/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3774 - acc: 0.8231\n",
      "Epoch 383/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3770 - acc: 0.8367\n",
      "Epoch 384/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3782 - acc: 0.8163\n",
      "Epoch 385/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3803 - acc: 0.8027\n",
      "Epoch 386/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3790 - acc: 0.8095\n",
      "Epoch 387/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3771 - acc: 0.8299\n",
      "Epoch 388/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3768 - acc: 0.8401\n",
      "Epoch 389/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3765 - acc: 0.8333\n",
      "Epoch 390/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3766 - acc: 0.8333\n",
      "Epoch 391/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3761 - acc: 0.8367\n",
      "Epoch 392/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3767 - acc: 0.8231\n",
      "Epoch 393/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3781 - acc: 0.8163\n",
      "Epoch 394/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3769 - acc: 0.8265\n",
      "Epoch 395/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3765 - acc: 0.8435\n",
      "Epoch 396/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3764 - acc: 0.8435\n",
      "Epoch 397/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3761 - acc: 0.8367\n",
      "Epoch 398/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3761 - acc: 0.8367\n",
      "Epoch 399/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3762 - acc: 0.8265\n",
      "Epoch 400/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3764 - acc: 0.8265\n",
      "Epoch 401/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3760 - acc: 0.8299\n",
      "Epoch 402/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3761 - acc: 0.8333\n",
      "Epoch 403/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3781 - acc: 0.8333\n",
      "Epoch 404/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3761 - acc: 0.8299\n",
      "Epoch 405/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3760 - acc: 0.8231\n",
      "Epoch 406/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3757 - acc: 0.8333\n",
      "Epoch 407/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3773 - acc: 0.8401\n",
      "Epoch 408/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3764 - acc: 0.8367\n",
      "Epoch 409/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3757 - acc: 0.8333\n",
      "Epoch 410/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3760 - acc: 0.8333\n",
      "Epoch 411/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3761 - acc: 0.8367\n",
      "Epoch 412/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3773 - acc: 0.8333\n",
      "Epoch 413/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3762 - acc: 0.8299\n",
      "Epoch 414/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3760 - acc: 0.8231\n",
      "Epoch 415/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3760 - acc: 0.8231\n",
      "Epoch 416/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3763 - acc: 0.8333\n",
      "Epoch 417/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3755 - acc: 0.8333\n",
      "Epoch 418/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3751 - acc: 0.8333\n",
      "Epoch 419/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3768 - acc: 0.8299\n",
      "Epoch 420/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3791 - acc: 0.8333\n",
      "Epoch 421/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3758 - acc: 0.8367\n",
      "Epoch 422/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3784 - acc: 0.8129\n",
      "Epoch 423/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3772 - acc: 0.8197\n",
      "Epoch 424/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3762 - acc: 0.8333\n",
      "Epoch 425/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3777 - acc: 0.8367\n",
      "Epoch 426/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3765 - acc: 0.8299\n",
      "Epoch 427/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3760 - acc: 0.8231\n",
      "Epoch 428/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3758 - acc: 0.8265\n",
      "Epoch 429/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3756 - acc: 0.8401\n",
      "Epoch 430/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3753 - acc: 0.8333\n",
      "Epoch 431/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3753 - acc: 0.8367\n",
      "Epoch 432/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3747 - acc: 0.8333\n",
      "Epoch 433/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3774 - acc: 0.8231\n",
      "Epoch 434/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3759 - acc: 0.8231\n",
      "Epoch 435/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3760 - acc: 0.8401\n",
      "Epoch 436/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3763 - acc: 0.8367\n",
      "Epoch 437/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3763 - acc: 0.8367\n",
      "Epoch 438/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3756 - acc: 0.8265\n",
      "Epoch 439/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3756 - acc: 0.8231\n",
      "Epoch 440/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3755 - acc: 0.8231\n",
      "Epoch 441/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3753 - acc: 0.8231\n",
      "Epoch 442/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3763 - acc: 0.8299\n",
      "Epoch 443/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3760 - acc: 0.8333\n",
      "Epoch 444/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3756 - acc: 0.8299\n",
      "Epoch 445/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3749 - acc: 0.8333\n",
      "Epoch 446/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3756 - acc: 0.8265\n",
      "Epoch 447/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3762 - acc: 0.8231\n",
      "Epoch 448/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3766 - acc: 0.8367\n",
      "Epoch 449/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3743 - acc: 0.8333\n",
      "Epoch 450/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3751 - acc: 0.8299\n",
      "Epoch 451/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3759 - acc: 0.8333\n",
      "Epoch 452/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3745 - acc: 0.8367\n",
      "Epoch 453/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3747 - acc: 0.8333\n",
      "Epoch 454/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3746 - acc: 0.8367\n",
      "Epoch 455/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3776 - acc: 0.8333\n",
      "Epoch 456/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3745 - acc: 0.8401\n",
      "Epoch 457/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3762 - acc: 0.8367\n",
      "Epoch 458/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3745 - acc: 0.8333\n",
      "Epoch 459/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3758 - acc: 0.8231\n",
      "Epoch 460/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3759 - acc: 0.8197\n",
      "Epoch 461/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3749 - acc: 0.8265\n",
      "Epoch 462/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3751 - acc: 0.8299\n",
      "Epoch 463/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3749 - acc: 0.8367\n",
      "Epoch 464/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3754 - acc: 0.8367\n",
      "Epoch 465/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3753 - acc: 0.8367\n",
      "Epoch 466/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3751 - acc: 0.8367\n",
      "Epoch 467/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3746 - acc: 0.8299\n",
      "Epoch 468/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3749 - acc: 0.8265\n",
      "Epoch 469/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3747 - acc: 0.8197\n",
      "Epoch 470/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3756 - acc: 0.8197\n",
      "Epoch 471/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3748 - acc: 0.8265\n",
      "Epoch 472/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3751 - acc: 0.8299\n",
      "Epoch 473/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3743 - acc: 0.8333\n",
      "Epoch 474/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3736 - acc: 0.8333\n",
      "Epoch 475/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3759 - acc: 0.8231\n",
      "Epoch 476/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3755 - acc: 0.8163\n",
      "Epoch 477/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3740 - acc: 0.8333\n",
      "Epoch 478/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3742 - acc: 0.8333\n",
      "Epoch 479/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3749 - acc: 0.8401\n",
      "Epoch 480/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3745 - acc: 0.8401\n",
      "Epoch 481/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3742 - acc: 0.8367\n",
      "Epoch 482/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3740 - acc: 0.8333\n",
      "Epoch 483/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3743 - acc: 0.8333\n",
      "Epoch 484/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3742 - acc: 0.8333\n",
      "Epoch 485/500\n",
      "10/10 [==============================] - 0s 3ms/step - loss: 0.3741 - acc: 0.8333\n",
      "Epoch 486/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3747 - acc: 0.8299\n",
      "Epoch 487/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3757 - acc: 0.8367\n",
      "Epoch 488/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3759 - acc: 0.8367\n",
      "Epoch 489/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3759 - acc: 0.8367\n",
      "Epoch 490/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3736 - acc: 0.8367\n",
      "Epoch 491/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3736 - acc: 0.8367\n",
      "Epoch 492/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3733 - acc: 0.8299\n",
      "Epoch 493/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3735 - acc: 0.8333\n",
      "Epoch 494/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3742 - acc: 0.8367\n",
      "Epoch 495/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3745 - acc: 0.8333\n",
      "Epoch 496/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3755 - acc: 0.8367\n",
      "Epoch 497/500\n",
      "10/10 [==============================] - 0s 1ms/step - loss: 0.3747 - acc: 0.8299\n",
      "Epoch 498/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3775 - acc: 0.8163\n",
      "Epoch 499/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3748 - acc: 0.8197\n",
      "Epoch 500/500\n",
      "10/10 [==============================] - 0s 2ms/step - loss: 0.3730 - acc: 0.8265\n"
     ]
    }
   ],
   "source": [
    "hist = main_model.fit(encoded, data.y_train, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7575757575757576,\n",
       " 0.1076923076923077,\n",
       " 0.8469387755102041,\n",
       " 0.7692307692307692]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = main_model.predict_classes(encoder(data.x_test))\n",
    "preds = preds.squeeze()\n",
    "metrics = ClassificationMetrics(data.y_test, preds)\n",
    "metrics.add_metrics(['pd', 'pf', 'accuracy', 'f1'])\n",
    "metrics.get_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DODGE on autoencoder outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = Data(x_train=encoder(data.x_train), \n",
    "             x_test=encoder(data.x_test), \n",
    "             y_train=data.y_train, \n",
    "             y_test=data.y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"n_runs\": 20,\n",
    "    \"transforms\": [\"standardize\"] * 50,\n",
    "    \"metrics\": [\"accuracy\", \"d2h\", \"pd\", \"pf\", \"auc\"],\n",
    "    \"random\": True,\n",
    "    \"learners\": [],\n",
    "    \"log_path\": \"./\",\n",
    "    \"data\": [data],\n",
    "    \"name\": \"dataclass\"\n",
    "}\n",
    "for _ in range(50):\n",
    "    config[\"learners\"].extend([DecisionTree(random=True), RandomForest(random=True), LogisticRegressionClassifier(random=True), NaiveBayes(random=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hooks': None, 'learner': DecisionTreeClassifier(splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=10.0, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=91), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=51), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=1000.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=32), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=1000.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=79), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=0.1, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=61), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=1000.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=33), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=1000.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=23), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=27), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=10.0, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=50), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=100.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=49), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=10.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=49), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=1000.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=28), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=10.0, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=48), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=1000.0, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=29), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=10.0, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=89), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=0.1, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=89), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=11), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=1000.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=19), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=10.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=38), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=1000.0, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=39), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=1000.0, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=45), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=92), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=100.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=77), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=10.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=89), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=0.1, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=78), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=10.0, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=14), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=0.1, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=82), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=1000.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=76), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=10.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=81), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=56), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=90), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=10.0, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=56), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=1000.0, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=51), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=0.1, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=15), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=10.0, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=85), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=1000.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=37), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=10.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=19), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=0.1, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=99), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=0.1, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=29), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=10.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=19), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=10.0, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=99), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=0.1, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=31), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=10.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=59), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=56), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=92), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=1000.0, penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(n_estimators=99), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=0.1, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(criterion='entropy', splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=23), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=100.0, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=92), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(C=10.0, solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': DecisionTreeClassifier(splitter='random'), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'splitter': ['best', 'random']}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': RandomForestClassifier(criterion='entropy', n_estimators=98), 'name': 'rf', 'random': True, 'random_map': {'criterion': ['gini', 'entropy'], 'n_estimators': (10, 100)}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': LogisticRegression(penalty='l1', solver='liblinear'), 'name': 'rf', 'random': True, 'random_map': {'penalty': ['l1', 'l2'], 'C': [0.1, 1.0, 10.0, 100.0, 1000.0]}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "{'hooks': None, 'learner': GaussianNB(), 'name': 'rf', 'random': True, 'random_map': {}, 'x_test': None, 'x_train': None, 'y_test': None, 'y_train': None}\n",
      "0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardizec|rf\n",
      "1\n",
      "standardizea|rf\n",
      "2\n",
      "standardizeE|rf\n",
      "3\n",
      "standardizet|rf\n",
      "4\n",
      "standardizex|rf\n",
      "5\n",
      "standardizeM|rf\n",
      "6\n",
      "standardizen|rf\n",
      "7\n",
      "standardizeo|rf\n",
      "8\n",
      "standardizeL|rf\n",
      "9\n",
      "standardizel|rf\n",
      "10\n",
      "standardizeF|rf\n",
      "11\n",
      "standardizeR|rf\n",
      "12\n",
      "standardizeG|rf\n",
      "13\n",
      "standardizeq|rf\n",
      "14\n",
      "standardizey|rf\n",
      "15\n",
      "standardizeg|rf\n",
      "16\n",
      "standardizee|rf\n",
      "17\n",
      "standardizep|rf\n",
      "18\n",
      "standardizeB|rf\n",
      "19\n",
      "standardizeZ|rf\n",
      "20\n",
      "standardizev|rf\n",
      "21\n",
      "standardizeY|rf\n",
      "22\n",
      "standardizeA|rf\n",
      "23\n",
      "standardizeK|rf\n",
      "24\n",
      "standardizeJ|rf\n",
      "25\n",
      "standardizes|rf\n",
      "26\n",
      "standardizeH|rf\n",
      "27\n",
      "standardizeN|rf\n",
      "28\n",
      "standardizek|rf\n",
      "29\n",
      "standardizew|rf\n",
      "0\n",
      "standardizeX|rf\n",
      "1\n",
      "standardizeR|rf\n",
      "2\n",
      "standardizen|rf\n",
      "3\n",
      "standardizeL|rf\n",
      "4\n",
      "standardizeP|rf\n",
      "5\n",
      "standardizeY|rf\n",
      "6\n",
      "standardizeK|rf\n",
      "7\n",
      "standardizez|rf\n",
      "8\n",
      "standardizew|rf\n",
      "9\n",
      "standardizea|rf\n",
      "10\n",
      "standardizeT|rf\n",
      "11\n",
      "standardizeH|rf\n",
      "12\n",
      "standardizeV|rf\n",
      "13\n",
      "standardizeu|rf\n",
      "14\n",
      "standardizeJ|rf\n",
      "15\n",
      "standardizeS|rf\n",
      "16\n",
      "standardizel|rf\n",
      "17\n",
      "standardizee|rf\n",
      "18\n",
      "standardizet|rf\n",
      "19\n",
      "standardizeZ|rf\n",
      "20\n",
      "standardizex|rf\n",
      "21\n",
      "standardizeO|rf\n",
      "22\n",
      "standardizec|rf\n",
      "23\n",
      "standardizeN|rf\n",
      "24\n",
      "standardizev|rf\n",
      "25\n",
      "standardizeg|rf\n",
      "26\n",
      "standardizek|rf\n",
      "27\n",
      "standardizeI|rf\n",
      "28\n",
      "standardizeo|rf\n",
      "29\n",
      "standardizeh|rf\n",
      "0\n",
      "standardizeb|rf\n",
      "1\n",
      "standardizeP|rf\n",
      "2\n",
      "standardizev|rf\n",
      "3\n",
      "standardizea|rf\n",
      "4\n",
      "standardizex|rf\n",
      "5\n",
      "standardizeS|rf\n",
      "6\n",
      "standardizeA|rf\n",
      "7\n",
      "standardizeh|rf\n",
      "8\n",
      "standardizey|rf\n",
      "9\n",
      "standardizeN|rf\n",
      "10\n",
      "standardizem|rf\n",
      "11\n",
      "standardizes|rf\n",
      "12\n",
      "standardizeR|rf\n",
      "13\n",
      "standardizeC|rf\n",
      "14\n",
      "standardizeT|rf\n",
      "15\n",
      "standardizeU|rf\n",
      "16\n",
      "standardizew|rf\n",
      "17\n",
      "standardizeW|rf\n",
      "18\n",
      "standardizeM|rf\n",
      "19\n",
      "standardizeH|rf\n",
      "20\n",
      "standardizeK|rf\n",
      "21\n",
      "standardizec|rf\n",
      "22\n",
      "standardizeX|rf\n",
      "23\n",
      "standardizeL|rf\n",
      "24\n",
      "standardizeG|rf\n",
      "25\n",
      "standardizez|rf\n",
      "26\n",
      "standardizej|rf\n",
      "27\n",
      "standardizeB|rf\n",
      "28\n",
      "standardizei|rf\n",
      "29\n",
      "standardizek|rf\n",
      "0\n",
      "standardizeU|rf\n",
      "1\n",
      "standardizer|rf\n",
      "2\n",
      "standardizeP|rf\n",
      "3\n",
      "standardized|rf\n",
      "4\n",
      "standardizeE|rf\n",
      "5\n",
      "standardizem|rf\n",
      "6\n",
      "standardizeC|rf\n",
      "7\n",
      "standardizet|rf\n",
      "8\n",
      "standardizeD|rf\n",
      "9\n",
      "standardizey|rf\n",
      "10\n",
      "standardizef|rf\n",
      "11\n",
      "standardizeM|rf\n",
      "12\n",
      "standardizes|rf\n",
      "13\n",
      "standardizel|rf\n",
      "14\n",
      "standardizeK|rf\n",
      "15\n",
      "standardizeq|rf\n",
      "16\n",
      "standardizeO|rf\n",
      "17\n",
      "standardizeG|rf\n",
      "18\n",
      "standardizeu|rf\n",
      "19\n",
      "standardizeA|rf\n",
      "20\n",
      "standardizeW|rf\n",
      "21\n",
      "standardizeB|rf\n",
      "22\n",
      "standardizeg|rf\n",
      "23\n",
      "standardizeI|rf\n",
      "24\n",
      "standardizex|rf\n",
      "25\n",
      "standardizei|rf\n",
      "26\n",
      "standardizeh|rf\n",
      "27\n",
      "standardizew|rf\n",
      "28\n",
      "standardizeR|rf\n",
      "29\n",
      "standardizez|rf\n",
      "0\n",
      "standardizem|rf\n",
      "1\n",
      "standardizeh|rf\n",
      "2\n",
      "standardizeP|rf\n",
      "3\n",
      "standardizeb|rf\n",
      "4\n",
      "standardizei|rf\n",
      "5\n",
      "standardizeS|rf\n",
      "6\n",
      "standardizeV|rf\n",
      "7\n",
      "standardizeo|rf\n",
      "8\n",
      "standardizee|rf\n",
      "9\n",
      "standardizey|rf\n",
      "10\n",
      "standardizeM|rf\n",
      "11\n",
      "standardizeZ|rf\n",
      "12\n",
      "standardizeT|rf\n",
      "13\n",
      "standardizeX|rf\n",
      "14\n",
      "standardizep|rf\n",
      "15\n",
      "standardizew|rf\n",
      "16\n",
      "standardizeY|rf\n",
      "17\n",
      "standardizeH|rf\n",
      "18\n",
      "standardizeF|rf\n",
      "19\n",
      "standardizeK|rf\n",
      "20\n",
      "standardizea|rf\n",
      "21\n",
      "standardizej|rf\n",
      "22\n",
      "standardizeg|rf\n",
      "23\n",
      "standardizeO|rf\n",
      "24\n",
      "standardizev|rf\n",
      "25\n",
      "standardizeE|rf\n",
      "26\n",
      "standardizeq|rf\n",
      "27\n",
      "standardizes|rf\n",
      "28\n",
      "standardizer|rf\n",
      "29\n",
      "standardizeJ|rf\n",
      "0\n",
      "standardizeO|rf\n",
      "1\n",
      "standardizen|rf\n",
      "2\n",
      "standardized|rf\n",
      "3\n",
      "standardizer|rf\n",
      "4\n",
      "standardizeK|rf\n",
      "5\n",
      "standardizeu|rf\n",
      "6\n",
      "standardizeA|rf\n",
      "7\n",
      "standardizeN|rf\n",
      "8\n",
      "standardizeP|rf\n",
      "9\n",
      "standardizeq|rf\n",
      "10\n",
      "standardizet|rf\n",
      "11\n",
      "standardizem|rf\n",
      "12\n",
      "standardizeo|rf\n",
      "13\n",
      "standardizeC|rf\n",
      "14\n",
      "standardizeF|rf\n",
      "15\n",
      "standardizeR|rf\n",
      "16\n",
      "standardizeZ|rf\n",
      "17\n",
      "standardizeD|rf\n",
      "18\n",
      "standardizeb|rf\n",
      "19\n",
      "standardizea|rf\n",
      "20\n",
      "standardizez|rf\n",
      "21\n",
      "standardizei|rf\n",
      "22\n",
      "standardizeL|rf\n",
      "23\n",
      "standardizeI|rf\n",
      "24\n",
      "standardizeQ|rf\n",
      "25\n",
      "standardizeB|rf\n",
      "26\n",
      "standardizeX|rf\n",
      "27\n",
      "standardizee|rf\n",
      "28\n",
      "standardizeH|rf\n",
      "29\n",
      "standardizeU|rf\n",
      "0\n",
      "standardizei|rf\n",
      "1\n",
      "standardizeY|rf\n",
      "2\n",
      "standardizeS|rf\n",
      "3\n",
      "standardizeI|rf\n",
      "4\n",
      "standardizeZ|rf\n",
      "5\n",
      "standardizep|rf\n",
      "6\n",
      "standardizer|rf\n",
      "7\n",
      "standardizeE|rf\n",
      "8\n",
      "standardizeg|rf\n",
      "9\n",
      "standardizeh|rf\n",
      "10\n",
      "standardized|rf\n",
      "11\n",
      "standardizen|rf\n",
      "12\n",
      "standardizex|rf\n",
      "13\n",
      "standardizeF|rf\n",
      "14\n",
      "standardizeN|rf\n",
      "15\n",
      "standardizeU|rf\n",
      "16\n",
      "standardizeJ|rf\n",
      "17\n",
      "standardizeP|rf\n",
      "18\n",
      "standardizeG|rf\n",
      "19\n",
      "standardizez|rf\n",
      "20\n",
      "standardizee|rf\n",
      "21\n",
      "standardizel|rf\n",
      "22\n",
      "standardizeL|rf\n",
      "23\n",
      "standardizeA|rf\n",
      "24\n",
      "standardizeB|rf\n",
      "25\n",
      "standardizes|rf\n",
      "26\n",
      "standardizev|rf\n",
      "27\n",
      "standardizeR|rf\n",
      "28\n",
      "standardizeC|rf\n",
      "29\n",
      "standardizeO|rf\n",
      "0\n",
      "standardizeI|rf\n",
      "1\n",
      "standardizeb|rf\n",
      "2\n",
      "standardizeS|rf\n",
      "3\n",
      "standardizeL|rf\n",
      "4\n",
      "standardizeW|rf\n",
      "5\n",
      "standardizeN|rf\n",
      "6\n",
      "standardizes|rf\n",
      "7\n",
      "standardizef|rf\n",
      "8\n",
      "standardizeP|rf\n",
      "9\n",
      "standardizeE|rf\n",
      "10\n",
      "standardizeH|rf\n",
      "11\n",
      "standardizeB|rf\n",
      "12\n",
      "standardizeq|rf\n",
      "13\n",
      "standardizeF|rf\n",
      "14\n",
      "standardizet|rf\n",
      "15\n",
      "standardizeX|rf\n",
      "16\n",
      "standardizeu|rf\n",
      "17\n",
      "standardizeV|rf\n",
      "18\n",
      "standardizeJ|rf\n",
      "19\n",
      "standardizek|rf\n",
      "20\n",
      "standardizeG|rf\n",
      "21\n",
      "standardizeU|rf\n",
      "22\n",
      "standardizej|rf\n",
      "23\n",
      "standardizew|rf\n",
      "24\n",
      "standardizeD|rf\n",
      "25\n",
      "standardizei|rf\n",
      "26\n",
      "standardizeo|rf\n",
      "27\n",
      "standardizeQ|rf\n",
      "28\n",
      "standardizeT|rf\n",
      "29\n",
      "standardizeO|rf\n",
      "0\n",
      "standardizeM|rf\n",
      "1\n",
      "standardizez|rf\n",
      "2\n",
      "standardizeX|rf\n",
      "3\n",
      "standardizeY|rf\n",
      "4\n",
      "standardizes|rf\n",
      "5\n",
      "standardizej|rf\n",
      "6\n",
      "standardizeC|rf\n",
      "7\n",
      "standardizeh|rf\n",
      "8\n",
      "standardizeO|rf\n",
      "9\n",
      "standardizeE|rf\n",
      "10\n",
      "standardizeg|rf\n",
      "11\n",
      "standardizey|rf\n",
      "12\n",
      "standardizec|rf\n",
      "13\n",
      "standardizel|rf\n",
      "14\n",
      "standardizeG|rf\n",
      "15\n",
      "standardizev|rf\n",
      "16\n",
      "standardizeW|rf\n",
      "17\n",
      "standardizeP|rf\n",
      "18\n",
      "standardizeZ|rf\n",
      "19\n",
      "standardizeq|rf\n",
      "20\n",
      "standardizeN|rf\n",
      "21\n",
      "standardizeo|rf\n",
      "22\n",
      "standardizem|rf\n",
      "23\n",
      "standardized|rf\n",
      "24\n",
      "standardizex|rf\n",
      "25\n",
      "standardizek|rf\n",
      "26\n",
      "standardizeI|rf\n",
      "27\n",
      "standardizeS|rf\n",
      "28\n",
      "standardizeF|rf\n",
      "29\n",
      "standardizee|rf\n",
      "0\n",
      "standardizep|rf\n",
      "1\n",
      "standardizeu|rf\n",
      "2\n",
      "standardizeF|rf\n",
      "3\n",
      "standardized|rf\n",
      "4\n",
      "standardizee|rf\n",
      "5\n",
      "standardizeG|rf\n",
      "6\n",
      "standardizeC|rf\n",
      "7\n",
      "standardizeX|rf\n",
      "8\n",
      "standardizem|rf\n",
      "9\n",
      "standardizec|rf\n",
      "10\n",
      "standardizeK|rf\n",
      "11\n",
      "standardizex|rf\n",
      "12\n",
      "standardizeT|rf\n",
      "13\n",
      "standardizeo|rf\n",
      "14\n",
      "standardizeP|rf\n",
      "15\n",
      "standardizev|rf\n",
      "16\n",
      "standardizeQ|rf\n",
      "17\n",
      "standardizei|rf\n",
      "18\n",
      "standardizew|rf\n",
      "19\n",
      "standardizey|rf\n",
      "20\n",
      "standardizea|rf\n",
      "21\n",
      "standardizes|rf\n",
      "22\n",
      "standardizeh|rf\n",
      "23\n",
      "standardizeI|rf\n",
      "24\n",
      "standardizeO|rf\n",
      "25\n",
      "standardizeJ|rf\n",
      "26\n",
      "standardizeE|rf\n",
      "27\n",
      "standardizeM|rf\n",
      "28\n",
      "standardizeA|rf\n",
      "29\n",
      "standardizef|rf\n",
      "0\n",
      "standardizez|rf\n",
      "1\n",
      "standardizeM|rf\n",
      "2\n",
      "standardizeq|rf\n",
      "3\n",
      "standardizeK|rf\n",
      "4\n",
      "standardizeV|rf\n",
      "5\n",
      "standardizeu|rf\n",
      "6\n",
      "standardizee|rf\n",
      "7\n",
      "standardizeB|rf\n",
      "8\n",
      "standardizem|rf\n",
      "9\n",
      "standardizeQ|rf\n",
      "10\n",
      "standardizeH|rf\n",
      "11\n",
      "standardizef|rf\n",
      "12\n",
      "standardizea|rf\n",
      "13\n",
      "standardizeZ|rf\n",
      "14\n",
      "standardizeA|rf\n",
      "15\n",
      "standardizeD|rf\n",
      "16\n",
      "standardizex|rf\n",
      "17\n",
      "standardizeL|rf\n",
      "18\n",
      "standardizeP|rf\n",
      "19\n",
      "standardizec|rf\n",
      "20\n",
      "standardizej|rf\n",
      "21\n",
      "standardizeU|rf\n",
      "22\n",
      "standardizek|rf\n",
      "23\n",
      "standardizes|rf\n",
      "24\n",
      "standardizeg|rf\n",
      "25\n",
      "standardizev|rf\n",
      "26\n",
      "standardizew|rf\n",
      "27\n",
      "standardizeo|rf\n",
      "28\n",
      "standardizeR|rf\n",
      "29\n",
      "standardizep|rf\n",
      "0\n",
      "standardizec|rf\n",
      "1\n",
      "standardizeB|rf\n",
      "2\n",
      "standardizez|rf\n",
      "3\n",
      "standardizet|rf\n",
      "4\n",
      "standardizev|rf\n",
      "5\n",
      "standardizeC|rf\n",
      "6\n",
      "standardizeQ|rf\n",
      "7\n",
      "standardizeH|rf\n",
      "8\n",
      "standardizeR|rf\n",
      "9\n",
      "standardizeh|rf\n",
      "10\n",
      "standardizee|rf\n",
      "11\n",
      "standardizel|rf\n",
      "12\n",
      "standardizeS|rf\n",
      "13\n",
      "standardizeD|rf\n",
      "14\n",
      "standardizeG|rf\n",
      "15\n",
      "standardizeE|rf\n",
      "16\n",
      "standardizen|rf\n",
      "17\n",
      "standardizex|rf\n",
      "18\n",
      "standardizeY|rf\n",
      "19\n",
      "standardizea|rf\n",
      "20\n",
      "standardizeO|rf\n",
      "21\n",
      "standardized|rf\n",
      "22\n",
      "standardizeJ|rf\n",
      "23\n",
      "standardizeu|rf\n",
      "24\n",
      "standardizeV|rf\n",
      "25\n",
      "standardizeI|rf\n",
      "26\n",
      "standardizeb|rf\n",
      "27\n",
      "standardizeP|rf\n",
      "28\n",
      "standardizeX|rf\n",
      "29\n",
      "standardizef|rf\n",
      "0\n",
      "standardizen|rf\n",
      "1\n",
      "standardizeq|rf\n",
      "2\n",
      "standardizet|rf\n",
      "3\n",
      "standardizeC|rf\n",
      "4\n",
      "standardizeP|rf\n",
      "5\n",
      "standardizek|rf\n",
      "6\n",
      "standardizel|rf\n",
      "7\n",
      "standardizeg|rf\n",
      "8\n",
      "standardizeW|rf\n",
      "9\n",
      "standardizeN|rf\n",
      "10\n",
      "standardizee|rf\n",
      "11\n",
      "standardizeV|rf\n",
      "12\n",
      "standardizej|rf\n",
      "13\n",
      "standardizeo|rf\n",
      "14\n",
      "standardizec|rf\n",
      "15\n",
      "standardizeX|rf\n",
      "16\n",
      "standardizeJ|rf\n",
      "17\n",
      "standardizey|rf\n",
      "18\n",
      "standardizef|rf\n",
      "19\n",
      "standardizeL|rf\n",
      "20\n",
      "standardizeO|rf\n",
      "21\n",
      "standardizeb|rf\n",
      "22\n",
      "standardizeB|rf\n",
      "23\n",
      "standardizeh|rf\n",
      "24\n",
      "standardizeK|rf\n",
      "25\n",
      "standardizeM|rf\n",
      "26\n",
      "standardizeS|rf\n",
      "27\n",
      "standardizeT|rf\n",
      "28\n",
      "standardizer|rf\n",
      "29\n",
      "standardizeH|rf\n",
      "0\n",
      "standardizeI|rf\n",
      "1\n",
      "standardizeN|rf\n",
      "2\n",
      "standardizea|rf\n",
      "3\n",
      "standardizeS|rf\n",
      "4\n",
      "standardizet|rf\n",
      "5\n",
      "standardizei|rf\n",
      "6\n",
      "standardizew|rf\n",
      "7\n",
      "standardizeX|rf\n",
      "8\n",
      "standardizeL|rf\n",
      "9\n",
      "standardizeh|rf\n",
      "10\n",
      "standardized|rf\n",
      "11\n",
      "standardizeZ|rf\n",
      "12\n",
      "standardizez|rf\n",
      "13\n",
      "standardizeU|rf\n",
      "14\n",
      "standardizeA|rf\n",
      "15\n",
      "standardizeP|rf\n",
      "16\n",
      "standardizey|rf\n",
      "17\n",
      "standardizeK|rf\n",
      "18\n",
      "standardizex|rf\n",
      "19\n",
      "standardizeb|rf\n",
      "20\n",
      "standardizeq|rf\n",
      "21\n",
      "standardizeu|rf\n",
      "22\n",
      "standardizel|rf\n",
      "23\n",
      "standardizeD|rf\n",
      "24\n",
      "standardizee|rf\n",
      "25\n",
      "standardizeV|rf\n",
      "26\n",
      "standardizem|rf\n",
      "27\n",
      "standardizeJ|rf\n",
      "28\n",
      "standardizej|rf\n",
      "29\n",
      "standardizeM|rf\n",
      "0\n",
      "standardizeP|rf\n",
      "1\n",
      "standardizei|rf\n",
      "2\n",
      "standardizex|rf\n",
      "3\n",
      "standardizeW|rf\n",
      "4\n",
      "standardizeL|rf\n",
      "5\n",
      "standardizeG|rf\n",
      "6\n",
      "standardizen|rf\n",
      "7\n",
      "standardizeY|rf\n",
      "8\n",
      "standardizeN|rf\n",
      "9\n",
      "standardizeo|rf\n",
      "10\n",
      "standardizeA|rf\n",
      "11\n",
      "standardizer|rf\n",
      "12\n",
      "standardizeQ|rf\n",
      "13\n",
      "standardized|rf\n",
      "14\n",
      "standardizet|rf\n",
      "15\n",
      "standardizeM|rf\n",
      "16\n",
      "standardizep|rf\n",
      "17\n",
      "standardizeF|rf\n",
      "18\n",
      "standardizeR|rf\n",
      "19\n",
      "standardizeH|rf\n",
      "20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standardizeK|rf\n",
      "21\n",
      "standardizeh|rf\n",
      "22\n",
      "standardizeB|rf\n",
      "23\n",
      "standardizeI|rf\n",
      "24\n",
      "standardizea|rf\n",
      "25\n",
      "standardizez|rf\n",
      "26\n",
      "standardizeJ|rf\n",
      "27\n",
      "standardizey|rf\n",
      "28\n",
      "standardizes|rf\n",
      "29\n",
      "standardizeC|rf\n",
      "0\n",
      "standardizet|rf\n",
      "1\n",
      "standardizeh|rf\n",
      "2\n",
      "standardizeS|rf\n",
      "3\n",
      "standardizeM|rf\n",
      "4\n",
      "standardizeO|rf\n",
      "5\n",
      "standardizeA|rf\n",
      "6\n",
      "standardizeG|rf\n",
      "7\n",
      "standardizeB|rf\n",
      "8\n",
      "standardizeI|rf\n",
      "9\n",
      "standardizez|rf\n",
      "10\n",
      "standardizeQ|rf\n",
      "11\n",
      "standardizeK|rf\n",
      "12\n",
      "standardizef|rf\n",
      "13\n",
      "standardizeV|rf\n",
      "14\n",
      "standardizes|rf\n",
      "15\n",
      "standardizec|rf\n",
      "16\n",
      "standardizeH|rf\n",
      "17\n",
      "standardizeW|rf\n",
      "18\n",
      "standardizen|rf\n",
      "19\n",
      "standardizeY|rf\n",
      "20\n",
      "standardizeE|rf\n",
      "21\n",
      "standardizeX|rf\n",
      "22\n",
      "standardizea|rf\n",
      "23\n",
      "standardizer|rf\n",
      "24\n",
      "standardizeJ|rf\n",
      "25\n",
      "standardizeg|rf\n",
      "26\n",
      "standardizeT|rf\n",
      "27\n",
      "standardizeD|rf\n",
      "28\n",
      "standardizem|rf\n",
      "29\n",
      "standardizeb|rf\n",
      "0\n",
      "standardizeY|rf\n",
      "1\n",
      "standardizek|rf\n",
      "2\n",
      "standardizeZ|rf\n",
      "3\n",
      "standardizeX|rf\n",
      "4\n",
      "standardizeh|rf\n",
      "5\n",
      "standardizex|rf\n",
      "6\n",
      "standardizeb|rf\n",
      "7\n",
      "standardizeS|rf\n",
      "8\n",
      "standardizeR|rf\n",
      "9\n",
      "standardizet|rf\n",
      "10\n",
      "standardizez|rf\n",
      "11\n",
      "standardizej|rf\n",
      "12\n",
      "standardizeE|rf\n",
      "13\n",
      "standardizew|rf\n",
      "14\n",
      "standardizeU|rf\n",
      "15\n",
      "standardizeI|rf\n",
      "16\n",
      "standardizeu|rf\n",
      "17\n",
      "standardizeG|rf\n",
      "18\n",
      "standardizen|rf\n",
      "19\n",
      "standardizep|rf\n",
      "20\n",
      "standardizec|rf\n",
      "21\n",
      "standardizeA|rf\n",
      "22\n",
      "standardizev|rf\n",
      "23\n",
      "standardizem|rf\n",
      "24\n",
      "standardizeQ|rf\n",
      "25\n",
      "standardizeO|rf\n",
      "26\n",
      "standardizeo|rf\n",
      "27\n",
      "standardizeH|rf\n",
      "28\n",
      "standardizeK|rf\n",
      "29\n",
      "standardizer|rf\n",
      "0\n",
      "standardizej|rf\n",
      "1\n",
      "standardizel|rf\n",
      "2\n",
      "standardizeM|rf\n",
      "3\n",
      "standardizeJ|rf\n",
      "4\n",
      "standardizeN|rf\n",
      "5\n",
      "standardizev|rf\n",
      "6\n",
      "standardizee|rf\n",
      "7\n",
      "standardizeR|rf\n",
      "8\n",
      "standardizeb|rf\n",
      "9\n",
      "standardizeA|rf\n",
      "10\n",
      "standardizeU|rf\n",
      "11\n",
      "standardizeX|rf\n",
      "12\n",
      "standardizeE|rf\n",
      "13\n",
      "standardizen|rf\n",
      "14\n",
      "standardizey|rf\n",
      "15\n",
      "standardizei|rf\n",
      "16\n",
      "standardizef|rf\n",
      "17\n",
      "standardizem|rf\n",
      "18\n",
      "standardizeZ|rf\n",
      "19\n",
      "standardizeT|rf\n",
      "20\n",
      "standardizeS|rf\n",
      "21\n",
      "standardizeB|rf\n",
      "22\n",
      "standardizeQ|rf\n",
      "23\n",
      "standardizeu|rf\n",
      "24\n",
      "standardizeo|rf\n",
      "25\n",
      "standardizeI|rf\n",
      "26\n",
      "standardizes|rf\n",
      "27\n",
      "standardized|rf\n",
      "28\n",
      "standardizew|rf\n",
      "29\n",
      "standardizek|rf\n",
      "0\n",
      "standardizex|rf\n",
      "1\n",
      "standardizeo|rf\n",
      "2\n",
      "standardizeJ|rf\n",
      "3\n",
      "standardizeL|rf\n",
      "4\n",
      "standardizeu|rf\n",
      "5\n",
      "standardizeP|rf\n",
      "6\n",
      "standardizey|rf\n",
      "7\n",
      "standardizeA|rf\n",
      "8\n",
      "standardizeB|rf\n",
      "9\n",
      "standardizeF|rf\n",
      "10\n",
      "standardizeh|rf\n",
      "11\n",
      "standardizer|rf\n",
      "12\n",
      "standardizes|rf\n",
      "13\n",
      "standardizet|rf\n",
      "14\n",
      "standardizeY|rf\n",
      "15\n",
      "standardizef|rf\n",
      "16\n",
      "standardizel|rf\n",
      "17\n",
      "standardizeZ|rf\n",
      "18\n",
      "standardizeq|rf\n",
      "19\n",
      "standardizeE|rf\n",
      "20\n",
      "standardizej|rf\n",
      "21\n",
      "standardizeK|rf\n",
      "22\n",
      "standardizeS|rf\n",
      "23\n",
      "standardizeU|rf\n",
      "24\n",
      "standardizeR|rf\n",
      "25\n",
      "standardizeM|rf\n",
      "26\n",
      "standardizeH|rf\n",
      "27\n",
      "standardizeI|rf\n",
      "28\n",
      "standardizeb|rf\n",
      "29\n",
      "standardizeV|rf\n",
      "0\n",
      "standardizet|rf\n",
      "1\n",
      "standardizeB|rf\n",
      "2\n",
      "standardizec|rf\n",
      "3\n",
      "standardizem|rf\n",
      "4\n",
      "standardizep|rf\n",
      "5\n",
      "standardizeT|rf\n",
      "6\n",
      "standardizeD|rf\n",
      "7\n",
      "standardizeq|rf\n",
      "8\n",
      "standardizeu|rf\n",
      "9\n",
      "standardizeG|rf\n",
      "10\n",
      "standardized|rf\n",
      "11\n",
      "standardizes|rf\n",
      "12\n",
      "standardizeW|rf\n",
      "13\n",
      "standardizex|rf\n",
      "14\n",
      "standardizeH|rf\n",
      "15\n",
      "standardizef|rf\n",
      "16\n",
      "standardizeS|rf\n",
      "17\n",
      "standardizeU|rf\n",
      "18\n",
      "standardizeL|rf\n",
      "19\n",
      "standardizeF|rf\n",
      "20\n",
      "standardizeC|rf\n",
      "21\n",
      "standardizel|rf\n",
      "22\n",
      "standardizej|rf\n",
      "23\n",
      "standardizeh|rf\n",
      "24\n",
      "standardizeI|rf\n",
      "25\n",
      "standardizeZ|rf\n",
      "26\n",
      "standardizeb|rf\n",
      "27\n",
      "standardizeA|rf\n",
      "28\n",
      "standardizeM|rf\n",
      "29\n",
      "standardizeY|rf\n"
     ]
    }
   ],
   "source": [
    "dodge = DODGE(config)\n",
    "dodge.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raise_utils.interpret import DODGEInterpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp = DODGEInterpreter(files=['./dataclass.txt'], max_by=0, metrics=[\"accuracy\", \"d2h\", \"pd\", \"pf\", \"auc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataclass.txt': {'accuracy': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         0.98979592, 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ]),\n",
       "  'd2h': array([0.70710678, 0.70710678, 0.70710678, 0.70710678, 0.70710678,\n",
       "         0.68746493, 0.70710678, 0.70710678, 0.70710678, 0.70710678,\n",
       "         0.70710678, 0.70710678, 0.70710678, 0.70710678, 0.70710678,\n",
       "         0.70710678, 0.70710678, 0.70710678, 0.70710678, 0.70710678]),\n",
       "  'pd': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         0.97222222, 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ]),\n",
       "  'pf': array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0.]),\n",
       "  'auc': array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         0.98611111, 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "         1.        , 1.        , 1.        , 1.        , 1.        ])}}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interp.interpret()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
